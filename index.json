[{"content":"1.命名规范 1.1.镜像命名规范 镜像命名规范：\u0026lt;小组名\u0026gt;/\u0026lt;项目名\u0026gt;/\u0026lt;镜像名\u0026gt;:\u0026lt;版本号\u0026gt;\n版本号：\u0026lt;主版本号\u0026gt;.\u0026lt;次版本号\u0026gt;.\u0026lt;修订号\u0026gt; eg: 1.0.0\n1.2.容器命名规范 容器命名规范：\u0026lt;小组名\u0026gt;-\u0026lt;项目名\u0026gt;-\u0026lt;容器名\u0026gt;-\u0026lt;版本号\u0026gt;\n","permalink":"/posts/reference/docker%E7%9B%B8%E5%85%B3%E7%9A%84%E9%83%A8%E7%BD%B2%E8%A7%84%E8%8C%83/","summary":"1.命名规范 1.1.镜像命名规范 镜像命名规范：\u0026lt;小组名\u0026gt;/\u0026lt;项目名\u0026gt;/\u0026lt;镜像名\u0026gt;:\u0026lt;版本号\u0026gt;","title":"docker 相关的部署规范"},{"content":"安装docker sudo apt -y update sudo apt -y upgrade sudo apt -y full-upgrade # 安装依赖 sudo apt install -y apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release # 添加官方GPG密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg #添加仓库 echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # 更新apt sudo apt -y update # 安装docker sudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin # 安装docker-compose sudo apt install -y docker-compose 1.镜像相关 # 构建镜像 docker build [选项] \u0026lt;上下文路径/URL/-\u0026gt; # 选项: -f, --file=\u0026#34;\u0026#34; # 指定要使用的Dockerfile路径（默认为./Dockerfile） # --force-rm=false # 在构建过程中删除中间容器 # --no-cache=false # 始终使用缓存 # --pull=false # 在构建过程中尝试去更新镜像的新版本 # --quiet=false # 安静模式，成功后只输出镜像ID # --rm=true # 在构建成功后删除临时容器 # -t, --tag=[] # 镜像名称（默认值：\u0026lt;上下文路径\u0026gt;的基本名称） # --ulimit=[] # Ulimit配置 # 拉取镜像 docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] # 查看镜像 docker images [选项] [仓库名] # 删除镜像 docker rmi [选项] \u0026lt;镜像1\u0026gt; [\u0026lt;镜像2\u0026gt; ...] # 查看镜像历史 docker history [选项] \u0026lt;镜像名\u0026gt; # 查看镜像详细信息 docker inspect [选项] \u0026lt;镜像名\u0026gt; 2.容器相关 # 创建容器 docker run [选项] \u0026lt;镜像名\u0026gt; [命令] #eg: docker run -d -p 8080:8080 --name tomcat tomcat:8.5.51 #选项 # -d 后台运行容器，并返回容器ID # -i 以交互模式运行容器，通常与 -t 同时使用 # -t 为容器重新分配一个伪输入终端，通常与 -i 同时使用 # -P 随机端口映射 # -p 指定端口映射，格式为：主机(宿主)端口:容器端口 # --name 指定容器名字 # --link 连接到其它容器 # --rm 容器退出后自动删除容器文件 # --volumes-from 从其它容器或数据卷挂载一些配置或其它文件 # --volume 挂载宿主机目录或文件，格式为：主机目录:容器目录 # --privileged=true 给容器内的root用户赋予最高权限，容器内的root用户就拥有了真正的root权限 # --restart # no 容器退出时不重启 # on-failure[:max-retries] 容器故障退出（返回值非零）时重启，最多重启max-retries次 # always 容器退出时总是重启 # unless-stopped 容器退出时总是重启，但是不考虑在Docker守护进程启动时就已经停止了的容器 # --env-file 从指定文件读入环境变量 # eg: # docker run -d -p 8080:8080 --name tomcat tomcat:8.5.51 --env-file ./env.list # 查看容器 docker ps [选项] # 删除容器 docker rm [选项] \u0026lt;容器名\u0026gt; # 启动容器 # 启动和创建容器的区别在于，启动容器是针对已经创建好的容器进行启动，而创建容器则是针对镜像进行的操作 docker start [选项] \u0026lt;容器名\u0026gt; # 停止容器 docker stop [选项] \u0026lt;容器名\u0026gt; # 查看容器日志 docker logs [选项] \u0026lt;容器名\u0026gt; # 查看容器内进程 docker top [选项] \u0026lt;容器名\u0026gt; # 查看容器详细信息 docker inspect [选项] \u0026lt;容器名\u0026gt; # 进入容器 docker exec [选项] \u0026lt;容器名\u0026gt; [命令] # 导出容器 docker export [选项] \u0026lt;容器名\u0026gt; # 导入容器 docker import [选项] \u0026lt;容器名\u0026gt; # 重命名容器 docker rename [选项] \u0026lt;容器名\u0026gt; \u0026lt;新容器名\u0026gt; # 查看容器使用的资源 docker stats [选项] \u0026lt;容器名\u0026gt; # 查看容器端口映射 docker port [选项] \u0026lt;容器名\u0026gt; # 导出容器中的文件 docker cp [选项] \u0026lt;容器名\u0026gt;:\u0026lt;容器内路径\u0026gt; \u0026lt;宿主机路径\u0026gt; # 选项: -a, --archive=false # 归档模式(默认) # -L, --follow-link=false # 总是解析符号链接 # -d, --device=false # 复制字符和块设备 # -r, --recursive=false # 递归复制整个目录 # -p, --pause=true # 暂停容器中的所有进程 docker 检查与排错 docker logs [选项] \u0026lt;容器名\u0026gt; # 选项: -f, --follow=false # 跟踪日志输出 # --since=\u0026#34;\u0026#34; # 显示自某个timestamp之后的日志，或相对时间，如42m（即42分钟） # --tail=\u0026#34;all\u0026#34; # 从日志末尾显示多少行日志， 默认是all # -t, --timestamps=false # 显示时间戳 # --until=\u0026#34;\u0026#34; # 显示自某个timestamp之前的日志，或相对时间，如42m（即42分钟） # 查看容器占用 docker stats [选项] \u0026lt;容器名\u0026gt; # 选项: --all=false # 显示所有容器（默认显示运行中的容器） # --format=\u0026#34;\u0026#34; # 使用Go模板显示 # --no-stream=false # 不显示实时流容器的统计信息 # --no-trunc=false # 不截断输出 # 停止所有容器 docker stop $(docker ps -a -q) # 移除所有容器 docker rm $(docker ps -a -q) # 移除所有镜像 docker image rmi $(docker images -q) # 清空docker中所有的东西 docker system prune -a # 清空缓存 docker system prune -f # 清空未使用的镜像 docker image prune -a # 清空未使用的容器 docker container prune # 清空未使用的卷 docker volume prune # 清空未使用的网络 docker network prune # 清空未使用的构建缓存 docker builder prune # 清空未使用的数据 docker system prune -a --volumes # 清空所有未使用的数据 docker system prune -a --volumes --force 3.容器日志 # 查看容器日志 docker logs [选项] \u0026lt;容器名\u0026gt; # 选项: -f, --follow=false # 跟踪日志输出 # --since=\u0026#34;\u0026#34; # 显示自某个timestamp之后的日志，或相对时间，如42m（即42分钟） # --tail=\u0026#34;all\u0026#34; # 从日志末尾显示多少行日志， 默认是all # -t, --timestamps=false # 显示时间戳 # --until=\u0026#34;\u0026#34; # 显示自某个timestamp之前的日志，或相对时间，如42m（即42分钟） docker submodule # 获取子模块 git submodule update --init --recursive docker-compose # 启动命令 docker-compose up [选项] [服务名] # 选项 # -d 后台运行 # --build 构建镜像 docker私服的相关命令 # 登录 docker login # 上传 docker push \u0026lt;镜像名\u0026gt; ","permalink":"/posts/tools/docker/docker%E5%91%BD%E4%BB%A4/","summary":"安装docker sudo apt -y update sudo apt -y upgrade sudo apt -y full-upgrade # 安装依赖 sudo apt install -y apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release # 添加官方GPG密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg #添加仓库 echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo","title":"Docker命令"},{"content":"修改用户密码 alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;123456\u0026#39;; 刷新权限 flush privileges; 添加一个远程用户 create user \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;password\u0026#39; GRANT all ON *.* TO \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39;; grant all privileges on *.* to \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; with grant option; *.*所有数据库下的所有表 创建数据库并设定中文编码 CREATE DATABASE `db_name` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 登录格式 mysql -h #{数据库IP} -P 3306 -u #{用户名} -p -D #{数据库名} 自增id 不连续时 SET @auto_id = 0; UPDATE 表名 SET 自增字段名 = (@auto_id := @auto_id + 1); ALTER TABLE 表名 AUTO_INCREMENT = 1; 文件 source 表 # 添加一列 alter table 表名 add column 列名 类型; 数据 # 插入数据 insert into 表名 (字段1,字段2) values (值1,值2); # 更新数据 update 表名 set 字段1=值1,字段2=值2 where 条件; # 删除数据 delete from 表名 where 条件; ","permalink":"/posts/tools/sql/mysql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","summary":"修改用户密码 alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;123456\u0026#39;; 刷新权限 flush privileges; 添加一个远程用户 create user \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;password\u0026#39; GRANT all ON *.* TO \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39;; grant all privileges on *.* to \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; with grant option; *.*所有数据库下的所有表 创建数据库并设","title":"MySql常用命令"},{"content":"git 常用命令 git 基本配置 git config --global user.name \u0026#34;your name\u0026#34; git config --global user.email \u0026#34;your email\u0026#34; git 基本操作 git init # 初始化仓库 git add . # 添加所有文件到暂存区 git commit -m \u0026#34;commit message\u0026#34; # 提交到本地仓库 git remote add origin git push -u origin master # 推送到远程仓库 git pull origin master # 拉取远程仓库 git clone # 克隆远程仓库 git status # 查看当前状态 git log # 查看提交日志 git diff # 查看修改内容 git branch # 查看分支 git checkout -b branch_name # 创建并切换到新分支 git checkout branch_name # 切换分支 git merge branch_name # 合并分支 git branch -d branch_name # 删除分支 git reset --hard HEAD^ # 回退到上一个版本 git reset --hard commit_id # 回退到指定版本 git reflog # 查看命令历史 git rm file_name # 删除文件 git stash # 暂存当前修改 git stash list # 查看暂存列表 git stash apply # 恢复暂存 git stash drop # 删除暂存 git stash pop # 恢复并删除暂存 git remote -v # 查看远程仓库地址 git remote set-url origin new_url # 修改远程仓库地址 git push origin --delete branch_name # 删除远程分支 git push origin :branch_name # 删除远程分支 git tag # 查看标签 git tag tag_name # 创建标签 git tag tag_name commit_id # 指定提交创建标签 git tag -a tag_name -m \u0026#34;tag message\u0026#34; # 创建带有说明的标签 git tag -d tag_name # 删除标签 git push origin tag_name # 推送标签到远程 git push origin --tags # 推送所有标签到远程 git push origin :refs/tags/tag_name # 删除远程标签 git push origin --delete tag tag_name # 删除远程标签 git checkout -- file_name # 撤销工作区修改 git reset HEAD file_name # 撤销暂存区修改 git reset --hard HEAD^ # 撤销本地提交 git reset --hard commit_id # 撤销本地提交 git config --global alias.st status # 设置别名 git config --global alias.co checkout # 设置别名 git config --global alias.ci commit # 设置别名 git config --global alias.br branch # 设置别名 git config --global alias.unstage \u0026#39;reset HEAD\u0026#39; # 设置别名 git config --global alias.last \u0026#39;log -1\u0026#39; # 设置别名 git config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; # 设置别名 git 子模块 git submodule add # 添加子模块 # 添加子模块并自定义子模块目录名称和分支 git submodule add \u0026lt;repository\u0026gt; [\u0026lt;path\u0026gt;] git submodule init # 初始化子模块 git submodule update # 更新子模块 git submodule foreach git pull # 更新所有子模块 # 删除子模块 # 1. 删除.gitmodules中对应子模块的条目 # 2. 删除.git/config中对应子模块的条目 # 3. 执行git rm --cached path/to/submodule # 4. 执行rm -rf .git/modules/path/to/submodule # 5. 执行rm -rf path/to/submodule ","permalink":"/posts/tools/git/git/","summary":"git 常用命令 git 基本配置 git config --global user.name \u0026#34;your name\u0026#34; git config --global user.email \u0026#34;your email\u0026#34; git 基本操作 git init # 初始化仓库 git add . # 添加所有文件到暂存区 git commit -m \u0026#34;commit message\u0026#34; # 提交到本地仓库 git remote add origin git push -u origin","title":"git"},{"content":"系统资源相关 # 查看内存 free -h # 查看cpu cat /proc/cpuinfo # 查看cpu使用情况 top # 查看GPU使用情况 nvidia-smi # 查看磁盘 df -h # 查看系统版本 cat /etc/os-release # 查看系统信息 uname -a # 列出所有文件夹和文件 显示占用空间 du -sh * # 查看文件夹大小 du -sh folder_name # 查看文件大小 du -sh file_name 用户相关 # 创建用户 useradd -m -s /bin/bash -d /home/username username ## 解释: -m 创建用户目录, -s 指定shell, -d 指定用户目录 # 设置密码 passwd username # 删除用户 userdel -r username # 添加用户的sudo权限 ## 编辑sudoers文件 vi /etc/sudoers ## 在root ALL=(ALL) ALL下面添加 username ALL=(ALL) ALL # 查看用户组 groups username # 修改用户组 usermod -g groupname username # 查看所有用户 cat /etc/passwd 目录挂载 # 查看挂载 df -h # 挂载目录 mount /dev/sdb1 /home/username/data # 卸载目录 umount /home/username/data # 挂载硬盘 ## 查看硬盘 fdisk -l ## 格式化硬盘 fdisk /dev/sdb ## 格式化为ext4 mkfs.ext4 /dev/sdb1 挂载目录并立即生效\n# 挂载目录 mount /dev/sdb1 /home/username/data # 立即生效 mount -a 文件 # 带权限复制 cp -rp source dest 工具 定时脚本 # 查看定时脚本 crontab -l curl # 下载文件 curl -o filename url 系统路径/变量 持久化添加/改变系统路径/变量\n# 添加到系统路径 echo \u0026#39;export PATH=$PATH:/home/username/bin\u0026#39; \u0026gt;\u0026gt; /etc/profile # 立即生效 source /etc/profile tool ssh # 生成密钥 ssh-keygen -t rsa -C \u0026#34;{email}\u0026#34; # 查看密钥 cat ~/.ssh/id_rsa.pub apt # 设置tsinghua源 sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo sed -i \u0026#39;s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apt/sources.list # 更新源 s ","permalink":"/posts/tips/linux/","summary":"系统资源相关 # 查看内存 free -h # 查看cpu cat /proc/cpuinfo # 查看cpu使用情况 top # 查看GPU使用情况 nvidia-smi # 查看磁盘 df -h # 查看系统版本 cat /etc/os-release # 查看系统信息 uname -a # 列","title":"Linux"},{"content":"wsl pass\n适用于Android的Windows子系统 ","permalink":"/posts/tips/windows%E4%B8%8B%E7%9A%84%E5%AD%90%E7%B3%BB%E7%BB%9F/","summary":"wsl pass 适用于Android的Windows子系统","title":"windows 下的子系统"},{"content":"C++ Sync thread的使用 #include \u0026lt;thread\u0026gt; #include \u0026lt;stdio.h\u0026gt; void my_func(int thread_id, int num_threads) { printf(\u0026#34;Hello from spawned thread %d of %d\\n\u0026#34;, thread_id, num_threads); } int main(int argc, char** argv) { std::thread t0 = std::thread(my_func, 0, 2); std::thread t1 = std::thread(my_func, 1, 2); printf(\u0026#34;The main thread is running concurrently with spawned threads.\\n\u0026#34;); t0.join(); t1.join(); printf(\u0026#34;Spawned threads have terminated at this point.\\n\u0026#34;); return 0; } mutex #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;mutex\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;thread\u0026gt; std::map\u0026lt;std::string, std::string\u0026gt; g_pages; std::mutex g_pages_mutex; void save_page(const std::string\u0026amp; url) { // simulate a long page fetch std::this_thread::sleep_for(std::chrono::seconds(2)); std::string result = \u0026#34;fake content\u0026#34;; std::lock_guard\u0026lt;std::mutex\u0026gt; guard(g_pages_mutex); g_pages[url] = result; } int main() { std::thread t1(save_page, \u0026#34;http://foo\u0026#34;); std::thread t2(save_page, \u0026#34;http://bar\u0026#34;); t1.join(); t2.join(); // safe to access g_pages without lock now, as the threads are joined for (const auto\u0026amp; pair : g_pages) std::cout \u0026lt;\u0026lt; pair.first \u0026lt;\u0026lt; \u0026#34; =\u0026gt; \u0026#34; \u0026lt;\u0026lt; pair.second \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Output\nhttp://bar =\u0026gt; fake content http://foo =\u0026gt; fake content condition_variable 线程调用 wait (lock)来指示它希望等待来自另一个线程的通知。\n注意，互斥对象(包装在 std: : only _ lock 中)被传递给 wait ()调用。当通知线程时，条件变量将获得锁。\n这意味着当调用 wait ()返回时，调用线程是锁的当前持有者。锁通常用于保护线程现在需要检查的共享变量，以确保它正在等待的条件为真。\n创建 N 个线程。N-1个线程等待来自线程0的通知，然后在接到通知后，自动递增一个受共享互斥锁保护的计数器。\n/* * Wrapper class around a counter, a condition variable, and a mutex. */ class ThreadState { public: std::condition_variable *condition_variable_; std::mutex *mutex_; int counter_; int num_waiting_threads_; ThreadState(int num_waiting_threads) { condition_variable_ = new std::condition_variable(); mutex_ = new std::mutex(); counter_ = 0; num_waiting_threads_ = num_waiting_threads; } ~ThreadState() { delete condition_variable_; delete mutex_; } }; void signal_fn(ThreadState *thread_state) { // Acquire mutex to make sure the shared counter is read in a // consistent state. thread_state-\u0026gt;mutex_-\u0026gt;lock(); while (thread_state-\u0026gt;counter_ \u0026lt; thread_state-\u0026gt;num_waiting_threads_) { thread_state-\u0026gt;mutex_-\u0026gt;unlock(); // Release the mutex before calling `notify_all()` to make sure // waiting threads have a chance to make progress. thread_state-\u0026gt;condition_variable_-\u0026gt;notify_all(); // Re-acquire the mutex to read the shared counter again. thread_state-\u0026gt;mutex_-\u0026gt;lock(); } thread_state-\u0026gt;mutex_-\u0026gt;unlock(); } void wait_fn(ThreadState *thread_state) { // A lock must be held in order to wait on a condition variable. // This lock is atomically released before the thread goes to sleep // when `wait()` is called. The lock is atomically re-acquired when // the thread is woken up using `notify_all()`. std::unique_lock\u0026lt;std::mutex\u0026gt; lk(*thread_state-\u0026gt;mutex_); thread_state-\u0026gt;condition_variable_-\u0026gt;wait(lk); // Increment the shared counter with the lock re-acquired to inform the // signaling thread that this waiting thread has successfully been // woken up. thread_state-\u0026gt;counter_++; printf(\u0026#34;Lock re-acquired after wait()...\\n\u0026#34;); lk.unlock(); } /* * Signaling thread spins until each waiting thread increments a shared * counter after being woken up from the `wait()` method. */ void condition_variable_example() { int num_threads = 3; printf(\u0026#34;==============================================================\\n\u0026#34;); printf(\u0026#34;Starting %d threads for signal-and-waiting...\\n\u0026#34;, num_threads); std::thread *threads = new std::thread[num_threads]; ThreadState *thread_state = new ThreadState(num_threads - 1); threads[0] = std::thread(signal_fn, thread_state); for (int i = 1; i \u0026lt; num_threads; i++) { threads[i] = std::thread(wait_fn, thread_state); } for (int i = 0; i \u0026lt; num_threads; i++) { threads[i].join(); } printf(\u0026#34;==============================================================\\n\u0026#34;); delete thread_state; delete[] threads; } part_a step 1 实现TaskSystemParallelSpawn void TaskSystemParallelSpawn::run(IRunnable *runnable, int num_total_tasks) { // // TODO: CS149 students will modify the implementation of this // method in Part A. The implementation provided below runs all // tasks sequentially on the calling thread. // std::atomic\u0026lt;int\u0026gt; taskId(0); int num_threads = this-\u0026gt;num_threads; std::thread threads[num_threads]; // 交叉分配任务 for (int i = 0; i \u0026lt; num_threads; i++) { threads[i] = std::thread([\u0026amp;, i]() { int task_id = taskId.fetch_add(1); while (task_id \u0026lt; num_total_tasks) { runnable-\u0026gt;runTask(task_id, num_total_tasks); task_id = taskId.fetch_add(1); } }); } for (int i = 0; i \u0026lt; num_threads; i++) { threads[i].join(); } // printf(\u0026#34;done\\n\u0026#34;); } Q:How will you assign tasks to your worker threads? Should you consider static or dynamic assignment of tasks to threads? A:交叉分配任务，动态分配任务\nQ:How will you ensure that all tasks are executed exactly once? A:使用原子变量taskId\nstep 2 实现 TaskSystemParallelThreadPoolSpinning step1 的overhead主要是创建线程的开销(尤其是计算量低的任务上)，因此使用线程池可以减少开销\n要求: 在TestSystem 创建时,或者在run时创建线程池\nQ1: 作为一个开始的实现，我们建议您将worker threads设计为连续循环，始终检查它们是否有更多的工作要执行。(进入 while 循环直到条件为真的线程通常称为“spinning”) 那么worker thread 如何确定有work要执行呢？\nTaskSystemParallelThreadPoolSpinning::TaskSystemParallelThreadPoolSpinning(int num_threads) : ITaskSystem(num_threads) { // // TODO: CS149 student implementations may decide to perform setup // operations (such as thread pool construction) here. // Implementations are free to add new class member variables // (requiring changes to tasksys.h). // exit_flag_ = false; for (int i = 0; i \u0026lt; num_threads; i++) { threads.emplace_back(\u0026amp;TaskSystemParallelThreadPoolSpinning::func, this); } } TaskSystemParallelThreadPoolSpinning::~TaskSystemParallelThreadPoolSpinning() { exit_flag_ = true; for (auto \u0026amp;thread : threads) { thread.join(); } } void TaskSystemParallelThreadPoolSpinning::run(IRunnable *runnable, int num_total_tasks) { // // TODO: CS149 students will modify the implementation of this // method in Part A. The implementation provided below runs all // tasks sequentially on the calling thread. // // printf(\u0026#34;run\\n\u0026#34;); runnable_ = runnable; num_tasks_ = num_total_tasks; num_tasks_done_ = num_total_tasks; for (int i = 0; i \u0026lt; num_total_tasks; i++) { tasks_mutex_.lock(); tasks_.push(i); tasks_mutex_.unlock(); } while (num_tasks_done_ \u0026lt; num_total_tasks) { std::this_thread::yield(); }; // Q:为什么要使用yield // A:因为如果不使用yield，那么线程会一直占用CPU，导致其他线程无法运行 // Q:那我直接死循环呢 // A:死循环会导致CPU占用率100%，导致其他线程无法运行 } Q2:确保 run ()实现所需的同步行为是非常重要的。如何更改 run ()的实现以确定批量任务启动中的所有任务都已完成？ A:使用原子变量num_tasks_done_，每个任务完成时，num_tasks_done_加一，当num_tasks_done_等于num_total_tasks时，所有任务完成\nstep 3 实现 TaskSystemParallelThreadPoolSleeping Step2的缺点： 当线程“spin”等待某些操作时，它们会利用 CPU 核心的执行资源。\n例如，工作线程可能会循环等待新任务到达。 另一个例子是，主线程可能会循环等待辅助线程完成所有任务，这样它就可以从 run ()调用返回。 这可能会影响性能，因为即使这些线程没有做有用的工作，也会使用 CPU 资源来运行这些线程。\n在任务的这一部分中，我们希望您通过让线程处于休眠状态来提高任务系统的效率，直到它们所等待的条件得到满足。\n您的实现可以选择使用条件变量来实现此行为。条件变量是一个同步原语，它允许线程在等待条件存在时休眠(不占用 CPU 处理资源)。其他线程向等待唤醒的线程发出“信号”，以查看它们所等待的条件是否已经满足。例如，如果没有工作要做，您的工作线程可能会处于休眠状态(这样它们就不会从尝试执行有用工作的线程那里占用 CPU 资源)。另一个例子是，调用 run ()的主应用程序线程可能希望在等待批量任务启动中的所有任务由工作线程完成时休眠。(否则，一个旋转的主线程将从工作线程那里夺走 CPU 资源!)有关 C + + 中条件变量的更多信息，请参见我们的 C + + 同步教程。\n您在这部分作业中的实现可能需要考虑棘手的race conditions 。您需要考虑许多可能的线程行为交错\n您可能需要考虑编写额外的测试用例来测试您的系统。赋值入门代码包括评分脚本用于评分代码性能的工作负载，但是我们也将使用一组更广泛的工作负载来测试您的实现的正确性，而我们在入门代码中并没有提供这些工作负载！\nThe assignment starter code includes the workloads that the grading script will use to grade the performance of your code, but we will also test the correctness of your implementation using a wider set of workloads that we are not providing in the starter code!\ntasksys.h\n/* * TaskSystemParallelThreadPoolSleeping: This class is the student\u0026#39;s * optimized implementation of a parallel task execution engine that uses * a thread pool. See definition of ITaskSystem in * itasksys.h for documentation of the ITaskSystem interface. */ class TaskSystemParallelThreadPoolSleeping : public ITaskSystem { public: TaskSystemParallelThreadPoolSleeping(int num_threads); ~TaskSystemParallelThreadPoolSleeping(); const char *name(); void run(IRunnable *runnable, int num_total_tasks); TaskID runAsyncWithDeps(IRunnable *runnable, int num_total_tasks, const std::vector\u0026lt;TaskID\u0026gt; \u0026amp;deps); void sync(); private: std::vector\u0026lt;std::thread\u0026gt; threads; int num_tasks_; bool exit_flag_; std::atomic\u0026lt;int\u0026gt; num_tasks_done_; std::queue\u0026lt;int\u0026gt; tasks_; std::mutex tasks_mutex_; IRunnable *runnable_{}; void func(); std::condition_variable *queue_condition_ = new std::condition_variable(); std::condition_variable *all_done_condition_ = new std::condition_variable(); int num_waiting_threads_; std::atomic\u0026lt;int\u0026gt; num_tasks_remaining_; std::mutex all_done_mutex_; }; tasksys.cpp\n/* * ================================================================ * Parallel Thread Pool Sleeping Task System Implementation * ================================================================ */ const char *TaskSystemParallelThreadPoolSleeping::name() { return \u0026#34;Parallel + Thread Pool + Sleep\u0026#34;; } void TaskSystemParallelThreadPoolSleeping::func() { int task_id; while (!exit_flag_) { task_id = -1; while (task_id == -1) { std::unique_lock\u0026lt;std::mutex\u0026gt; lk(tasks_mutex_); // 等待任务 queue_condition_-\u0026gt;wait(lk, [] { return 1; }); if (exit_flag_) { return; } if (!tasks_.empty()) { task_id = tasks_.front(); tasks_.pop(); } } runnable_-\u0026gt;runTask(task_id, num_tasks_); num_tasks_remaining_--; if (!num_tasks_remaining_) { // 通知主线程 // printf(\u0026#34;notify_all_done\\n\u0026#34;); all_done_condition_-\u0026gt;notify_one(); } else { // 通知其他线程 // printf(\u0026#34;notify_all\\n\u0026#34;); queue_condition_-\u0026gt;notify_one(); } } } TaskSystemParallelThreadPoolSleeping::TaskSystemParallelThreadPoolSleeping(int num_threads) : ITaskSystem(num_threads) { // // TODO: CS149 student implementations may decide to perform setup // operations (such as thread pool construction) here. // Implementations are free to add new class member variables // (requiring changes to tasksys.h). // exit_flag_ = false; for (int i = 0; i \u0026lt; num_threads; i++) { threads.emplace_back(\u0026amp;TaskSystemParallelThreadPoolSleeping::func, this); } } TaskSystemParallelThreadPoolSleeping::~TaskSystemParallelThreadPoolSleeping() { // // TODO: CS149 student implementations may decide to perform cleanup // operations (such as thread pool shutdown construction) here. // Implementations are free to add new class member variables // (requiring changes to tasksys.h). // exit_flag_ = true; queue_condition_-\u0026gt;notify_all(); for (auto \u0026amp;thread : threads) { thread.join(); } } void TaskSystemParallelThreadPoolSleeping::run(IRunnable *runnable, int num_total_tasks) { // // TODO: CS149 students will modify the implementation of this // method in Parts A and B. The implementation provided below runs all // tasks sequentially on the calling thread. // runnable_ = runnable; num_tasks_ = num_total_tasks; num_tasks_remaining_ = num_total_tasks; tasks_mutex_.lock(); for (int i = 0; i \u0026lt; num_total_tasks; i++) { tasks_.push(i); } tasks_mutex_.unlock(); // 通知其他线程 queue_condition_-\u0026gt;notify_all(); // printf(\u0026#34;run\\n\u0026#34;); while (num_tasks_remaining_) { std::unique_lock\u0026lt;std::mutex\u0026gt; lk2(all_done_mutex_); all_done_condition_-\u0026gt;wait(lk2, [] { return 1; }); } // printf(\u0026#34;all done\\n\u0026#34;); // printf(\u0026#34;all done\\n\u0026#34;); } 结果分析:\nsleep对spin的提升效果不明显，可能是因为任务太少，线程切换的开销比较大.\n运行结果:\n================================================================================ Running task system grading harness... (11 total tests) - Detected CPU with 16 execution contexts - Task system configured to use at most 8 threads ================================================================================ ================================================================================ Executing test: super_super_light... Reference binary: ./runtasks_ref_linux Results for: super_super_light STUDENT REFERENCE PERF? [Serial] 5.281 5.788 0.91 (OK) [Parallel + Always Spawn] 95.221 92.995 1.02 (OK) [Parallel + Thread Pool + Spin] 10.877 10.446 1.04 (OK) [Parallel + Thread Pool + Sleep] 6.943 42.705 0.16 (OK) ================================================================================ Executing test: super_light... Reference binary: ./runtasks_ref_linux Results for: super_light STUDENT REFERENCE PERF? [Serial] 37.497 37.844 0.99 (OK) [Parallel + Always Spawn] 108.136 108.805 0.99 (OK) [Parallel + Thread Pool + Spin] 10.777 13.615 0.79 (OK) [Parallel + Thread Pool + Sleep] 10.274 44.686 0.23 (OK) ================================================================================ Executing test: ping_pong_equal... Reference binary: ./runtasks_ref_linux Results for: ping_pong_equal STUDENT REFERENCE PERF? [Serial] 603.419 606.739 0.99 (OK) [Parallel + Always Spawn] 167.412 178.638 0.94 (OK) [Parallel + Thread Pool + Spin] 105.983 123.525 0.86 (OK) [Parallel + Thread Pool + Sleep] 108.243 148.316 0.73 (OK) ================================================================================ Executing test: ping_pong_unequal... Reference binary: ./runtasks_ref_linux Results for: ping_pong_unequal STUDENT REFERENCE PERF? [Serial] 1126.19 1109.329 1.02 (OK) [Parallel + Always Spawn] 259.271 260.822 0.99 (OK) [Parallel + Thread Pool + Spin] 199.088 198.013 1.01 (OK) [Parallel + Thread Pool + Sleep] 198.777 214.293 0.93 (OK) ================================================================================ Executing test: recursive_fibonacci... Reference binary: ./runtasks_ref_linux Results for: recursive_fibonacci STUDENT REFERENCE PERF? [Serial] 1052.273 1128.069 0.93 (OK) [Parallel + Always Spawn] 156.014 172.113 0.91 (OK) [Parallel + Thread Pool + Spin] 156.31 171.337 0.91 (OK) [Parallel + Thread Pool + Sleep] 156.462 166.476 0.94 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop STUDENT REFERENCE PERF? [Serial] 411.426 423.96 0.97 (OK) [Parallel + Always Spawn] 537.747 532.353 1.01 (OK) [Parallel + Thread Pool + Spin] 99.286 104.844 0.95 (OK) [Parallel + Thread Pool + Sleep] 95.817 239.76 0.40 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop_fewer_tasks... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop_fewer_tasks STUDENT REFERENCE PERF? [Serial] 413.681 415.961 0.99 (OK) [Parallel + Always Spawn] 514.021 505.234 1.02 (OK) [Parallel + Thread Pool + Spin] 108.644 117.702 0.92 (OK) [Parallel + Thread Pool + Sleep] 106.84 260.724 0.41 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop_fan_in... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop_fan_in STUDENT REFERENCE PERF? [Serial] 212.534 211.52 1.00 (OK) [Parallel + Always Spawn] 76.402 76.09 1.00 (OK) [Parallel + Thread Pool + Spin] 37.203 39.662 0.94 (OK) [Parallel + Thread Pool + Sleep] 36.523 57.039 0.64 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop_reduction_tree... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop_reduction_tree STUDENT REFERENCE PERF? [Serial] 208.076 207.488 1.00 (OK) [Parallel + Always Spawn] 45.054 45.227 1.00 (OK) [Parallel + Thread Pool + Spin] 33.079 33.9 0.98 (OK) [Parallel + Thread Pool + Sleep] 34.502 38.389 0.90 (OK) ================================================================================ Executing test: spin_between_run_calls... Reference binary: ./runtasks_ref_linux Results for: spin_between_run_calls STUDENT REFERENCE PERF? [Serial] 353.553 382.373 0.92 (OK) [Parallel + Always Spawn] 180.401 197.119 0.92 (OK) [Parallel + Thread Pool + Spin] 205.374 222.315 0.92 (OK) [Parallel + Thread Pool + Sleep] 214.819 197.579 1.09 (OK) ================================================================================ Executing test: mandelbrot_chunked... Reference binary: ./runtasks_ref_linux Results for: mandelbrot_chunked STUDENT REFERENCE PERF? [Serial] 257.289 256.815 1.00 (OK) [Parallel + Always Spawn] 34.395 34.058 1.01 (OK) [Parallel + Thread Pool + Spin] 34.241 34.922 0.98 (OK) [Parallel + Thread Pool + Sleep] 35.191 35.273 1.00 (OK) ================================================================================ Overall performance results [Serial] : All passed Perf [Parallel + Always Spawn] : All passed Perf [Parallel + Thread Pool + Spin] : All passed Perf [Parallel + Thread Pool + Sleep] : All passed Perf part_b 在任务的 B 部分中，您将扩展您的 A 部分任务系统实现，以支持可能依赖于以前任务的任务的异步启动。这些任务间依赖关系创建了任务执行库必须遵守的调度约束。\nITaskSystem 接口还有一个方法:\nvirtual TaskID runAsyncWithDeps(IRunnable* runnable, int num_total_tasks, const std::vector\u0026lt;TaskID\u0026gt;\u0026amp; deps) = 0; RunAsyncWithDeps ()类似于 run () ，因为它也用于执行 num total asks 任务的批量启动。但是，它与 run ()在许多方面有所不同\u0026hellip;\nAsynchronous Task Launch 首先，使用 runAsyncWithDeps ()创建的任务由任务系统与调用线程异步执行。\n这意味着 runAsyncWithDeps ()应该立即返回给调用方，即使任务尚未完成执行。\n该方法返回与此批量任务启动关联的唯一标识符。\n调用线程可以通过调用 sync ()来确定大容量任务启动的实际完成时间。\nvirtual void sync() = 0;\n只有当与之前所有批量任务启动关联的任务完成时，sync ()才返回给调用方。例如，考虑以下代码:\n// assume taskA and taskB are valid instances of IRunnable... std::vector\u0026lt;TaskID\u0026gt; noDeps; // empty vector ITaskSystem *t = new TaskSystem(num_threads); // bulk launch of 4 tasks TaskID launchA = t-\u0026gt;runAsyncWithDeps(taskA, 4, noDeps); // bulk launch of 8 tasks TaskID launchB = t-\u0026gt;runAsyncWithDeps(taskB, 8, noDeps); // at this point tasks associated with launchA and launchB // may still be running t-\u0026gt;sync(); // at this point all 12 tasks associated with launchA and launchB // are guaranteed to have terminated 如上面的注释中所述，在线程调用sync() runAsyncWithDeps() ) 的任务已完成。 准确地说， runAsyncWithDeps()告诉您的任务系统执行新的批量任务启动，但您的实现可以灵活地在下次调用sync()之前随时执行这些任务。 请注意，此规范意味着无法保证您的实现在从 launchB 启动任务之前先执行 launchA 中的任务！\nSupport for Explicit Dependencies runAsyncWithDeps()的第二个关键细节是它的第三个参数：TaskID 标识符向量，必须引用之前使用runAsyncWithDeps()启动的批量任务。 该向量指定当前批量任务启动中的任务所依赖的先前任务。 因此，在依赖向量中给出的启动中的所有任务完成之前，您的任务运行时无法开始执行当前批量任务启动中的任何任务！ 例如，考虑以下示例：\nstd::vector\u0026lt;TaskID\u0026gt; noDeps; // empty vector std::vector\u0026lt;TaskID\u0026gt; depOnA; std::vector\u0026lt;TaskID\u0026gt; depOnBC; ITaskSystem *t = new TaskSystem(num_threads); TaskID launchA = t-\u0026gt;runAsyncWithDeps(taskA, 128, noDeps); depOnA.push_back(launchA); TaskID launchB = t-\u0026gt;runAsyncWithDeps(taskB, 2, depOnA); TaskID launchC = t-\u0026gt;runAsyncWithDeps(taskC, 6, depOnA); depOnBC.push_back(launchB); depOnBC.push_back(launchC); TaskID launchD = t-\u0026gt;runAsyncWithDeps(taskD, 32, depOnBC); t-\u0026gt;sync(); 上面的代码有四个批量任务启动（taskA：128 个任务，taskB：2 个任务，taskC：6 个任务，taskD：32 个任务）。 请注意，任务 B 和任务 C 的启动取决于任务 A。 taskD 的批量启动 ( launchD ) 取决于launchB和launchC的结果。 因此，虽然您的任务运行时可以按任意顺序（包括并行）处理与launchB和launchC关联的任务，但这些启动中的所有任务必须在launchA的任务完成后开始执行，并且它们必须在运行时开始之前完成从launchD执行任何任务。\n我们可以通过任务图直观地说明这些依赖关系。 任务图是有向无环图 (DAG)，其中图中的节点对应于批量任务启动，从节点 X 到节点 Y 的边表示 Y 对 X 输出的依赖关系。上述代码的任务图是： 请注意，如果您在具有八个执行上下文的 Myth 计算机上运行上面的示例，则并行调度launchB和launchC中的任务的能力可能非常有用，因为单独的批量任务启动都不足以使用所有执行机器的资源。\nTask 您必须从 A 部分扩展任务系统实现，才能正确实现TaskSystem::runAsyncWithDeps()和TaskSystem::sync() 。 与 A 部分一样，我们为您提供以下入门提示：\nIt may be helpful to think about the behavior of runAsyncWithDeps() as pushing a record corresponding to the bulk task launch, or perhaps records corresponding to each of the tasks in the bulk task launch onto a \u0026ldquo;work queue\u0026rdquo;. Once the record to work to do is in the queue, runAsyncWithDeps() can return to the caller. The trick in this part of the assignment is performing the appropriate bookkeeping to track dependencies. What must be done when all the tasks in a bulk task launch complete? (This is the point when new tasks may become available to run.) It can be helpful to have two data structures in your implementation: (1) a structure representing tasks that have been added to the system via a call to runAsyncWithDeps(), but are not yet ready to execute because they depend on tasks that are still running (these tasks are \u0026ldquo;waiting\u0026rdquo; for others to finish) and (2) a \u0026ldquo;ready queue\u0026rdquo; of tasks that are not waiting on any prior tasks to finish and can safely be run as soon as a worker thread is available to process them. You need not worry about integer wrap around when generating unique task launch ids. We will not hit your task system with over 2^31 bulk task launches. You can assume all programs will either call only run() or only runAsyncWithDeps(); that is, you do not need to handle the case where a run() call needs to wait for all proceeding calls to runAsyncWithDeps() to finish. 在part_b/子目录中实现B部分实现，以与正确的参考实现（ part_b/runtasks_ref_* ）进行比较。\n","permalink":"/posts/cmu-15418cs-618/asst2/","summary":"C++ Sync thread的使用 #include \u0026lt;thread\u0026gt; #include \u0026lt;stdio.h\u0026gt; void my_func(int thread_id, int num_threads) { printf(\u0026#34;Hello from spawned thread %d of %d\\n\u0026#34;, thread_id, num_threads); } int main(int argc, char** argv) { std::thread t0 = std::thread(my_func, 0, 2); std::thread t1 = std::thread(my_func, 1, 2); printf(\u0026#34;The main thread is running concurrently with spawned threads.\\n\u0026#34;); t0.join(); t1.join(); printf(\u0026#34;Spawned threads have terminated at this point.\\n\u0026#34;); return 0; }","title":"asst2"},{"content":"多阶段构建docker镜像 多阶段构建的修改不会保留到下一阶段，只有COPY和ADD命令会保留到下一阶段\nusages：\n第一阶段：编译/打包程序依赖 多阶段用途：\n缩小镜像体积 新系统build时出现Cannot autolaunch D-Bus without X11 $DISPLAY docker 拉取包时需要登录.\n问题出在Linux缺少一个密码管理包gnupg，它用于加密，我们在登录时需要这个包将密码加密后才能完成，因此直接安装\nsudo apt install gnupg2 pass ","permalink":"/posts/tools/docker/docker/","summary":"多阶段构建docker镜像 多阶段构建的修改不会保留到下一阶段，只有COPY和ADD命令会保留到下一阶段 usages： 第一阶段：编译/打包程序","title":"docker Usage"},{"content":"docker中的npm # 设置npm源 npm config set registry https://registry.npm.taobao.org ","permalink":"/posts/tools/npm/npm/","summary":"docker中的npm # 设置npm源 npm config set registry https://registry.npm.taobao.org","title":"npm"},{"content":"无法连接 Q:\nopenai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host=\u0026#39;api.openai.com\u0026#39;, port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError(\u0026#39;Unable to connect to proxy\u0026#39;, SSLError(SSLZeroReturnError(6, \u0026#39;TLS/SSL connection has been closed (EOF) (_ssl.c:1131)\u0026#39;)))) A:\n问题出在模块 urllib3 的版本，报错的是 1.26.3，没报错的是 1.25.11 在原报错环境中使用下面命令重装低版本 urllib3： pip install urllib3==1.25.11 然后测试果然就没问题了。 ","permalink":"/posts/qa/python/openai/","summary":"无法连接 Q: openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host=\u0026#39;api.openai.com\u0026#39;, port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError(\u0026#39;Unable to connect to proxy\u0026#39;, SSLError(SSLZeroReturnError(6, \u0026#39;TLS/SSL connection has been closed (EOF) (_ssl.c:1131)\u0026#39;)))) A: 问题出在模块 urllib3 的版本，报错的是 1.26.3，没报错的是 1.25.11 在原报错环境中使","title":"openai 相关QA"},{"content":"锁 unique_lock 和 lock_guard unique_lock 和 lock_guard 都是 RAII 的封装，都是用来管理 mutex 的，但是 unique_lock 比 lock_guard 更加灵活，可以随时 unlock 和 lock，而 lock_guard 只能在构造的时候 lock，在析构的时候 unlock。\nunique_lock:\n```cpp std::mutex mtx; std::unique_lock\u0026lt;std::mutex\u0026gt; lck(mtx); lck.unlock(); lck.lock(); ``` lock_guard:\n```cpp std::mutex mtx; std::lock_guard\u0026lt;std::mutex\u0026gt; lck(mtx); ``` 和 condition_variable 使用时的区别 unique_lock 和 lock_guard 都可以和 condition_variable 一起使用，但是 unique_lock 更加灵活，可以随时 unlock 和 lock，而 lock_guard 只能在构造的时候 lock，在析构的时候 unlock。\nunique_lock:\n```cpp std::mutex mtx; std::condition_variable cv; std::unique_lock\u0026lt;std::mutex\u0026gt; lck(mtx); cv.wait(lck); /* 这部分仍然被锁住 */ lck.unlock(); lck.lock(); ``` lock_guard:\n```cpp std::mutex mtx; std::condition_variable cv; std::lock_guard\u0026lt;std::mutex\u0026gt; lck(mtx); cv.wait(lck); /* 这部分已经被解锁 */ ``` ","permalink":"/posts/c++/%E5%A4%9A%E7%BA%BF%E7%A8%8B/","summary":"锁 unique_lock 和 lock_guard unique_lock 和 lock_guard 都是 RAII 的封装，都是用来管理 mutex 的，但是 unique_lock 比 lock_guard 更加灵活，可以随时 unlock 和 lock，而 lock_guard 只能在构造的时候 lock，在析构的时候 unloc","title":"多线程"},{"content":"markdown 快捷键 删除线: alt + s\n待办事项勾选/取消勾选: alt + c\nterminal 命令行创建: Ctrl + Shift + ~\n命令行切换: Ctrl + fn + upArrow/downArrow\nview Run Run python file in terminal: Ctrl + F5\n","permalink":"/posts/tools/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%85%A8/","summary":"markdown 快捷键 删除线: alt + s 待办事项勾选/取消勾选: alt + c terminal 命令行创建: Ctrl + Shift + ~ 命令行切换: Ctrl + fn + upArrow/downArrow view Run Run python file in terminal: Ctrl + F5","title":"工作学习流(vscode快捷键)"},{"content":"打包 mvn clean package -Dmaven.test.skip=true 常见问题 找不到主类 Error: Could not find or load main class com.xxx.xxx.xxx Caused by: java.lang.ClassNotFoundException: com.xxx.xxx.xxx 解决方法：在pom.xml中添加如下配置\n","permalink":"/posts/tools/maven/maven/","summary":"打包 mvn clean package -Dmaven.test.skip=true 常见问题 找不到主类 Error: Could not find or load main class com.xxx.xxx.xxx Caused by: java.lang.ClassNotFoundException: com.xxx.xxx.xxx 解决方法：在pom.xml中添加如下配置","title":"maven"},{"content":"nginx安装 1. 安装依赖 yum install -y gcc gcc-c++ autoconf automake make yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 2. 转发后端图片 # 1. 创建目录 mkdir -p /data/nginx/cache # 2. 修改目录权限 chown -R nginx:nginx /data/nginx/cache ","permalink":"/posts/tools/nginx/nginx/","summary":"nginx安装 1. 安装依赖 yum install -y gcc gcc-c++ autoconf automake make yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 2. 转发后端图片 # 1. 创建目录 mkdir -p /data/nginx/cache # 2. 修改目录权限 chown -R nginx:nginx /data/nginx/cache","title":"nginx"},{"content":"常用命令 # 连接 mongosh ip[:port]/database -u username -p password # 查看数据库 show dbs # 切换数据库 use database # 查看集合 show collections # file # 查看集合数据 db.{collection}.find() # 按条件查看集合数据 ## pid=1 db.{collection}.find({pid:1}) ## 限制4条 db.{collection}.find().limit(4) ## 只显示其中一个字段 db.{collection}.find({}, {name:1}) ## 统计数量 db.{collection}.find().count() ## 全部删除 db.{collection}.remove({}) ## 插入或更新数据 ","permalink":"/posts/tools/sql/mongodb/","summary":"常用命令 # 连接 mongosh ip[:port]/database -u username -p password # 查看数据库 show dbs # 切换数据库 use database # 查看集合 show collections # file # 查看集合数据 db.{collection}.find() # 按条件查看集合数据 ## pid=1 db.{collection}.find({pid:1}) ## 限制4条 db.{collection}.find().limit(4) ## 只显示其","title":"mongoDB"},{"content":"一、什么是I/O模型 及 I/O模型的分类 二、I/O 多路复用 三、实际应用 Reactor模式 Proactor模式 事件驱动模式 ","permalink":"/posts/knowledge/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BC%96%E7%A8%8B/io%E6%A8%A1%E5%9E%8B/","summary":"一、什么是I/O模型 及 I/O模型的分类 二、I/O 多路复用 三、实际应用 Reactor模式 Proactor模式 事件驱动模式","title":"I/O模型"},{"content":"正在读的 待读的 现在很多计算机学院也开始使用《计算机网络:自顶向下方法》这本不错的教材，如果没有看过的可以看下。当然还是那句话你一定要看懂而不是看完。比如三次握手和四次挥手的细节，你一定要很清楚。 然后你就可以找一本网络编程的实战书来看下，如果你没有使用任何socket api编程的经验，你可以看看韩国人尹圣雨写的这本《TCP/IP网络编程》，这本书从基础的socket api介绍到比较高级的io复用技术，有非常详细和生动的例子。如果你是初级水平，强烈建议看看这本书。网络编程的细节需要注意的地方实在太多了，这本书上都有介绍。很多人尤其是一些学生，写了一些可以相互聊天的小程序就觉得自己熟悉网络通信了，但是这类程序拿到互联网上或者离开局域网，不是连接出错，就是数据总是收发不全。我当年也是这么过来的，看看这本书，你就能明白许多网络故障的原因。\n等你有了一定的网络编程以后（熟练使用常见socket API），你可以看看游双的《Linux高性能服务器编程》，这本书给没有基础的人或者基础不扎实的人的感觉是，尤其是书的前三章，这书怎么这么垃圾，又把网络理论书上面的东西搬过来凑字数，但是如果你有基础再按照书上的步骤在机器上实践一遍，你会发现，真是一本难得的、良心的书，桃李不言下自成蹊吧。\n如果你掌握了这本说上说的这些知识，你再看陈硕老师的《Linux多线程服务端编程》或者去看像libevent这样的开源网络库，你会进一步的得到提升。这也是我学习网络编程的一些经验和经历吧。\n","permalink":"/posts/book/%E5%BE%85%E8%AF%BB%E4%B9%A6%E7%9B%AE%E5%BD%95/","summary":"正在读的 待读的 现在很多计算机学院也开始使用《计算机网络:自顶向下方法》这本不错的教材，如果没有看过的可以看下。当然还是那句话你一定要看懂而不","title":"读书目录"},{"content":"# 查看环境 conda env list # 创建环境 conda create -n py3 python=3.6 # 通过yml文件创建环境 conda env create -f environment.yml # 激活环境 conda activate py3 # 退出环境 conda deactivate # 删除环境 conda remove -n py3 --all 迁移时可能会出现pip问题 可以在yml的pip:上面加上pip\nname: py3 channels: - defaults dependencies: - python=3.6 - pip - pip: - -r requirements.txt ","permalink":"/posts/tools/python/conda/","summary":"# 查看环境 conda env list # 创建环境 conda create -n py3 python=3.6 # 通过yml文件创建环境 conda env create -f environment.yml # 激活环境 conda activate py3 # 退出环境 conda deactivate # 删除环境 conda remove -n py3 --all 迁移时可能会出现pi","title":"conda"},{"content":"记把深度学习项目装入docker 安装时出现选项\n# RUN apt-get install libglib2.0-dev -y # 由于安装libglib2.0-dev的时候，bash会有交互操作叫你选择对应的时区，在docker build的时候没有交互的，所以需要加上DEBIAN_FRONTEND=\u0026#34;noninteractive\u0026#34; RUN DEBIAN_FRONTEND=\u0026#34;noninteractive\u0026#34; apt -y install libglib2.0-dev docker清理 在win10下，docker是基于wsl2的，所以docker的镜像和容器都是在wsl2的文件系统中。 所以在清理完docker的镜像和容器后，需要对wsl的盘进行压缩。\n# 停止所有的容器 docker stop $(docker ps -aq) # 删除所有未使用的容器 docker volume prune # 删除所有未使用的镜像 docker image prune -a # 删除缓存 docker builder prune # 查看当前占用的空间 docker system df 对wsl2的盘进行压缩\nwsl --shutdown # 查看wsl2的盘 wsl --list -v # 使用diskpart压缩 diskpart # open window Diskpart select vdisk file=\u0026#34;D:\\ubuntu\\wsl\\docker-desktop-data\\ext4.vhdx\u0026#34; attach vdisk readonly compact vdisk detach vdisk exit docker中安装conda # 安装conda RUN apt-get install -y wget # yhyu13 : donwload anaconda package \u0026amp; install RUN wget \u0026#34;https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh\u0026#34; RUN sh Anaconda3-2023.03-1-Linux-x86_64.sh -b -p /opt/conda # RUN rm /anaconda.sh RUN ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh RUN echo \u0026#34;. /opt/conda/etc/profile.d/conda.sh\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # yhyu13 : add conda to path ENV PATH /opt/conda/bin:/opt/conda/condabin:$PATH docker-compose 使用gpu version: \u0026#39;3.7\u0026#39; services: pytorch: build: . runtime: nvidia environment: - NVIDIA_VISIBLE_DEVICES=all - NVIDIA_DRIVER_CAPABILITIES=all volumes: - .:/workspace ports: - \u0026#34;8888:8888\u0026#34; - \u0026#34;6006:6006\u0026#34; command: bash -c \u0026#34;jupyter notebook --ip wsl 盘迁移到非系统盘 一般情况下 wsl盘的位置在 C:\\Users\\\u0026lt;用户名\u0026gt;\\AppData\\Local\\Docker\\wsl\ndocker的盘在 C:\\Users\\\u0026lt;用户名\u0026gt;\\AppData\\Local\\Docker\\wsl\\data\n# 1. 停止wsl wsl --shutdown # 2. 查看wsl状态 wsl --list -v # 可以看到docker有两个wsl，一个是docker-desktop-data，一个是docker-desktop # 只需要迁移docker-desktop-data即可,另一个很小 # 3. 迁移wsl wsl --export Ubuntu-20.04 D:\\ubuntu\\wsl\\Ubuntu-20.04.tar # 4. 删除wsl wsl --unregister Ubuntu-20.04 # 5. 查看是否删除成功 wsl --list -v # 6. 导入wsl wsl --import Ubuntu-20.04 D:\\ubuntu\\wsl\\Ubuntu-20.04 D:\\ubuntu\\wsl\\Ubuntu-20.04.tar --version 2 # 7. 查看是否导入成功 wsl --list -v docker 中设置特定版本的python # 创建一个基础镜像 FROM ubuntu:20.04 # 重置apt-get RUN rm -rf /etc/apt/sources.list # 安装conda # yhyu13 : install additional packages # 设置apt的源为tsinghua镜像源 RUN sed -i \u0026#39;s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apt/sources.list RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl wget # 安装conda RUN curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\ \u0026amp;\u0026amp; bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\ \u0026amp;\u0026amp; rm Miniconda3-latest-Linux-x86_64.sh # 创建conda环境并安装python RUN /opt/conda/bin/conda create -n py38 python=3.8.5 ENV PATH /opt/conda/envs/py38/bin:$PATH docker中使用display 在启动时需要设置环境变量DISPLAY\nwin下的情况 参考在Docker for Windows中运行GUI程序\n前后端项目静态资源转发 后端 springboot时： 把静态资源放在static目录下，然后在application中配置\nspring: mvc: static-path-pattern: /static/** resources: static-locations: classpath:/static/ 如果设置了拦截器，需要在拦截器上加入\n@Configuration public class WebMvcConfig implements WebMvcConfigurer { @Bean public CorsInterceptor corsInterceptor() { return new CorsInterceptor(); } @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(corsInterceptor()) .addPathPatterns(\u0026#34;/**\u0026#34;); } @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\u0026#34;/static/**\u0026#34;).addResourceLocations(\u0026#34;classpath:/static/\u0026#34;, \u0026#34;file:static/\u0026#34;); } } jar 包和静态路径关系\n- .jar - static 前端，需要在nginx上加入转发后访问静态路径后缀。\nlocation /api { rewrite ^/api(.*) $1 break; proxy_pass $SERVER_URL; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Photo $scheme; location ~*.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt|swf|xml|woff|woff2|ttf|eot|svg)$ { rewrite ^/api(.*) $1 break; proxy_pass $SERVER_URL; proxy_redirect off; } } ","permalink":"/posts/tips/docker/","summary":"记把深度学习项目装入docker 安装时出现选项 # RUN apt-get install libglib2.0-dev -y # 由于安装libglib2.0-dev的时候，bash会有交互操作叫你选择对应的时","title":"docker相关技巧"},{"content":"ISPC language SPMD: Single Program Multiple Data 一种花哨的方式来说，就是一种并行编程的范式，它的特点是：在编程时，我们只需要写一个程序，然后在运行时，这个程序会被复制多份，每一份都会被分配到不同的处理器上去执行，这样就实现了并行。\n用ISPC实现sinx\nexport void sinx(uniform int N, uniform int terms,uniform float* x, uniform float* result){ //assume N % programCount == 0 for (uniform int i = 0; i \u0026lt; N;i += programCount){ int idx = i + programIndex; float value = x[idx]; float number = value * value * value; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 0; j \u0026lt; terms; j++){ value += sign * number / denom; number *= value * value; denom *= (2 * j + 3) * (2 * j + 2); sign *= -1; } result[idx] = value; } } 使用C++来调用 调用ISPC的东西是个程序实例的集合, gang.\n#include \u0026lt;iostream\u0026gt; #include \u0026#34;sinx.ispc.h\u0026#34; int N = 1000000; int terms = 10; float* x = new float[N]; float* result = new float[N]; //init x //execute ispc::sinx(N,terms,x,result); ispc中不需要手动设置programCount,programIndex\nprogramCount: number of simultaneous program instances in the gang (uniform value)\nprogramIndex: id of the current program instance in the gang(a non-uniform value)\nuniform value: 一个值在gang中的所有实例中都是一样的\n如果在ispc中直接使用sinx 并不会更快.\n因为有一些相同的工作会被重复做很多次. 通过分离他们,可以减少重复计算的次数,从而提高效率.\n一个设想的实现方法如下: ISPC是为了更容易编写SIMD代码而设计的, 只需要通过特殊的宏或编译指示就可以使用SIMD指令.\nprogramCount 就是 向量宽度\nSPMD programming abstraction\nISPC compiler generates SIMD implementation\nversion2版本的代码,这是分块进行而不是交错的.\nexport void sinx(uniform int N, uniform int terms,uniform float* x, uniform float* result){ uniform int count = N / programCount; int start = programIndex * count; //assume N % programCount == 0 for (uniform int i = 0; i \u0026lt; count;i += 1){ int idx = start + i; float value = x[idx]; float number = value * value * value; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 0; j \u0026lt; terms; j++){ value += sign * number / denom; number *= value * value; denom *= (2 * j + 3) * (2 * j + 2); sign *= -1; } result[idx] = value; } } 交错通常比分块更好,因为分块会导致数据的访问不连续. 当计算量不均匀时,分块会导致一些处理器的负载过重,而另一些处理器的负载过轻.\n并且因为是同时进行的, 交错可以访问邻近的数据,这样可以增加cache的命中率.\n根本原因: 矢量加载指令(寄存器)是一次加载多个数据,如果在很短的时间内,要加载的数据是连续的,那么就可以一次加载多个数据,如果数据是不连续的,那么就需要多次加载,这样就会降低效率. 如果有个聪明的编译器,它可以自动将分块的代码转换为交错的代码,这样就可以兼顾两者的优点.\nforeach就可以实现这个功能,让程序员不需要关心这些细节.\nexport void sinx(uniform int N, uniform int terms,uniform float* x, uniform float* result){ foreach(i = 0 ... N){ float value = x[i]; float number = value * value * value; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 0; j \u0026lt; terms; j++){ value += sign * number / denom; number *= value * value; denom *= (2 * j + 3) * (2 * j + 2); sign *= -1; } result[i] = value; } } ISPC的错误例子:\nexport uniform float sumall(uniform int N, uniform float* x){ uniform float sum = 0; foreach(i = 0 ... N){ sum += x[i]; } return sum; } 错误:编译器会报错,因为sum是一个uniform value,它在所有的实例中都是一样的,但是在foreach中,每个实例都会对sum进行修改,这样就会导致错误.\n修正这个错误:\nexport uniform float sumall(uniform int N, uniform float* x){ uniform float sum = 0; float partial_sum = 0; foreach(i = 0 ... N){ partial_sum += x[i]; } sum = reduce_add(partial_sum); return sum; } reduce_add原语: 允许将一组不同的值合并为一个值,这个值在所有的实例中都是一样的.\n编译后的细节 ISPC tasks: 基本上就是一个线程,但是它可以被分配到不同的处理器上去执行.\n三种并行编程范式 和 三种 machine architecture 聚焦于 communication 和 cooperation\n使用pthread时要call operate system 而在ISPC中,只需要call compiler\nThree models of communication(abstraction) 1.Shared address space asst3中会用到\n多个线程之间通过互斥锁来进行通信\n在硬件中, Dance-hall model 所有处理器在同一侧.\nSymmetric Multiprocessor(SMP) system 就是如此\n最简单的方式是总线, 但这样无法扩展,因为总线的带宽是有限的. 但实际中: 还有一种访问本地内存的方式,就是通过cache,这样就可以减少对总线的访问,从而提高效率. Non-Uniform Memory Access(NUMA) system 但它为程序员引入的复杂性是很大的,因为程序员需要手动的将数据放到本地内存中,这样才能提高效率.\nshared address space的优点:\n程序员不需要关心数据的传输 程序员不需要关心数据的分布 2.Message passing aasst4中会用到\n由于实现缓存一致性需要额外的成本，因此在大型系统中，共享内存的实现是不可行的。在这种情况下，消息传递是一种更好的选择。\n在消息传递中，每个处理器都有自己的私有内存，而且没有共享内存。要在处理器之间传递数据，必须使用显式的消息传递原语。\n不需要任何硬件支持，因此可以在任何系统上实现。只需要网络。\n可以构建大型系统，因为没有共享内存的限制。\n这些原语允许程序员在处理器之间传递数据，但是程序员必须显式地指定数据的传输。这种方式的缺点是，程序员需要关心数据的传输，这样就会增加程序员的负担。\n3.Data parallel asst2中会用到\n上面两种方式可以在任何硬件上实现。\nData parallel对程序员来说是最简单的，因为程序员不需要关心数据的传输，也不需要关心数据的分布。但是，它只能在特定的硬件上实现，因为它需要硬件支持。\n过去我们使用SIMD，现在使用SPMD。\n并行程序的问题\n这样的并行会得到不确定的结果。\n那么如何有原则性地使用并行呢？\n有一个抽象概念是stream，可以避免并行竞争问题。\n两个函数间的用法：\n当如果使用stream，就必须创建tmp。不得不把临时数据写入浪费的带宽中。\n所以我们希望也许有一些新的运算符可以做更加高级的操作。\ngather: 将数据从不同的stream中收集到一个stream中。 scatter: 将数据从一个stream中分散到不同的stream中。\nintel包括了gather，但不包括scatter。 总结 这些并不是完全独立的，而是可以组合使用的。\n通常在实践中为了得到最好的性能，会使用以上所有的方式。\n多核芯片内部通常是shared address space，但小规模情况下使用message passing。\n","permalink":"/posts/cmu-15418cs-618/l3/","summary":"ISPC language SPMD: Single Program Multiple Data 一种花哨的方式来说，就是一种并行编程的范式，它的特点是：在编程时，我们只需要写一个程序，然后在运行时，这个程序会被复制多份，每","title":"Abstraction vs implementation"},{"content":"参考\n任务\nProgram 1: Parallel Fractal Generation Using Threads (20 points) 提示: 需要先看CMU15-418/CS149的L2再完成Pro1\n任务描述: 用多线程画mandelbrot fractal.\n代码中给出了串行的实现, 你需要实现多线程的版本.\n多线程版本中只需要修改 workerThreadStart函数. 不需要手动创建线程, 也不需要手动join线程. 直接调用mandelbrotThread().\n1.1 \u0026amp; 1.2, 计算在2,3,4,5,6,7,8,16,32个线程下的加速比 编写并观察 workerThreadStart函数的实现:\n345void workerThreadStart(WorkerArgs *const args) { // TODO FOR CS149 STUDENTS: Implement the body of the worker // thread here. Each thread should make a call to mandelbrotSerial() // to compute a part of the output image. For example, in a // program that uses two threads, thread 0 could compute the top // half of the image and thread 1 could compute the bottom half. // printf(\u0026#34;Hello world from thread %d\\n\u0026#34;, args-\u0026gt;threadId); double startTime = CycleTimer::currentSeconds(); // 每个线程负责的行数(除不尽的部分由最后一个线程负责) int height = args-\u0026gt;height / args-\u0026gt;numThreads; int startRow = args-\u0026gt;threadId * height; int numRows = height; if (args-\u0026gt;threadId == args-\u0026gt;numThreads - 1) { // 如果是最后一个线程，那么就要把除不尽的部分也算上 numRows = height + args-\u0026gt;height % args-\u0026gt;numThreads; } printf(\u0026#34;Thread %d startRow: %d, numRows: %d\\n\u0026#34;, args-\u0026gt;threadId, startRow, numRows); mandelbrotSerial(args-\u0026gt;x0, args-\u0026gt;y0, args-\u0026gt;x1, args-\u0026gt;y1, args-\u0026gt;width, args-\u0026gt;height, startRow, numRows, args-\u0026gt;maxIterations, args-\u0026gt;output); double endTime = CycleTimer::currentSeconds(); printf(\u0026#34;Thread %d time: %.3f ms\\n\u0026#34;, args-\u0026gt;threadId, (endTime - startTime) * 1000); } 结果:\n线程数 加速比 2 1.97 3 1.63 4 2.31 5 2.37 6 3.08 7 3.15 8 3.74 16 5.14 可以观察到，加速比和线程数并不是线性相关.\n猜测原因 猜测可能的原因有:\n线程通信的开销 每个线程分配的任务不均匀 1.3 查看每个线程的执行时间,验证猜想 当线程数为4时, 每个线程的执行时间如下: Thread 0 time: 63.974 ms Thread 3 time: 65.563 ms Thread 2 time: 259.972 ms Thread 1 time: 260.669 ms\n当线程数为8时, 每个线程的执行时间如下: Thread 0 time: 13.702 ms Thread 7 time: 16.831 ms Thread 1 time: 57.324 ms Thread 6 time: 61.069 ms Thread 5 time: 113.431 ms Thread 2 time: 115.753 ms Thread 4 time: 164.736 ms Thread 3 time: 166.306 ms\n可以看到,中间线程分配的任务更多,执行时间更长. 因此在增加线程数时,加速比并不是线性增加的.\n1.4 任务描述:\n解决上面的问题,使得加速比更接近线性. 如: 8线程时的加速比需要在7~8之间. 解决方法需要具有适用性, 适用所有的线程数. tips: 有一个非常简单的静态赋值可以实现这个目标，并且线程之间不需要通信/同步.\n解决方案 思路: 根据代码可知, 每行的计算是独立的, 因此可以将每行分配给不同的线程. 但由上面的实验可知,中间行的计算量比较大.\n因此我们不应该直接平均切分行, 而是以线程数量为步长,线程交叉依次分配行. 即 第i个线程分配k*n+i行.\nvoid workerThreadStart(WorkerArgs *const args) { // TODO FOR CS149 STUDENTS: Implement the body of the worker // thread here. Each thread should make a call to mandelbrotSerial() // to compute a part of the output image. For example, in a // program that uses two threads, thread 0 could compute the top // half of the image and thread 1 could compute the bottom half. // printf(\u0026#34;Hello world from thread %d\\n\u0026#34;, args-\u0026gt;threadId); double startTime = CycleTimer::currentSeconds(); /* 方案1 // 每个线程负责的行数(除不尽的部分由最后一个线程负责) int baseHeight = args-\u0026gt;height / args-\u0026gt;numThreads; int startRow = args-\u0026gt;threadId * baseHeight; int numRows = baseHeight; int yu = args-\u0026gt;height % args-\u0026gt;numThreads; // 均匀分配剩余行 if (args-\u0026gt;threadId \u0026lt; yu) { numRows++; } startRow += std::min(args-\u0026gt;threadId, yu); printf(\u0026#34;Thread %d startRow: %d, numRows: %d\\n\u0026#34;, args-\u0026gt;threadId, startRow, numRows); mandelbrotSerial(args-\u0026gt;x0, args-\u0026gt;y0, args-\u0026gt;x1, args-\u0026gt;y1, args-\u0026gt;width, args-\u0026gt;height, startRow, numRows, args-\u0026gt;maxIterations, args-\u0026gt;output); */ // 方案2, 依次分配行 int height = args-\u0026gt;height; for (int i = args-\u0026gt;threadId; i \u0026lt; height; i += args-\u0026gt;numThreads) { mandelbrotSerial(args-\u0026gt;x0, args-\u0026gt;y0, args-\u0026gt;x1, args-\u0026gt;y1, args-\u0026gt;width, args-\u0026gt;height, i, 1, args-\u0026gt;maxIterations, args-\u0026gt;output); } double endTime = CycleTimer::currentSeconds(); printf(\u0026#34;Thread %d time: %.3f ms\\n\u0026#34;, args-\u0026gt;threadId, (endTime - startTime) * 1000); } 输出结果:\nThread 3 time: 88.842 ms Thread 1 time: 89.680 ms Thread 0 time: 89.717 ms Thread 7 time: 90.280 ms Thread 5 time: 90.715 ms Thread 6 time: 90.743 ms Thread 2 time: 91.049 ms Thread 4 time: 92.982 ms [mandelbrot thread]: [93.318] ms Wrote image file mandelbrot-thread.ppm (7.10x speedup from 8 threads)\n上面的解决方案使得每个线程的执行时间基本相同,因此加速比接近线性. 在8线程时,加速比为7.1.\n1.5 16线程和8线程的加速比 现在16线程是否明显优于8线程? 给出是或否的原因. (6.45x speedup from 16 threads) 16线程并没有明显由于8线程,反而还更慢. 原因:\n电脑本身是4核, 超线程后是8线程. 16线程时线程切换反而导致开销增加. 总结 pro1的目的是为了认识到并行计算的overhead, 以及多线程在计算上也应该是依次交替分配的. 不能简单的平均分配.\npro1是通过垂直分割来实现并行计算. 而向量化是通过水平分割来实现并行计算.\nprogram-2-vectorizing-code-using-simd-intrinsics 前提: L2 任务描述： 使用SIMD指令(CS149intrin.h提供的),来实现clampedExpVector函数.\n示例函数:\nvoid absVector(float* values, float* output, int N) { __cs149_vec_float x; __cs149_vec_float result; __cs149_vec_float zero = _cs149_vset_float(0.f); __cs149_mask maskAll, maskIsNegative, maskIsNotNegative; // Note: Take a careful look at this loop indexing. This example // code is not guaranteed to work when (N % VECTOR_WIDTH) != 0. // Why is that the case? for (int i=0; i\u0026lt;N; i+=VECTOR_WIDTH) { // All ones maskAll = _cs149_init_ones(); // All zeros maskIsNegative = _cs149_init_ones(0); // Load vector of values from contiguous memory addresses _cs149_vload_float(x, values+i, maskAll); // x = values[i]; // Set mask according to predicate _cs149_vlt_float(maskIsNegative, x, zero, maskAll); // if (x \u0026lt; 0) { // Execute instruction using mask (\u0026#34;if\u0026#34; clause) _cs149_vsub_float(result, zero, x, maskIsNegative); // output[i] = -x; // Inverse maskIsNegative to generate \u0026#34;else\u0026#34; mask maskIsNotNegative = _cs149_mask_not(maskIsNegative); // } else { // Execute instruction (\u0026#34;else\u0026#34; clause) _cs149_vload_float(result, values+i, maskIsNotNegative); // output[i] = x; } // Write results back to memory _cs149_vstore_float(output+i, result, maskAll); } } 示例函数absVector并不能适用于所有情况,原因如下: 当n%VECTOR_WIDTH != 0时, 会越界.\n1\u0026amp;2 实现clampedExpVector函数 void clampedExpVector(float *values, int *exponents, float *output, int N) { // // CS149 STUDENTS TODO: Implement your vectorized version of // clampedExpSerial() here. // // Your solution should work for any value of // N and VECTOR_WIDTH, not just when VECTOR_WIDTH divides N // __cs149_vec_float one, nine; __cs149_vec_int zeroInt, oneInt; oneInt = _cs149_vset_int(1); zeroInt = _cs149_vset_int(0); one = _cs149_vset_float(1.f); nine = _cs149_vset_float(9.999999f); for (int i = 0; i \u0026lt; N; i += VECTOR_WIDTH) { __cs149_mask maskAll, maskIsZero, maskIsNotZero; __cs149_vec_float x; __cs149_vec_int y; // All ones maskAll = _cs149_init_ones(); // All zeros maskIsZero = _cs149_init_ones(0); // 防止在最后一次循环时，i+VECTOR_WIDTH超出N if (i + VECTOR_WIDTH \u0026gt; N) { maskAll = _cs149_init_ones(N - i); } // float x = values[i]; _cs149_vload_float(x, values + i, maskAll); // int y = exponents[i]; _cs149_vload_int(y, exponents + i, maskAll); // if (y == 0) _cs149_veq_int(maskIsZero, y, zeroInt, maskAll); // { // output[i] = 1.f; // } _cs149_vstore_float(output + i, one, maskIsZero); // else maskIsNotZero = _cs149_mask_not(maskIsZero); // 消除最后一次循环时，i+VECTOR_WIDTH超出N的情况 maskIsNotZero = _cs149_mask_and(maskIsNotZero, maskAll); { // float result = x; __cs149_vec_float result = x; // int count = y - 1; __cs149_vec_int count; _cs149_vsub_int(count, y, oneInt, maskIsNotZero); // 哪些count\u0026gt;0 __cs149_mask countMark; _cs149_vgt_int(countMark, count, zeroInt, maskIsNotZero); // while (count \u0026gt; 0) while (_cs149_cntbits(countMark) \u0026gt; 0) { // result *= x; _cs149_vmult_float(result, result, x, countMark); // count--; _cs149_vsub_int(count, count, oneInt, countMark); // 哪些count\u0026gt;0 _cs149_vgt_int(countMark, count, zeroInt, countMark); } // if (result \u0026gt; 9.999999f) __cs149_mask gtNineMask; _cs149_vgt_float(gtNineMask, result, nine, maskIsNotZero); // { reult = 9.999999f;} _cs149_vmove_float(result, nine, gtNineMask); // output[i] = result; _cs149_vstore_float(output + i, result, maskIsNotZero); } } } 通过init_ones来防止在有n%vectorWith!=0时 越界.\n在最开始的maskAll时设置 在取反码后也要设置一次 count循环: 通过设置一个mask来标记哪些count\u0026gt;0, 从而实现循环.\n修改vectorWidth为2, 4, 8, to 16来回答: Does the vector utilization increase, decrease or stay the same as VECTOR_WIDTH changes? Why?\nvectorWidth为2时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 2 Total Vector Instructions: 162728 Vector Utilization: 77.0% Utilized Vector Lanes: 250653 Total Vector Lanes: 325456\nvectorWidth为4时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 3 Total Vector Instructions: 119440 Vector Utilization: 72.2% Utilized Vector Lanes: 258879 Total Vector Lanes: 358320\nvectorWidth为8时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 8 Total Vector Instructions: 51628 Vector Utilization: 66.0% Utilized Vector Lanes: 272539 Total Vector Lanes: 413024\nvectorWidth为16时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 16 Total Vector Instructions: 26968 Vector Utilization: 64.2% Utilized Vector Lanes: 277188 Total Vector Lanes: 431488\n可以发现, 随着vectorWidth的增加, vectorUtilization也在减少.\n原因: 有多个条件语句,当vectorWidth增加时, 每次在某个条件中不执行的指令也会增加.\n3 实现arraySumVector float arraySumVector(float *values, int N) { // // CS149 STUDENTS TODO: Implement your vectorized version of arraySumSerial here // __cs149_vec_float sum = _cs149_vset_float(0.f); for (int i = 0; i \u0026lt; N; i += VECTOR_WIDTH) { __cs149_mask maskAll; __cs149_vec_float x; // All ones maskAll = _cs149_init_ones(); // 防止在最后一次循环时，i+VECTOR_WIDTH超出N if (i + VECTOR_WIDTH \u0026gt; N) { maskAll = _cs149_init_ones(N - i); } // float x = values[i]; _cs149_vload_float(x, values + i, maskAll); // sum += x; _cs149_vadd_float(sum, sum, x, maskAll); } float result = 0.f; // log2(VECTOR_WIDTH)内解决 for (int i = 0; i \u0026lt; log2(VECTOR_WIDTH); i++) { // 使用_cs149_hadd_float函数，将sum中的每两个元素相加 // 再使用_cs149_interleave_float函数，将sum中的每两个元素交叉放置 // 重复log2(VECTOR_WIDTH)次 _cs149_hadd_float(sum, sum); _cs149_interleave_float(sum, sum); } // 将sum中的第一个元素赋值给result result = sum.value[0]; return result; } 假设VECTOR_WIDTHs始终是N的因子.\n可以实现在O(N/VECTOR_WIDTH + log2(VECTOR_WIDTH))的时间内完成计算.\n最后的log2实现方式. 提示中给了两个函数 hadd: 将每两个元素相加 interleave: 将每两个元素交叉放置\n因此我们可以类似与归并排序的方式,将sum中的每两个元素相加,再将每两个元素交叉放置. 重复log2(VECTOR_WIDTH)次后,第一个元素就是结果.\nprogram-3 ISPC 前提: L3\npart1 ISPC basic 任务:学习ISPC基本概念和编写.\nISPC是一种编译器,可以将C代码编译为SIMD指令.\npart2 ISPC task 任务描述: 观察ISPCtask执行的结果\n1 启动mandelbrot_ispc \u0026ndash;tasks\n结果: [mandelbrot serial]: [424.881] ms Wrote image file mandelbrot-serial.ppm [mandelbrot ispc]: [97.180] ms Wrote image file mandelbrot-ispc.ppm [mandelbrot multicore ispc]: [48.986] ms Wrote image file mandelbrot-task-ispc.ppm (4.37x speedup from ISPC) (8.67x speedup from task ISPC)\n因为设置了两个task所以大约是两倍的加速比 对于 ISPC\n2 修改mandelbrot_ispc_withtasks()中的task数量, you should be able to achieve performance that exceeds the sequential version of the code by over 32 times! How did you determine how many tasks to create? Why does the number you chose work best?\n根据机器的最大超线程数量设置 我设置了16个task, 因为我的机器是4核8线程, 16个task可以使得每个线程都有两个task.\n3 what happens when you launch 10,000 ISPC tasks? What happens when you launch 10,000 threads?\n向量加速\n思考题: Q: Why are there two different mechanisms (foreach and launch) for expressing independent, parallelizable work to the ISPC system? A:foreach是将一个任务分配给多个线程,而launch是将多个任务分配给多个线程.\nQ: Couldn\u0026rsquo;t the system just partition the many iterations of foreach across all cores and also emit the appropriate SIMD code for the cores? A:\nprogram-4 Iterative sqrt (15 points) 用sqrt复习ISPC的基本概念\n1 运行结果: [sqrt serial]: [1316.793] ms [sqrt ispc]: [301.134] ms [sqrt task ispc]: [52.439] ms (4.37x speedup from ISPC) (25.11x speedup from task ISPC) 4.37x speedup due to SIMD 25.11 / 4.37 = 5.74x speedup due to multi-core\n2 构造数组使得加速比最大.\n全部数为2.998. 思路: 因为每个元素相同可以让计算更均匀,2.998可以充分调动cpu 结构: (5.60x speedup from ISPC) (30.39x speedup from task ISPC)\n3 构造数组使得加速比最小.\n全部数为1 思路: 1的sqrt计算迭代最少.\n结果: (2.50x speedup from ISPC) (3.08x speedup from task ISPC)\nprogram-5 BLAS saxpy (10 points) 1 运行观察加速比 [saxpy ispc]: [25.098] ms [11.874] GB/s [1.594] GFLOPS [saxpy task ispc]: [18.438] ms [16.164] GB/s [2.169] GFLOPS (1.36x speedup from use of tasks)\n因为需要访问内存所以加速比不高.\n2 Even though saxpy loads one element from X, one element from Y, and writes one element to result the multiplier by 4 is correct. Why is this the case? (Hint, think about how CPU caches work.)\n当程序写入结果的一个元素时，它首先将包含这个元素的缓存行提取到缓存中。这需要一个内存操作。然后，当不需要这个缓存行时，它将从缓存中闪现出来，这需要另一个内存操作。\n","permalink":"/posts/cmu-15418cs-618/asst1-performance-analysis-on-a-quad-core-cpu/","summary":"参考 任务 Program 1: Parallel Fractal Generation Using Threads (20 points) 提示: 需要先看CMU15-418/CS149的L2再完成Pro1 任务描述: 用多线程画mandelbrot fractal. 代码中给","title":"asst1"},{"content":"并行程序 void sinx(int N, int terms, float* x, float* result) { for (int i = 0; i \u0026lt; N; i++) { float value = x[i]; float number = x[i] * x[i] * x[i]; int sign = -1; int denom = 6; for (int j = 0; j \u0026lt; terms; j++) { value += sign * number / denom; sign *= -1; denom *= (2*j+3)*(2*j+2); number *= x[i] * x[i]; } result[i] = value; } } 转换成汇编后大致如下:\nld r0, addr[r1] mul ri, r0, r0 mul r1, r1, r0 可以看到每次循环都是独立的。 对于最简单的是顺序执行。 通过超线程(超标量处理器具有从单个指令流中提取多个指令的能力)可以提高性能。有时称指令级并行性。(ILP, Instruction Level Parallelism) 但在这些汇编指令中必须顺序执行。 因此实现指令级并行性是一个挑战。 但即使是纯顺序执行的代码,也有很多方式使其运行更快(基于写代码的方式和编译器的智能程度). Pentium 4 比如先取多条指令等. (有个黑匣子会预测分支,预测错误的话就会清空流水线,浪费时间) 解决方法: 1. 通过pthread编写并行性的程序 2. 假设有一种语言可以表示并行性,编译器可以自动并行化程序 如: forall(int i from 0 to n-1){} 自动并行化可能的解决方法: 1. 直接分为k个线程,每个线程处理n/k个循环. 然后将结果合并 2. 在硬件上执行. 有一堆性能较低但具有并行性的处理器时, 也需要更多电力/时间来驱动很多信号从一端到另一端. ## CPU \u0026amp;\u0026amp; GPU GPU将核心的概念带到了极致, 抛弃了所有的分支预测, 只是控制逻辑而不完成计算. 对于上面的程序有垂直和水平两种分割方式: - 垂直: 每个线程处理一个循环 - 水平: 同时处理多个循环, 如先同时进行所有的第一个乘法... ## SIMD Single Instruction Multiple Data 假设我正在执行的多次操作之间没有依赖关系,都能够并行运行. a single instruction is applied to multiple data elements simultaneously. 即: 同时对8个数值和另一个地方的8个数值取出并进行加法. 有时这些数值可以被称作向量. 使用AVX intrinsics的向量化程序: ```c++ void sinx(int N, int terms, float* x, float* result) { for (int i = 0; i \u0026lt; N; i+=8) { __m256 origx = _mm256_load_ps(\u0026amp;x[i]); __m256 value = origx; __m256 number = _mm256_mul_ps(origx, _mm256_mul_ps(origx, origx)); float sign = -1; __m256 denom = _mm256_set1_ps(6); for (int j = 0; j \u0026lt; terms; j++) { //value += sign * number / denom; __m256 tmp = _mm256_div_ps(number, denom); tmp = _mm256_mul_ps(tmp, _mm256_set1_ps(sign)); value = _mm256_add_ps(value, tmp); sign *= -1; //denom *= (2*j+3)*(2*j+2); denom = _mm256_mul_ps(denom, _mm256_set1_ps((2*j+3)*(2*j+2))); //number *= x[i] * x[i]; number = _mm256_mul_ps(number, _mm256_mul_ps(origx, origx)); } _mm256_store_ps(\u0026amp;result[i], value); //result[i] = value; } } 编译成汇编后大致如下:\nvloadps xmm0, addr[r1] vmulps xmm1, xmm0, xmm0 vmulps xmm2, xmm1, xmm0 ... ... ... vstoreps addr[xmm2], xmm0 AVX代表高级矢量扩展, 256代表每次可以处理256位的数据, 也就是8个float. 有多个版本:\nAVX: 128位 = 4 * 4 * 8 = 32字节 AVX2: 256位 = 8 * 4 * 8 = 32字节 AVX512: 512位 = 16 * 4 * 8 = 64字节 XMM寄存器是特殊的32字节 256位寄存器, 有16个, 从xmm0到xmm15. 用于支持vectorized SIMD指令.\n那么有没有办法让编译器自动将代码向量化呢?\n有,GCC的-O3选项可以自动向量化代码. 但只有非常结构化,精心编写的代码才能被自动向量化.\n条件 如果加入条件判断,如何向量化?\nif(x \u0026lt; 0){ x = -x; }else{ x = x; } SIMD可能的做法: 设置一个掩码, 用于标记哪些元素需要执行哪些不需要执行.\nx \u0026lt; 0: 1 1 0 0 1 0 0 0 x = -x: 1 1 0 0 1 0 0 0 翻转: 0 0 1 1 0 1 1 1 x = x: 0 0 1 1 0 1 1 1 但大多时候只保留了一半的效率,因为每次有可能只有一半的数据需要执行. 不过这很好的保证了一致性,因为分支结束后又回到了同一个执行路径. 即保持一致性,远离分歧.\ncoherent execution: 所有的线程都执行相同的指令.\ndivergent: a lack of instruction stream coherence.\n对于生成这些矢量操作,要么有聪明的编译器,要么就是有耐心的程序员.\nSIMD execution on many modern GPUs SPMD: Single Program Multiple Data\nGPU给的不是SIMD,而是SPMD. 单个程序,多个数据. 意味着程序的不同部分可以执行不同的指令.\n在这之下,还是用SIMD来实现大部分逻辑,采用异构的方式来实现并行.\n但有n个加法, 即两个包含n个值的向量相加. 实际上不是所有单位都在等待计算.而是会先计算出如何分配到块中,底层块的实际大小是32, 32values而不是32byte. 这个被称作SIMD宽度,一般是8-32.\nGPU和CPU的差别 CPU i7:\n4核 8 SIMD ALUs per core 每秒大概几千次浮点运算 GPU: RTX 1080\n20 cores 32 SIMD ALUs per core 每秒大概8m次浮点运算 GPU的核心摒弃了分支预测等只用做control,因此可以有更多的ALU.填充进来.\n大概是80:1的原始计算能力差异.\n总结 三种方法实现并行计算\n多核CPU:\n线程级实现并行 SIMD:\n指令级并行 通过向量化指令实现 但依赖于事先知道执行的指令优先级顺序 Superscaler: exploit ILP within an instruction stream\npaart2 accessing memory Memory latency: 从CPU到内存的时间\nexample: DRAM访问时间 100 cycles, 100ns Memory bandwidth: 从内存到CPU的时间\nexample: 20GB/s 其实不是很快 Stall: CPU等待内存的时间 当cpu试图进行读取而内存不可用时，就会停等知道内存可用.\n缓存就是为了解决Stall的问题.\n在多级缓存中,靠近核心的缓存是私有的. 这样可以通过写入读出L2缓存的数据来实现通讯,而不需要经过DRAM.\n缓存对延迟和带宽都有帮助.\nPrefecthing reduces stalls 硬件通常通过预取来减少延迟. 即预测下一次可能会访问的数据,并将其提前读取到缓存中. 不过可能会造成信息泄露\n使用预取的效果: Multi-threading reduces stalls 让多个线程交替进行, 如asst1/prog2的实现\n这也是超线程的实现,在一个核心中多路复用多个指令流. 对于CPU\u0026amp;GPU, 谁来组织线程是不同的做法.(操作系统 or 硬件)\n通常情况下内存要比其他因素更加限制速度\n","permalink":"/posts/cmu-15418cs-618/l2/","summary":"并行程序 void sinx(int N, int terms, float* x, float* result) { for (int i = 0; i \u0026lt; N; i++) { float value = x[i]; float number = x[i] * x[i] * x[i]; int sign = -1; int denom = 6; for (int j = 0; j \u0026lt; terms; j++) { value += sign * number / denom; sign *= -1; denom *= (2*j+3)*(2*j+2); number *= x[i]","title":"L2"},{"content":"lambda表达式 描述:\n一个匿名函数对象 一个可调用的代码单元 一个函数对象的语法糖 语法规则: {};\n[]: lambda表达式的引导符 (): 参数列表 {}: 函数体\n具体示例\nfor (auto \u0026amp;thread: threads) { thread = std::thread([\u0026amp;taskId, num_total_tasks, runnable] { for (int id = taskId++; id \u0026lt; num_total_tasks; id = taskId++) runnable-\u0026gt;runTask(id, num_total_tasks); }); } 在示例中, thread的初始化是一个lambda表达式, 该lambda表达式的参数列表为空, 函数体为\n{ for (int id = taskId++; id \u0026lt; num_total_tasks; id = taskId++) runnable-\u0026gt;runTask(id, num_total_tasks); } 这样一个thread可以负载多个任务.\n如果不用lambda表达式, 那么就需要定义一个函数, 然后将函数的地址传递给thread, 这样就会增加代码量.\n也就是说实际上 thread的参数可以是一个函数对象, 也可以是一个函数指针, 也可以是一个lambda表达式.\n","permalink":"/posts/c++/modern-c++/","summary":"lambda表达式 描述: 一个匿名函数对象 一个可调用的代码单元 一个函数对象的语法糖 语法规则: {}; []: lambda表达式的引导符 (): 参数列表 {}: 函数体 具","title":"Modern C++"},{"content":"poetry 出现的错误及解决方法 poetry install 时Failed to create the collection: Prompt dismissed 解决方案: 关闭keyring\npython3 -m keyring --disable 原因: https://github.com/python-poetry/poetry/issues/1917\n","permalink":"/posts/tools/python/poetry/","summary":"poetry 出现的错误及解决方法 poetry install 时Failed to create the collection: Prompt dismissed 解决方案: 关闭keyring python3 -m keyring --disable 原因: https://github.com/python-poetry/poetry/issues/1917","title":"poetry"},{"content":"使用\n进入pods的容器\nkubectl exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- /bin/bash # 查看对应容器的日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 错误和解决方案 minikube 挂载 本地目录进minikube时,作为mysql的数据目录,但是mysql无法启动 挂载方式: 在minikube正常启动后, 使用\nminikube mount \u0026lt;本地目录\u0026gt;:\u0026lt;minikube目录\u0026gt; 进行挂载\n检查问题\n# 进入pod 的 db容器内查看日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 输出为\nfind: File system loop detected; \u0026#39;/var/lib/mysql/test\u0026#39; is part of the same file system loop as \u0026#39;/var/lib/mysql/\u0026#39;. 原因是挂载时发现循环\n解决方案:\n关闭并删除minikube minikube stop minikube delete 在minikube启动时就挂载 minikube start --mount --mount-string=\u0026#34;\u0026lt;本地目录\u0026gt;:\u0026lt;minikube目录\u0026gt;\u0026#34; 问题解决\nminikube 中 设置ingress未转发的问题 参考Could not access Kubernetes Ingress in Browser on Windows Home with Minikube?\n问题1： 当使用minikube时，设置ingress后，minikube ssh 内部可以通过ingress转发的服务端口访问。 但127.0.0.1 或 minikube ip 在主机上无法访问。\n解决方法：\nSet custom domain IP to 127.0.01 in %WINDIR%\\System32\\drivers\\etc\\hosts file, i.e. by adding line 127.0.0.1 my-k8s.com Get ingress pod name: kubectl get pods -n ingress-nginx Start port forwarding: kubectl -n ingress-nginx port-forward pod/ingress-nginx-controller-5d88495688-dxxgw --address 0.0.0.0 80:80 443:443, where you should replace ingress-nginx-controller-5d88495688-dxxgw with your ingress pod name. Enjoy using ingress on custom domain in any browser (but only when port forwarding is active) 问题2: ingress中使用prefix的转发规则时,无法获取路径中的query\n解决方法:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; # 需要添加这个 nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: defaultBackend: service: name: default-http-backend port: number: 80 rules: - host: fuzzs-scene-sim-test.localhost http: paths: - path: /FuzzsSceneSimTest(/|$)(.*) # 后缀加上(/|$)(.*) 用于获取query pathType: ImplementationSpecific backend: service: name: fuzzs-scene-sim-test port: number: 8089 ","permalink":"/posts/tools/k8s/minikube/","summary":"使用 进入pods的容器 kubectl exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- /bin/bash # 查看对应容器的日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 错误和解决方案 minikube 挂载 本地目录进minikube时,作为mysql的数据","title":"minikube"},{"content":"本规范用于在开发过程中，使得项目能够更好的部署，更好的维护。\n使用时间: 当项目开始开发时，就应该遵守本规范。\n核心要点:\n管理依赖库 使用docker 端口、ip地址等使用环境变量 路径不能写死！尤其是绝对路径和根目录等，需要放在环境变量中！！ 后端 python项目 python常见的依赖库管理有:\npoetry requirements.txt pipenv poetry 初始化\npoetry init 安装依赖\npoetry install 使用poetry运行项目\npoetry run python main.py 添加依赖\npoetry add \u0026lt;package\u0026gt; dockerfile示例\nFROM python:3.8.5-slim-buster WORKDIR /app # 拷贝依赖文件 COPY pyproject.toml poetry.lock ./ # 设置国内源 RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 安装poetry RUN pip install poetry # 安装依赖 RUN poetry config virtualenvs.create false \\ \u0026amp;\u0026amp; poetry install --no-dev --no-interaction --no-ansi # tips: 先只拷贝依赖文件，再安装依赖，可以利用docker的缓存机制，加快构建速度. # (防止只是项目文件改变，而依赖文件没有改变，导致重新安装依赖) # 拷贝项目文件 COPY . . # 运行项目 CMD [\u0026#34;poetry\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;main.py\u0026#34;] requirements.txt 导出依赖\npip freeze \u0026gt; requirements.txt 安装依赖\npip install -r requirements.txt dockerfile示例\nFROM python:3.8.5-slim-buster WORKDIR /app # 拷贝依赖文件 COPY requirements.txt ./ # 设置国内源 RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 安装依赖 RUN pip install -r requirements.txt # 拷贝项目文件 COPY . . # 运行项目 CMD [\u0026#34;python\u0026#34;, \u0026#34;main.py\u0026#34;] pipenv 初始化\npipenv --python 3.8 安装依赖\npipenv install 使用pipenv运行项目\npipenv run python main.py 添加依赖\npipenv install \u0026lt;package\u0026gt; dockerfile示例\nFROM python:3.8.5-slim-buster WORKDIR /app # 设置国内源 RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 拷贝依赖文件 COPY Pipfile Pipfile.lock ./ # 安装依赖 RUN pip install pipenv \\ \u0026amp;\u0026amp; pipenv install --system --deploy --ignore-pipfile # 拷贝项目文件 COPY . . # 运行项目 CMD [\u0026#34;pipenv\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;main.py\u0026#34;] SpringBoot项目 参考 这里都以maven作为依赖管理工具。\n主要保留pom.xml文件\ndockerfile示例\n# 第一阶段: 构建jar包 FROM maven:3.6.3-jdk-8-slim AS build WORKDIR /app COPY pom.xml ./ # 设置国内源 RUN mvn -B -e -C -T 1C org.apache.maven.plugins:maven-dependency-plugin:3.1.2:go-offline # 拷贝项目文件 COPY . . # 构建jar包 RUN mvn clean install -DskipTests # 第二阶段: 运行jar包 FROM openjdk:8-jdk-alpine WORKDIR /app # 拷贝第一阶段构建的jar包 COPY --from=build /app/target/demo-0.0.1-SNAPSHOT.jar ./ # 运行项目 CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;target/demo-0.0.1-SNAPSHOT.jar\u0026#34;] 数据库 通常后端要连接数据库，这里只是简单的示例，实际项目中应该使用docker-compose来管理多个容器。\ndockerfile示例\nFROM mysql:8.0.22 # 设置时区 ENV TZ=Asia/Shanghai # 设置root密码 ENV MYSQL_ROOT_PASSWORD=123456 # 设置数据库名 ENV MYSQL_DATABASE=test # 设置用户名 ENV MYSQL_USER=test # 设置密码 ENV MYSQL_PASSWORD=123456 # 设置端口 EXPOSE 3306 单独运行mysql\ndocker run -d -p 3306:3306 --name mysql -v /path/to/mysql/data:/var/lib/mysql mysql:8.0.22 前端 前端使用npm作为依赖管理工具, 使用nginx作为web服务器。\n必要的文件:\npackage.json # 依赖文件 package-lock.json # 锁定依赖版本 nginx.conf # nginx配置文件 dockerfile npm npm初始化\nnpm init 安装依赖\nnpm install 添加依赖(默认添加到dependencies, 添加到devDependencies需要加上\u0026ndash;save-dev参数(或者-D)\nnpm install \u0026lt;package\u0026gt; nginx nginx.conf示例\nserver { listen 80; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; location / { try_files $uri $uri/ /index.html; } } docker docker 使用多阶段构建\ndockerfile示例\n# 第一阶段: 构建项目 FROM node:lts-alpine as build WORKDIR /app # 拷贝依赖文件 COPY package.json package-lock.json ./ # 安装依赖 RUN npm install # 拷贝项目文件 COPY . . # 构建项目 RUN npm run build # 第一段构建完成, 获得/app/build文件夹 # 使用nginx作为web服务器 FROM nginx:1.19.4-alpine # 拷贝nginx配置文件 COPY nginx.conf /etc/nginx/conf.d/default.conf # 拷贝第一阶段构建的项目文件 COPY --from=build /app/build /usr/share/nginx/html # 运行nginx CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 项目部署 TODO 封装整个项目(单个项目时) 经过上面的步骤已经将前后端 数据库封装到docker中了,但每次启动项目都需要手动启动三个容器, 这里使用docker-compose来管理多个容器。\ndocker-compose.yml示例\nversion: \u0026#39;3.8\u0026#39; services: mysql: image: mysql:8.0.22 environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: test MYSQL_USER: test MYSQL_PASSWORD: 123456 ports: - 3306:3306 volumes: - ./mysql/data:/var/lib/mysql backend: build: ./backend ports: - 8080:8080 depends_on: - mysql frontend: build: ./frontend ports: - 80:80 depends_on: - backend 整个项目作为k8s的一个服务(多个项目时) 上面是使用docker-compose来管理 一个项目的多个容器.\n但如果有多个项目, 每个项目都有多个容器, 这时候就需要使用k8s来管理了.\n我们把一个项目(多个容器)作为一个k8s的一个服务.\nk8s的配置文件示例\napiVersion: v1 kind: Service metadata: name: test labels: app: test spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: test type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: test spec: replicas: 1 selector: matchLabels: app: test template: metadata: labels: app: test spec: containers: - name: mysql image: mysql:8.0.22 env: - name: TZ value: Asia/Shanghai - name: MYSQL_ROOT_PASSWORD value: 123456 - name: MYSQL_DATABASE value: test - name: MYSQL_USER value: test - name: MYSQL_PASSWORD value: 123456 ports: - containerPort: 3306 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql - name: backend image: backend:latest ports: - containerPort: 8080 env: - name: MYSQL_HOST value: mysql - name: MYSQL_PORT value: \u0026#34;3306\u0026#34; - name: MYSQL_DATABASE value: test - name: MYSQL_USER value: test - name: MYSQL_PASSWORD value: 123456 - name: frontend image: frontend:latest ports: - containerPort: 80 volumes: - name: mysql-data hostPath: path: /path/to/mysql/data ","permalink":"/posts/reference/%E5%88%A9%E4%BA%8E%E9%83%A8%E7%BD%B2%E7%9A%84%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/","summary":"本规范用于在开发过程中，使得项目能够更好的部署，更好的维护。 使用时间: 当项目开始开发时，就应该遵守本规范。 核心要点: 管理依赖库 使用docke","title":"利于部署的开发规范手册"},{"content":"GPU\n图形渲染 图像中的每个对象都有很自然的并行性。\n","permalink":"/posts/cmu-15418cs-618/l7/","summary":"GPU 图形渲染 图像中的每个对象都有很自然的并行性。","title":"L7"},{"content":"grep工具\n","permalink":"/posts/tools/grep/","summary":"grep工具","title":"grep"},{"content":"虚拟文件系统\n/proc/cpuinfo\nmodel name cpu MHz - 频率 cache size - 缓存大小 siblings - 逻辑cpu数量 processor - 逻辑cpu编号 cpu cores - 物理cpu数量 core id - 物理cpu编号 这样查看的cpu数量很多\nsiblings是逻辑cpu的数量\ncpu cores是物理cpu的数量\n为什么报告的processor数量是40而siblings是20呢? 因为报告的processor包括超线程的逻辑cpu. 这样操作系统就可以直接根据逻辑cpu的数量来分配任务.\nMemory bandwidth - 内存带宽 Power consumption - 功耗 能源消耗实际上是一个很大的问题. Intel code name - 代号 Functional units\nlatency - 延迟 issue time - 发射时间 capacity - 容量 微处理架构\nfunction units latency - 延迟，执行一个指令所需要的时钟周期数(不包括等待) issue time - 发射时间，指令发射到执行所需要的时钟周期数(包括等待) capacity - 容量 优化的地方:\n搞清楚到底哪些代码是执行次数最多的(内部循环)(对实际使用情况来说) 基本运算消耗时间: 除法 \u0026gt; 乘法 \u0026gt; 加法 \u0026gt; 位移 基本的程序: 合并重复计算的简单的提升: 将除法次数减少,(不依赖于内层循环的变量的计算拿出来)\n循环展开 loop unrolling\n如果每一次循环都要进行一次是否终止的测试,开销会很大.(尤其是一次循环的计算 相比于 循环次数来说很小 时)\n所以处理器从简单的策略开始,如预测循环的次数. 大部分都是基于统计预测的.\n如果可以预测循环的次数,就可以将循环展开. 每次循环多执行4 或 8 或\u0026hellip;次原来循环做的事情.\n但展开时不一定均匀,\nuniform可以使得循环展开的更好.\n为什么8维向量获得了超过8倍的加速呢? 因为uniform, 原本要做8次的判断,现在只需要做一次.\n常规优化提升了15倍 向量优化提升了5.4倍 总计提升了82倍\n向量化很好且是free的,但不能忽略了传统的优化\n传统的优化(213 program)使得速度提升了三倍\n要做到极致的优化,就比如要花3个星期的时间在编码风格上, 最后30分钟花在向量化上.\n但要看情况来决定编码风格的优化. 因为如果我们编写的代码不是执行次数最多(如内核,场景仿真,高频), 那么可能更需要的是可读性.\n可读性变差 可能会导致bug很容易被引入, 并且非常不容易被发现和维护.\n","permalink":"/posts/cmu-15418cs-618/l6/","summary":"虚拟文件系统 /proc/cpuinfo model name cpu MHz - 频率 cache size - 缓存大小 siblings - 逻辑cpu数量 processor - 逻辑cpu编号 cpu cores - 物理cpu数量 core id - 物理cpu编号 这样查看的cpu数量很","title":"L6"},{"content":"光标移动 h: 左移一个字符 j: 下移一行 k: 上移一行 l: 右移一个字符\nw: 移动到下一个单词的开头 e: 移动到当前单词末尾 b: 移动到上一个单词的开头\n0: 移动到行首 $: 移动到行尾\nn + 上面的命令: 移动n次\ngg: 移动到文件开头 G: 移动到文件末尾\n上面所有命令构成了一个移动命令，可以和d命令组合使用，删除从当前光标到移动命令所指的位置的内容\nctrl + f: 下翻一页 ctrl + b: 上翻一页 ctrl + G: 显示当前光标所在行的行号\nctrl + i: 跳转到上次位置· ctrl + o: 跳转到下次位置\nG + n: 移动到第n行\n插入 i: 在当前光标处插入 I: 在当前行首插入\na: 在当前光标后插入 A: 在当前行尾插入\no: 在当前行下方插入一行 O: 在当前行上方插入一行\n删除 x: 删除当前光标所在的字符 X: 删除当前光标所在的前一个字符\ndd: 删除当前行 D: 删除当前光标所在位置到行尾的内容\nd + 移动命令: 删除从当前光标到移动命令所指的位置的内容\n如: dw: 删除当前光标所在的单词 db: 删除当前光标所在的单词 d$: 删除当前光标所在位置到行尾的内容 dnG: 删除当前光标所在行到第n行的内容 dG: 删除当前光标所在行到文件末尾的内容\n剪切 上面删除的内容都会被保存到剪切板中\n删除并进入插入模式 s: 删除当前光标所在的字符并进入插入模式 S: 删除当前行并进入插入模式\nc + 移动命令: 删除从当前光标到移动命令所指的位置的内容并进入插入模式\n如: cw: 删除当前光标所在的单词并进入插入模式 c$: 删除当前光标所在位置到行尾的内容并进入插入模式 cnG: 删除当前光标所在行到第n行的内容并进入插入模式\n复制 y + 移动命令: 复制从当前光标到移动命令所指的位置的内容\n如: yw: 复制当前光标所在的单词 yb: 复制当前光标所在的单词 y$: 复制当前光标所在位置到行尾的内容 ynG: 复制当前光标所在行到第n行的内容\n粘贴 所有删除的内容都会被保存到剪切板中，可以使用p命令将剪切板中的内容粘贴到当前光标所在位置 p: 将剪切板中的内容粘贴到当前光标所在位置的后面 P: 将剪切板中的内容粘贴到当前光标所在位置的前面\n替换 r + 字符: 将当前光标所在的字符替换为指定的字符\nR + 字符串: 将当前光标所在位置开始的字符串替换为指定的字符串\n撤销 u: 撤销上一次操作 U: 撤销当前行的所有操作\nctrl + r: 恢复上一次撤销的操作\n重复 . : 重复上一次操作\n查找 / + 关键字: 从当前光标开始向下查找关键字 ? + 关键字: 从当前光标开始向上查找关键字\n输完后按回车，会跳转到第一个匹配的位置.\nn: 跳转到下一个匹配的位置 N: 跳转到上一个匹配的位置\n: 进阶命令 :w 保存文件 :q 退出 :q! 强制退出，不保存 :wq 保存并退出 :wq! 强制保存并退出\n上面的命令 + 文件名: 保存文件到指定的文件名\n:help 命令名: 查看命令的帮助文档\n替换 :%s/old/new/g 将所有的old替换为new :%s/old/new/gc 将所有的old替换为new，替换前询问是否替换\n:#,#s/old/new/g 将第#行到第#行的old替换为new\n外部命令 :! + 命令: 执行外部命令\n如: :!ls 执行ls命令 :!dir 执行dir命令\n","permalink":"/posts/tools/vim/%E6%8C%87%E4%BB%A4%E6%89%8B%E5%86%8C/","summary":"光标移动 h: 左移一个字符 j: 下移一行 k: 上移一行 l: 右移一个字符 w: 移动到下一个单词的开头 e: 移动到当前单词末尾 b: 移动到上一个单词的开头 0: 移动到行首 $: 移","title":"vim的使用"},{"content":"三种分配策略的总结 静态分配 优点:\n几乎没有运行时的开销(关于分配) 缺点:\n不总是均匀的分配任务 什么时候使用:\n(最简单的例子) 当知道每个任务的工作量相当的时候 当每个任务的工作量是可预测的,但不一定相等的时候 半静态分配\n场景: 当工作量会随时间发生改变,当变化比较慢时.(任务量不可预测) 做法: 定期的重新分配任务 动态分配 场景: 当每个任务的工作量或者任务的数量是不可预测的时候\n每个计算单元都要去获取任务\n但这样的实现, 每次的任务可能会很少, 会使得更多的开销在争夺锁(获取任务的锁)上面.\n有一个办法是一次性计算更多的任务.\n但分配更多的任务可能会导致负载不平衡.\n因此需要在分配任务数量上要找一个平衡, 不花费过多的时间在争夺锁上, 也不会导致负载不平衡.\nSchedule long tasks first 但如果有一个大任务在最后，将出现如下情况： 因此，如果知道有一个大任务，可以提前处理，而不是放到最后一个.\nWork stealing 当一个计算单元没有任务的时候, 从其他计算单元那里偷取任务.\n实现的一些问题:\n1.从哪个线程开始偷取任务呢? 有随机的, 也有从最后一个开始偷取的.\n2.应该偷取多少任务呢? 应该偷取尽可能多一些,这样可以减少偷取任务的次数.\n3.怎样检测一个计算单元是否有任务呢? 可能会循环遍历,\n4.使用本地队列(分布式队列)会更快(在有互斥锁的情况下)\n还有一种方式是使用特殊的数据结构来存储任务间的依赖关系, 从而可以在任务完成的时候, 自动的调度下一个任务. 缺点是额外开销 常见的并行编程模式 循环 创建显示线程 递归时的并行\n递归可以编写出简单的代码, 但是递归的并行化是比较困难的.\n因为递归的并行化需要在递归的每一层都要进行并行化, 并且需要在每一层都要进行同步.\n但只要有独立的子问题, 就可以创造很多潜在的并行性.\nFork-Join pattern cilk_spawn: 会创建一个新的线程, 并且在新的线程中执行函数, 并且不会阻塞当前的线程.\ncilk_sync: 会等待所有的子线程执行完毕, 并且会阻塞当前的线程.\n每个函数的结尾隐式的调用了cilk_sync.\nexample: 有一个主线程+fork的线程. 快排的例子: 在规模较小的时候, 使用串行的快排. 这样可以减少线程的创建和销毁的开销. 不要忽略了抽象和实现的区别. spawn不是生成一个具体的线程, 而是声明这里有一个可以并行的任务.\n任务的数量至少需要比硬件线程多,但也不能大于100倍. 8倍是一个比较好的选择. Cilk的实现 假设我们要去实现clik_spawn 和 cilk_sync 线程池的实现(CILB):\nthread1 需要找到一种方法来发现有新的任务可以执行. 所以thread 0不能简单的调用foo, 它的作用是执行foo.\n但需要在执行foo前,把特殊的东西放入工作队列中.\n此时如果另一个线程突然变得空闲, 它就可以从工作队列中获取任务.\n为什么不把foo放入队列, 直接执行bar呢?(上面是执行foo bar放入队列)\n这涉及到 continuation first(child stealing) 和 child first(continuation stealing) 的问题.\ncontinuation first会导致线程0的大量工作排队.(广度优先队列) child first会导致其他线程把下一个任务偷走时, 会导致线程0的工作队列为空.(深度优先队列)\n实际上child first是合理的.(在递归中是最合适的)\n在递归程序中,会先将所有深度的任务放入队列中.\n按照之前优先执行大任务的策略, 其他线程会优先从队列顶部(先入的)中偷取任务. 因为在分而治之的算法中, 大任务会被分解成小任务, 因此大任务会先被放入队列中.\n实际中使用了双端队列:\n从队列头部获取任务 从队列尾部放入任务 但之前有一个问题: 很多队列,该从哪个队列中获取任务呢? 也许是随机的. 偷取任务的时候, 不随机的更可能会引起负载不均衡.\n本地线程访问的是本地队列的尾部, 偷取时也是放入尾部.(偷其他队列的头部) 这样也有利于空间局部性.\n那么如何实现同步呢?\nExample1: stalling join policy 拖延政策: 所有我创建的任务都必须完成后, 我才能继续执行. Example2: greedy join policy(cilk的实现方法)\n有一个跟踪数据结构,但那个东西可以四处移动.\n最后一个完成的线程会偷走这个数据结构\n所以一旦最后一个任务完成, 就可以继续执行了.\n这样不会浪费时间等待同步.\n第一个方法实现起来更简单,但速度更慢. 因为它总是首线程只等待其他线程完成.\n总结 ","permalink":"/posts/cmu-15418cs-618/l5/","summary":"三种分配策略的总结 静态分配 优点: 几乎没有运行时的开销(关于分配) 缺点: 不总是均匀的分配任务 什么时候使用: (最简单的例子) 当知道每个任务的工作","title":"L5 Work distribution and scheduling"},{"content":"主要用三种方式实现并行程序(没有进行真正的优化)\n例子 n-body simulation\n创建并行程序的过程\n1. Decomposition 主要思想: 创造至少足够的任务让所有的处理单元都有事情做\nAmdahl\u0026rsquo;s Law: 串行部分的比例越大, 并行程序的加速比就越小,因为增加处理单元的数量并不能减少串行部分的时间\n分解的任务更多是程序员的工作, 编译器还无法很好的帮助我们\n2.Assignment 需要考虑让每个处理单元尽可能减少沟通.\n有一种方法是随机分配,但会最大化沟通 还有一个极端是全部由一个处理单元完成,但是这样就没有并行了\n这是另一个挑战\n分配可以静态也可以动态发生\n静态: 在程序开始时就确定好. 动态: 在程序运行时分配 静态分配的问题:\n无法适应不同的输入(如:工作量不均匀) 无法适应不同的处理单元数量 动态分配: 通过消息传递来实现, 每个处理单元都有一个队列, 用来存放需要处理的任务(tasks). 当一个处理单元完成了一个任务, 就从队列中取出一个任务来处理 缺点: 队列需要同步, 会有额外的开销\n3. Orchestration 编排阶段 编排的目标是: 减少沟通和同步的成本, preserve locality of data reference, reduce overhead.\n4.mapping 这是程序员最不需要关心的, 交给编译器就好了 example 顺序程序: 那么如何并行执行呢?\nStep1: identify dependencies(problem decomposition) 因为会迭代很多次,所以会引起不同迭代次数的数据竞争.\n有一种划分方法是沿着对角线: 不足之处是:\n有些对角线很短, 负载不均衡 需要额外的计算(对角线下标) 另一种方法是滚动数组: 用两个数组, 一个用来存放当前迭代的结果, 一个用来存放上一次迭代的结果\n这样计算时不会有数据竞争.\n但很多人不希望有额外的内存开销.\n事实上使用的是红黑排序.\n每次迭代只更新红色的部分, 然后再翻转. 这样就不需要复制数组了.\nStep2: assign tasks 我们不把每一个元素作为一个任务,而是把每一行作为一个任务.\n同时: 红黑排序有一个同步的步骤: 必须等待所有的红色部分都计算完毕, 才能开始计算黑色部分.\n为了最小化沟通, 相邻行作为捆包是更好的选择, 这样只在更新边界时需要沟通.\n三种实现方法 Data-parallel expression of solver 这个的特点是系统做了很多工作, 程序只需要指定哪里需要并行.\nshared-address-space code version1 : 但是有个锁会使得程序变慢 version2: 有三个barrier来保证红黑顺序 为什么是三个呢?\n每一部分都要被分割\n最后一个是为了diff的分割 第一个是为了myDiff的分割 第二个是为了diff的分割\n所以可以使用diff数组\nversion3: barrier的问题: barrier还是有点笨重, 这会强制所有线程到一个起跑线 但如果有更精确的信息, 只需要等待依赖的线程就好了\nmessage-passing code 需要有额外的划分,来存储相邻处理器的数据\n同时,在最后计算diff时,需要等待所有的处理器都计算完毕. 这里选中了一个processor zero来计算diff, 其他的处理器都发送自己的diff给它.\n但沟通时有可能发生死锁. 因为每个处理器都在等待其他处理器的消息, 但是自己的消息又没有发送出去.\n所以需要分奇偶来发送\n","permalink":"/posts/cmu-15418cs-618/l4/","summary":"主要用三种方式实现并行程序(没有进行真正的优化) 例子 n-body simulation 创建并行程序的过程 1. Decomposition 主要思想: 创造至少足够的任务让所有的处理单元都有事情做 Amdahl\u0026rsquo;s Law: 串行","title":"L4 Parallel Programing basics"},{"content":"Parallelism 加速比 Speed up 是指： 程序在单处理器上运行的时间 / 程序在多处理器上运行的时间\n我们一般会期望用两倍的硬件得到两倍的速度提升,但是实际上并不是这样的。\n制约性能提升可能的因素有:\n资源分配不均匀 通信开销 短板效应 共享资源读写冲突 为什么要去了解硬件？\n什么是限制性能的因素？ 导致性能瓶颈的原因是什么？ Efficiency fast != efficient\n什么是效率？ 尽可能地利用资源，减少浪费 比如按时间租用服务器。\n总结 并行程序的挑战：\n负载均衡 Load balance 通信延迟 Communication latency 集体工作时，真正用于计算的时间很少 ","permalink":"/posts/cmu-15418cs-618/l1-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%92%8C%E9%AB%98%E6%95%88/","summary":"Parallelism 加速比 Speed up 是指： 程序在单处理器上运行的时间 / 程序在多处理器上运行的时间 我们一般会期望用两倍的硬件得到两倍的速度提升,但是实际上并不是这样的","title":"Why parallelism? Why efficiency?"},{"content":"TL;DR 背景: 已使用nextcloud和typora写笔记 需求: 将笔记转换为博客.(且因为本人太懒,😂 所以需要全自动化) 在nextcloud中, 专门设置一个文件夹\u0026quot;笔记\u0026quot; 转换为博客文件夹 不能改变原来记笔记的方式 不能有任何新增的操作 方案: 使用hugo搭建博客 使用Github pages部署博客 使用Github Actions自动化部署 使用py脚本将笔记转换为博客 使用任务计划程序定时执行py脚本 使用hugo搭建博客 参考: hugo官网 Hugo+Github Pages+Github Action博客方案之二 Hugo+Github Pages+Github Action博客方案之三 PaperMod主题\n创建github仓库 要创建两个仓库\n一个仓库用于存放博客源码 一个仓库用于存放博客静态文件 创建博客静态文件仓库 设置仓库名为: 用户名.github.io 我的博客仓库\n创建博客源码仓库 设置仓库名为: hugo-blog // 仓库名可以自定义 我的博客源码仓库\n安装hugo scoop install hugo 创建hugo博客 hugo new site hugo-blog 安装主题 cd hugo-blog ## 进入博客目录, 这个是博客源码仓库 git init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive ## needed when you reclone your repo (submodules may not get cloned automatically) 配置主题 这里使用yaml格式的配置文件, 也可以使用toml格式的配置文件 所以需要删除config.toml文件, 并创建config.yaml文件\nconfig.yaml:\nbaseURL: / title: ysyy\u0026#39;s blog theme: PaperMod languageCode: zh-cn 剩余配置参考\n创建文章 hugo new posts/first/hello-world.md 本地预览 hugo server -D 生成静态文件 生成静态文件, 生成的静态文件在 public文件夹中。 之后我们将这个文件夹中复制到博客静态文件仓库中\nhugo 部署到github pages 创建静态文件夹\ngit clone git@用户名.github.io.git cd 用户名.github.io cp -r hugo-blog/public/* ./ 提交到github\ngit add . git commit -m \u0026#34;first commit\u0026#34; git push origin main 配置github pages 在github中的 用户名.github.io仓库中, 点击 Settings选项卡, 找到 GitHub Pages选项, 将 Source选项设置为 main分支, 点击 Save按钮, 这样就可以通过 https://用户名.github.io访问博客了\n使用Github Actions自动化部署 参考\n如果每一次更新/发布新博客都需要手动执行上面的步骤, 那么就太麻烦了, 所以我们需要自动化部署\n在博客源码仓库的根目录下创建 .github/workflows/deploy.yml文件\nname: ysyyblog on: push: branches: - main jobs: build-deploy: runs-on: ubuntu-20.04 # runs-on: macos-latest steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.PERSONAL_TOKEN }} # 另外还支持 deploy_token 和 github_token external_repository: ysyyhhh/ysyyhhh.github.io # 修改为你的 静态文件GitHub Pages 仓库 publish_dir: ./public # keep_files: false publish_branch: main # 如果使用自定义域名，还需要添加下面一行配置 # cname: www 创建personal_token 在github主页的右上角点击头像, 点击 Settings选项卡, 找到 Developer settings选项,\n找到 Personal access tokens选项, 点击 Generate new token按钮, 创建一个新的token\n配置personal_token 在hugo-blog仓库中, 点击 Settings选项卡, 找到 Secrets选项, 点击 New repository secret按钮,\n新增一个名为 PERSONAL_TOKEN的secret, 值为上面创建的personal_token\n测试自动化部署 在本地的hugo-blog仓库中, 修改 content/posts/first/hello-world.md文件, 然后提交到github\n可以在 Actions选项卡中查看自动化部署的状态\n如果在 Actions选项卡中看到了 build-deploy任务, 且状态为 success, 那么就说明自动化部署成功了\n可以在 用户名.github.io仓库中查看是否已经更新.\n使用任务计划程序和py脚本实现全自动化 上面的步骤已经让我们发布笔记的过程变成:\n使用hugo new / 直接编辑 content的文件 来创建笔记 提交到hugo-blog仓库 然后hugo-blog仓库就会自动部署到用户名.github.io仓库中\n虽然已经只剩两步了,但遵循能自动化就自动化的原则, 我们还是要把这两步也自动化\n使用py脚本将笔记转换为博客 安装python这些步骤就省去了,这里直接给出py脚本\n\u0026#39;\u0026#39;\u0026#39; 每天定时更新博客内容 1.进入项目根目录: D:/program_about/hugo/hugo-blog 2. 将D:/nextcloud/笔记/下的文件同步到 ./content/posts/下 3. 执行./push.bat 或 git add . \u0026amp;\u0026amp; git commit -m \u0026#34;update\u0026#34; \u0026amp;\u0026amp; git push \u0026#39;\u0026#39;\u0026#39; import os import shutil def create_index(root, name): \u0026#39;\u0026#39;\u0026#39; name = A.md 在root下生成\u0026#39;A\u0026#39;文件夹 将A.md移动到A文件夹下，并重命名为index.md 如果 存在 root + \u0026#39;/img\u0026#39; 的文件夹 将 root + \u0026#39;/img\u0026#39; 复制到 root + \u0026#39;/A/img\u0026#39; 下 \u0026#39;\u0026#39;\u0026#39; # 生成文件夹 dir_name = name.split(\u0026#39;.\u0026#39;)[0] print(root, name, dir_name) os.mkdir(os.path.join(root, dir_name)) # 移动文件 shutil.move(os.path.join(root, name), os.path.join(root, dir_name, \u0026#39;index.md\u0026#39;)) # 处理img if os.path.exists(os.path.join(root, \u0026#39;img\u0026#39;)): shutil.copytree(os.path.join(root, \u0026#39;img\u0026#39;), os.path.join(root, dir_name, \u0026#39;img\u0026#39;)) def adjust(dir): os.chdir(dir) \u0026#39;\u0026#39;\u0026#39; 将所有下面的格式 - A.md - img - A-1.png 转换成 - A - index.md - img - A-1.png 如果遇到\u0026#34;.md\u0026#34;文件,直接删除 \u0026#39;\u0026#39;\u0026#39; for(root, dirs, files) in os.walk(\u0026#34;.\u0026#34;): root = os.path.join(dir, root) for name in files: if name == \u0026#39;.md\u0026#39;: os.remove(os.path.join(root, name)) continue if name.endswith(\u0026#39;.md\u0026#39;): create_index(root, name) for name in dirs: # 递归调用 adjust(os.path.join(root, name)) def sync(): root_path = \u0026#39;D:/program_about/hugo/hugo-blog\u0026#39; os.chdir(root_path) # 当文件已存在时，无法创建该文件。: \u0026#39;./content/posts/\u0026#39; shutil.rmtree(\u0026#39;./content/posts/\u0026#39;) # git中也要删除 os.system(\u0026#39;git rm -r ./content/posts/\u0026#39;) shutil.copytree(\u0026#39;D:/nextcloud/笔记/\u0026#39;, \u0026#39;./content/posts/\u0026#39;) # 把所有文件夹和文件的名称大写转换为小写 os.chdir(\u0026#39;./content/posts/\u0026#39;) for root, dirs, files in os.walk(\u0026#34;.\u0026#34;): for name in files: new_name = name.lower() os.rename(os.path.join(root, name), os.path.join(root, new_name)) for name in dirs: new_name = name.lower() os.rename(os.path.join(root, name), os.path.join(root, new_name)) # 调整文件夹结构 adjust(root_path+\u0026#39;./content/posts/\u0026#39;) # 上传到git # os.chdir(\u0026#39;./content/posts/\u0026#39;) os.chdir(\u0026#39;D:/program_about/hugo/hugo-blog\u0026#39;) os.system(\u0026#39;git add ./content/posts/\u0026#39;) os.system(\u0026#39;git commit -m \u0026#34;update\u0026#34;\u0026#39;) os.system(\u0026#39;git push\u0026#39;) os.chdir(\u0026#39;D:/program_about/hugo/hugo-blog\u0026#39;) print(\u0026#39;sync done\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: sync() 将上面的路径修改为自己的路径, 然后保存为 sync.py文件 可以执行py脚本,测试一下\n关于图片路径问题 参考方案\n因为我平时的图片路径是\n- A.md - img - A-1.png 但是hugo会将A.md文件转换为A文件夹, 所以此时是无法访问A-1.png的.\n这里是通过改变相对路径关系来解决的, 即代码中的adjust()\n当然如果你有图床就不需要这么麻烦了\n使用任务计划程序定时执行py脚本 参考 这里我使用的是win10自带的任务计划程序, 其他系统的任务计划程序也是类似的\n以下步骤由Claude生成\n下面是如何使用Windows任务计划程序来配置定时每天执行Python脚本的步骤: 打开任务计划程序(Windows + R 输入taskschd.msc回车) 点击\u0026#34;操作\u0026#34;栏中的\u0026#34;创建基本任务\u0026#34; 输入任务名称,选择触发器为每天定时,设置执行时间 在操作栏中,点击“新建” 选择“启动一个程序” 在“程序/脚本”框中输入Python解释器的路径,例如C:\\Python37\\python.exe 在“添加参数(可选)”中输入python脚本文件的完整路径,例如C:\\Users\\username\\script.py 点击“确定”保存此操作 在下一页中选择用户账号,例如“当前用户” 点击“确定”完成创建任务 根据需要配置触发器记录和其他选项 点击“确定”保存任务 任务将在设定的时间自动执行python脚本文件 每次修改脚本后需要停止原有任务,然后再新建一个相同的任务来加载修改后的脚本代码。 需要注意python interpreter路径和脚本路径的正确性。定时执行格式也需要正确,这样就可以实现Windows系统中的自动定时任务执行Python脚本了。 ","permalink":"/posts/tools/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","summary":"TL;DR 背景: 已使用nextcloud和typora写笔记 需求: 将笔记转换为博客.(且因为本人太懒,😂 所以需要全自动化) 在nextcloud中, 专","title":"用Hugo + Github Pages/Action + py + 任务计划程序 搭建 全自动化markdown笔记转博客"}]