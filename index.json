[{"content":"ISPC language SPMD: Single Program Multiple Data 一种花哨的方式来说，就是一种并行编程的范式，它的特点是：在编程时，我们只需要写一个程序，然后在运行时，这个程序会被复制多份，每一份都会被分配到不同的处理器上去执行，这样就实现了并行。\n用ISPC实现sinx\nexport void sinx(uniform int N, uniform int terms,uniform float* x, uniform float* result){ //assume N % programCount == 0 for (uniform int i = 0; i \u0026lt; N;i += programCount){ int idx = i + programIndex; float value = x[idx]; float number = value * value * value; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 0; j \u0026lt; terms; j++){ value += sign * number / denom; number *= value * value; denom *= (2 * j + 3) * (2 * j + 2); sign *= -1; } result[idx] = value; } } 使用C++来调用 调用ISPC的东西是个程序实例的集合, gang.\n#include \u0026lt;iostream\u0026gt; #include \u0026#34;sinx.ispc.h\u0026#34; int N = 1000000; int terms = 10; float* x = new float[N]; float* result = new float[N]; //init x //execute ispc::sinx(N,terms,x,result); ispc中不需要手动设置programCount,programIndex\nprogramCount: number of simultaneous program instances in the gang (uniform value)\nprogramIndex: id of the current program instance in the gang(a non-uniform value)\nuniform value: 一个值在gang中的所有实例中都是一样的\n如果在ispc中直接使用sinx 并不会更快.\n因为有一些相同的工作会被重复做很多次. 通过分离他们,可以减少重复计算的次数,从而提高效率.\n一个设想的实现方法如下: ISPC是为了更容易编写SIMD代码而设计的, 只需要通过特殊的宏或编译指示就可以使用SIMD指令.\nprogramCount 就是 向量宽度\nSPMD programming abstraction\nISPC compiler generates SIMD implementation\nversion2版本的代码,这是分块进行而不是交错的.\nexport void sinx(uniform int N, uniform int terms,uniform float* x, uniform float* result){ uniform int count = N / programCount; int start = programIndex * count; //assume N % programCount == 0 for (uniform int i = 0; i \u0026lt; count;i += 1){ int idx = start + i; float value = x[idx]; float number = value * value * value; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 0; j \u0026lt; terms; j++){ value += sign * number / denom; number *= value * value; denom *= (2 * j + 3) * (2 * j + 2); sign *= -1; } result[idx] = value; } } 交错通常比分块更好,因为分块会导致数据的访问不连续. 当计算量不均匀时,分块会导致一些处理器的负载过重,而另一些处理器的负载过轻.\n并且因为是同时进行的, 交错可以访问邻近的数据,这样可以增加cache的命中率.\n根本原因: 矢量加载指令(寄存器)是一次加载多个数据,如果在很短的时间内,要加载的数据是连续的,那么就可以一次加载多个数据,如果数据是不连续的,那么就需要多次加载,这样就会降低效率. 如果有个聪明的编译器,它可以自动将分块的代码转换为交错的代码,这样就可以兼顾两者的优点.\nforeach就可以实现这个功能,让程序员不需要关心这些细节.\nexport void sinx(uniform int N, uniform int terms,uniform float* x, uniform float* result){ foreach(i = 0 ... N){ float value = x[i]; float number = value * value * value; uniform int denom = 6; uniform int sign = -1; for (uniform int j = 0; j \u0026lt; terms; j++){ value += sign * number / denom; number *= value * value; denom *= (2 * j + 3) * (2 * j + 2); sign *= -1; } result[i] = value; } } ISPC的错误例子:\nexport uniform float sumall(uniform int N, uniform float* x){ uniform float sum = 0; foreach(i = 0 ... N){ sum += x[i]; } return sum; } 错误:编译器会报错,因为sum是一个uniform value,它在所有的实例中都是一样的,但是在foreach中,每个实例都会对sum进行修改,这样就会导致错误.\n修正这个错误:\nexport uniform float sumall(uniform int N, uniform float* x){ uniform float sum = 0; float partial_sum = 0; foreach(i = 0 ... N){ partial_sum += x[i]; } sum = reduce_add(partial_sum); return sum; } reduce_add原语: 允许将一组不同的值合并为一个值,这个值在所有的实例中都是一样的.\n编译后的细节 ISPC tasks: 基本上就是一个线程,但是它可以被分配到不同的处理器上去执行.\n三种并行编程范式 和 三种 machine architecture 聚焦于 communication 和 cooperation\n使用pthread时要call operate system 而在ISPC中,只需要call compiler\nThree models of communication(abstraction) 1.Shared address space asst3中会用到\n多个线程之间通过互斥锁来进行通信\n在硬件中, Dance-hall model 所有处理器在同一侧.\nSymmetric Multiprocessor(SMP) system 就是如此\n最简单的方式是总线, 但这样无法扩展,因为总线的带宽是有限的. 但实际中: 还有一种访问本地内存的方式,就是通过cache,这样就可以减少对总线的访问,从而提高效率. Non-Uniform Memory Access(NUMA) system 但它为程序员引入的复杂性是很大的,因为程序员需要手动的将数据放到本地内存中,这样才能提高效率.\nshared address space的优点:\n程序员不需要关心数据的传输 程序员不需要关心数据的分布 2.Message passing aasst4中会用到\n由于实现缓存一致性需要额外的成本，因此在大型系统中，共享内存的实现是不可行的。在这种情况下，消息传递是一种更好的选择。\n在消息传递中，每个处理器都有自己的私有内存，而且没有共享内存。要在处理器之间传递数据，必须使用显式的消息传递原语。\n不需要任何硬件支持，因此可以在任何系统上实现。只需要网络。\n可以构建大型系统，因为没有共享内存的限制。\n这些原语允许程序员在处理器之间传递数据，但是程序员必须显式地指定数据的传输。这种方式的缺点是，程序员需要关心数据的传输，这样就会增加程序员的负担。\n3.Data parallel asst2中会用到\n上面两种方式可以在任何硬件上实现。\nData parallel对程序员来说是最简单的，因为程序员不需要关心数据的传输，也不需要关心数据的分布。但是，它只能在特定的硬件上实现，因为它需要硬件支持。\n过去我们使用SIMD，现在使用SPMD。\n并行程序的问题\n这样的并行会得到不确定的结果。\n那么如何有原则性地使用并行呢？\n有一个抽象概念是stream，可以避免并行竞争问题。\n两个函数间的用法：\n当如果使用stream，就必须创建tmp。不得不把临时数据写入浪费的带宽中。\n所以我们希望也许有一些新的运算符可以做更加高级的操作。\ngather: 将数据从不同的stream中收集到一个stream中。 scatter: 将数据从一个stream中分散到不同的stream中。\nintel包括了gather，但不包括scatter。 总结 这些并不是完全独立的，而是可以组合使用的。\n通常在实践中为了得到最好的性能，会使用以上所有的方式。\n多核芯片内部通常是shared address space，但小规模情况下使用message passing。\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l3/","summary":"ISPC language SPMD: Single Program Multiple Data 一种花哨的方式来说，就是一种并行编程的范式，它的特点是：在编程时，我们只需要写一个程序，然后在运行时，这个程序会被复制多份，每","title":"Abstraction vs implementation"},{"content":"参考\n任务\nProgram 1: Parallel Fractal Generation Using Threads (20 points) 提示: 需要先看CMU15-418/CS149的L2再完成Pro1\n任务描述: 用多线程画mandelbrot fractal.\n代码中给出了串行的实现, 你需要实现多线程的版本.\n多线程版本中只需要修改 workerThreadStart函数. 不需要手动创建线程, 也不需要手动join线程. 直接调用mandelbrotThread().\n1.1 \u0026amp; 1.2, 计算在2,3,4,5,6,7,8,16,32个线程下的加速比 编写并观察 workerThreadStart函数的实现:\n345void workerThreadStart(WorkerArgs *const args) { // TODO FOR CS149 STUDENTS: Implement the body of the worker // thread here. Each thread should make a call to mandelbrotSerial() // to compute a part of the output image. For example, in a // program that uses two threads, thread 0 could compute the top // half of the image and thread 1 could compute the bottom half. // printf(\u0026#34;Hello world from thread %d\\n\u0026#34;, args-\u0026gt;threadId); double startTime = CycleTimer::currentSeconds(); // 每个线程负责的行数(除不尽的部分由最后一个线程负责) int height = args-\u0026gt;height / args-\u0026gt;numThreads; int startRow = args-\u0026gt;threadId * height; int numRows = height; if (args-\u0026gt;threadId == args-\u0026gt;numThreads - 1) { // 如果是最后一个线程，那么就要把除不尽的部分也算上 numRows = height + args-\u0026gt;height % args-\u0026gt;numThreads; } printf(\u0026#34;Thread %d startRow: %d, numRows: %d\\n\u0026#34;, args-\u0026gt;threadId, startRow, numRows); mandelbrotSerial(args-\u0026gt;x0, args-\u0026gt;y0, args-\u0026gt;x1, args-\u0026gt;y1, args-\u0026gt;width, args-\u0026gt;height, startRow, numRows, args-\u0026gt;maxIterations, args-\u0026gt;output); double endTime = CycleTimer::currentSeconds(); printf(\u0026#34;Thread %d time: %.3f ms\\n\u0026#34;, args-\u0026gt;threadId, (endTime - startTime) * 1000); } 结果:\n线程数 加速比 2 1.97 3 1.63 4 2.31 5 2.37 6 3.08 7 3.15 8 3.74 16 5.14 可以观察到，加速比和线程数并不是线性相关.\n猜测原因 猜测可能的原因有:\n线程通信的开销 每个线程分配的任务不均匀 1.3 查看每个线程的执行时间,验证猜想 当线程数为4时, 每个线程的执行时间如下: Thread 0 time: 63.974 ms Thread 3 time: 65.563 ms Thread 2 time: 259.972 ms Thread 1 time: 260.669 ms\n当线程数为8时, 每个线程的执行时间如下: Thread 0 time: 13.702 ms Thread 7 time: 16.831 ms Thread 1 time: 57.324 ms Thread 6 time: 61.069 ms Thread 5 time: 113.431 ms Thread 2 time: 115.753 ms Thread 4 time: 164.736 ms Thread 3 time: 166.306 ms\n可以看到,中间线程分配的任务更多,执行时间更长. 因此在增加线程数时,加速比并不是线性增加的.\n1.4 任务描述:\n解决上面的问题,使得加速比更接近线性. 如: 8线程时的加速比需要在7~8之间. 解决方法需要具有适用性, 适用所有的线程数. tips: 有一个非常简单的静态赋值可以实现这个目标，并且线程之间不需要通信/同步.\n解决方案 思路: 根据代码可知, 每行的计算是独立的, 因此可以将每行分配给不同的线程. 但由上面的实验可知,中间行的计算量比较大.\n因此我们不应该直接平均切分行, 而是以线程数量为步长,线程交叉依次分配行. 即 第i个线程分配k*n+i行.\nvoid workerThreadStart(WorkerArgs *const args) { // TODO FOR CS149 STUDENTS: Implement the body of the worker // thread here. Each thread should make a call to mandelbrotSerial() // to compute a part of the output image. For example, in a // program that uses two threads, thread 0 could compute the top // half of the image and thread 1 could compute the bottom half. // printf(\u0026#34;Hello world from thread %d\\n\u0026#34;, args-\u0026gt;threadId); double startTime = CycleTimer::currentSeconds(); /* 方案1 // 每个线程负责的行数(除不尽的部分由最后一个线程负责) int baseHeight = args-\u0026gt;height / args-\u0026gt;numThreads; int startRow = args-\u0026gt;threadId * baseHeight; int numRows = baseHeight; int yu = args-\u0026gt;height % args-\u0026gt;numThreads; // 均匀分配剩余行 if (args-\u0026gt;threadId \u0026lt; yu) { numRows++; } startRow += std::min(args-\u0026gt;threadId, yu); printf(\u0026#34;Thread %d startRow: %d, numRows: %d\\n\u0026#34;, args-\u0026gt;threadId, startRow, numRows); mandelbrotSerial(args-\u0026gt;x0, args-\u0026gt;y0, args-\u0026gt;x1, args-\u0026gt;y1, args-\u0026gt;width, args-\u0026gt;height, startRow, numRows, args-\u0026gt;maxIterations, args-\u0026gt;output); */ // 方案2, 依次分配行 int height = args-\u0026gt;height; for (int i = args-\u0026gt;threadId; i \u0026lt; height; i += args-\u0026gt;numThreads) { mandelbrotSerial(args-\u0026gt;x0, args-\u0026gt;y0, args-\u0026gt;x1, args-\u0026gt;y1, args-\u0026gt;width, args-\u0026gt;height, i, 1, args-\u0026gt;maxIterations, args-\u0026gt;output); } double endTime = CycleTimer::currentSeconds(); printf(\u0026#34;Thread %d time: %.3f ms\\n\u0026#34;, args-\u0026gt;threadId, (endTime - startTime) * 1000); } 输出结果:\nThread 3 time: 88.842 ms Thread 1 time: 89.680 ms Thread 0 time: 89.717 ms Thread 7 time: 90.280 ms Thread 5 time: 90.715 ms Thread 6 time: 90.743 ms Thread 2 time: 91.049 ms Thread 4 time: 92.982 ms [mandelbrot thread]: [93.318] ms Wrote image file mandelbrot-thread.ppm (7.10x speedup from 8 threads)\n上面的解决方案使得每个线程的执行时间基本相同,因此加速比接近线性. 在8线程时,加速比为7.1.\n1.5 16线程和8线程的加速比 现在16线程是否明显优于8线程? 给出是或否的原因. (6.45x speedup from 16 threads) 16线程并没有明显由于8线程,反而还更慢. 原因:\n电脑本身是4核, 超线程后是8线程. 16线程时线程切换反而导致开销增加. 总结 pro1的目的是为了认识到并行计算的overhead, 以及多线程在计算上也应该是依次交替分配的. 不能简单的平均分配.\npro1是通过垂直分割来实现并行计算. 而向量化是通过水平分割来实现并行计算.\nprogram-2-vectorizing-code-using-simd-intrinsics 前提: L2 任务描述： 使用SIMD指令(CS149intrin.h提供的),来实现clampedExpVector函数.\n示例函数:\nvoid absVector(float* values, float* output, int N) { __cs149_vec_float x; __cs149_vec_float result; __cs149_vec_float zero = _cs149_vset_float(0.f); __cs149_mask maskAll, maskIsNegative, maskIsNotNegative; // Note: Take a careful look at this loop indexing. This example // code is not guaranteed to work when (N % VECTOR_WIDTH) != 0. // Why is that the case? for (int i=0; i\u0026lt;N; i+=VECTOR_WIDTH) { // All ones maskAll = _cs149_init_ones(); // All zeros maskIsNegative = _cs149_init_ones(0); // Load vector of values from contiguous memory addresses _cs149_vload_float(x, values+i, maskAll); // x = values[i]; // Set mask according to predicate _cs149_vlt_float(maskIsNegative, x, zero, maskAll); // if (x \u0026lt; 0) { // Execute instruction using mask (\u0026#34;if\u0026#34; clause) _cs149_vsub_float(result, zero, x, maskIsNegative); // output[i] = -x; // Inverse maskIsNegative to generate \u0026#34;else\u0026#34; mask maskIsNotNegative = _cs149_mask_not(maskIsNegative); // } else { // Execute instruction (\u0026#34;else\u0026#34; clause) _cs149_vload_float(result, values+i, maskIsNotNegative); // output[i] = x; } // Write results back to memory _cs149_vstore_float(output+i, result, maskAll); } } 示例函数absVector并不能适用于所有情况,原因如下: 当n%VECTOR_WIDTH != 0时, 会越界.\n1\u0026amp;2 实现clampedExpVector函数 void clampedExpVector(float *values, int *exponents, float *output, int N) { // // CS149 STUDENTS TODO: Implement your vectorized version of // clampedExpSerial() here. // // Your solution should work for any value of // N and VECTOR_WIDTH, not just when VECTOR_WIDTH divides N // __cs149_vec_float one, nine; __cs149_vec_int zeroInt, oneInt; oneInt = _cs149_vset_int(1); zeroInt = _cs149_vset_int(0); one = _cs149_vset_float(1.f); nine = _cs149_vset_float(9.999999f); for (int i = 0; i \u0026lt; N; i += VECTOR_WIDTH) { __cs149_mask maskAll, maskIsZero, maskIsNotZero; __cs149_vec_float x; __cs149_vec_int y; // All ones maskAll = _cs149_init_ones(); // All zeros maskIsZero = _cs149_init_ones(0); // 防止在最后一次循环时，i+VECTOR_WIDTH超出N if (i + VECTOR_WIDTH \u0026gt; N) { maskAll = _cs149_init_ones(N - i); } // float x = values[i]; _cs149_vload_float(x, values + i, maskAll); // int y = exponents[i]; _cs149_vload_int(y, exponents + i, maskAll); // if (y == 0) _cs149_veq_int(maskIsZero, y, zeroInt, maskAll); // { // output[i] = 1.f; // } _cs149_vstore_float(output + i, one, maskIsZero); // else maskIsNotZero = _cs149_mask_not(maskIsZero); // 消除最后一次循环时，i+VECTOR_WIDTH超出N的情况 maskIsNotZero = _cs149_mask_and(maskIsNotZero, maskAll); { // float result = x; __cs149_vec_float result = x; // int count = y - 1; __cs149_vec_int count; _cs149_vsub_int(count, y, oneInt, maskIsNotZero); // 哪些count\u0026gt;0 __cs149_mask countMark; _cs149_vgt_int(countMark, count, zeroInt, maskIsNotZero); // while (count \u0026gt; 0) while (_cs149_cntbits(countMark) \u0026gt; 0) { // result *= x; _cs149_vmult_float(result, result, x, countMark); // count--; _cs149_vsub_int(count, count, oneInt, countMark); // 哪些count\u0026gt;0 _cs149_vgt_int(countMark, count, zeroInt, countMark); } // if (result \u0026gt; 9.999999f) __cs149_mask gtNineMask; _cs149_vgt_float(gtNineMask, result, nine, maskIsNotZero); // { reult = 9.999999f;} _cs149_vmove_float(result, nine, gtNineMask); // output[i] = result; _cs149_vstore_float(output + i, result, maskIsNotZero); } } } 通过init_ones来防止在有n%vectorWith!=0时 越界.\n在最开始的maskAll时设置 在取反码后也要设置一次 count循环: 通过设置一个mask来标记哪些count\u0026gt;0, 从而实现循环.\n修改vectorWidth为2, 4, 8, to 16来回答: Does the vector utilization increase, decrease or stay the same as VECTOR_WIDTH changes? Why?\nvectorWidth为2时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 2 Total Vector Instructions: 162728 Vector Utilization: 77.0% Utilized Vector Lanes: 250653 Total Vector Lanes: 325456\nvectorWidth为4时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 3 Total Vector Instructions: 119440 Vector Utilization: 72.2% Utilized Vector Lanes: 258879 Total Vector Lanes: 358320\nvectorWidth为8时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 8 Total Vector Instructions: 51628 Vector Utilization: 66.0% Utilized Vector Lanes: 272539 Total Vector Lanes: 413024\nvectorWidth为16时, 结果如下: ****************** Printing Vector Unit Statistics ******************* Vector Width: 16 Total Vector Instructions: 26968 Vector Utilization: 64.2% Utilized Vector Lanes: 277188 Total Vector Lanes: 431488\n可以发现, 随着vectorWidth的增加, vectorUtilization也在减少.\n原因: 有多个条件语句,当vectorWidth增加时, 每次在某个条件中不执行的指令也会增加.\n3 实现arraySumVector float arraySumVector(float *values, int N) { // // CS149 STUDENTS TODO: Implement your vectorized version of arraySumSerial here // __cs149_vec_float sum = _cs149_vset_float(0.f); for (int i = 0; i \u0026lt; N; i += VECTOR_WIDTH) { __cs149_mask maskAll; __cs149_vec_float x; // All ones maskAll = _cs149_init_ones(); // 防止在最后一次循环时，i+VECTOR_WIDTH超出N if (i + VECTOR_WIDTH \u0026gt; N) { maskAll = _cs149_init_ones(N - i); } // float x = values[i]; _cs149_vload_float(x, values + i, maskAll); // sum += x; _cs149_vadd_float(sum, sum, x, maskAll); } float result = 0.f; // log2(VECTOR_WIDTH)内解决 for (int i = 0; i \u0026lt; log2(VECTOR_WIDTH); i++) { // 使用_cs149_hadd_float函数，将sum中的每两个元素相加 // 再使用_cs149_interleave_float函数，将sum中的每两个元素交叉放置 // 重复log2(VECTOR_WIDTH)次 _cs149_hadd_float(sum, sum); _cs149_interleave_float(sum, sum); } // 将sum中的第一个元素赋值给result result = sum.value[0]; return result; } 假设VECTOR_WIDTHs始终是N的因子.\n可以实现在O(N/VECTOR_WIDTH + log2(VECTOR_WIDTH))的时间内完成计算.\n最后的log2实现方式. 提示中给了两个函数 hadd: 将每两个元素相加 interleave: 将每两个元素交叉放置\n因此我们可以类似与归并排序的方式,将sum中的每两个元素相加,再将每两个元素交叉放置. 重复log2(VECTOR_WIDTH)次后,第一个元素就是结果.\nprogram-3 ISPC 前提: L3\npart1 ISPC basic 任务:学习ISPC基本概念和编写.\nISPC是一种编译器,可以将C代码编译为SIMD指令.\npart2 ISPC task 任务描述: 观察ISPCtask执行的结果\n1 启动mandelbrot_ispc \u0026ndash;tasks\n结果: [mandelbrot serial]: [424.881] ms Wrote image file mandelbrot-serial.ppm [mandelbrot ispc]: [97.180] ms Wrote image file mandelbrot-ispc.ppm [mandelbrot multicore ispc]: [48.986] ms Wrote image file mandelbrot-task-ispc.ppm (4.37x speedup from ISPC) (8.67x speedup from task ISPC)\n因为设置了两个task所以大约是两倍的加速比 对于 ISPC\n2 修改mandelbrot_ispc_withtasks()中的task数量, you should be able to achieve performance that exceeds the sequential version of the code by over 32 times! How did you determine how many tasks to create? Why does the number you chose work best?\n根据机器的最大超线程数量设置 我设置了16个task, 因为我的机器是4核8线程, 16个task可以使得每个线程都有两个task.\n3 what happens when you launch 10,000 ISPC tasks? What happens when you launch 10,000 threads?\n向量加速\n思考题: Q: Why are there two different mechanisms (foreach and launch) for expressing independent, parallelizable work to the ISPC system? A:foreach是将一个任务分配给多个线程,而launch是将多个任务分配给多个线程.\nQ: Couldn\u0026rsquo;t the system just partition the many iterations of foreach across all cores and also emit the appropriate SIMD code for the cores? A:\nprogram-4 Iterative sqrt (15 points) 用sqrt复习ISPC的基本概念\n1 运行结果: [sqrt serial]: [1316.793] ms [sqrt ispc]: [301.134] ms [sqrt task ispc]: [52.439] ms (4.37x speedup from ISPC) (25.11x speedup from task ISPC) 4.37x speedup due to SIMD 25.11 / 4.37 = 5.74x speedup due to multi-core\n2 构造数组使得加速比最大.\n全部数为2.998. 思路: 因为每个元素相同可以让计算更均匀,2.998可以充分调动cpu 结构: (5.60x speedup from ISPC) (30.39x speedup from task ISPC)\n3 构造数组使得加速比最小.\n全部数为1 思路: 1的sqrt计算迭代最少.\n结果: (2.50x speedup from ISPC) (3.08x speedup from task ISPC)\nprogram-5 BLAS saxpy (10 points) 1 运行观察加速比 [saxpy ispc]: [25.098] ms [11.874] GB/s [1.594] GFLOPS [saxpy task ispc]: [18.438] ms [16.164] GB/s [2.169] GFLOPS (1.36x speedup from use of tasks)\n因为需要访问内存所以加速比不高.\n2 Even though saxpy loads one element from X, one element from Y, and writes one element to result the multiplier by 4 is correct. Why is this the case? (Hint, think about how CPU caches work.)\n当程序写入结果的一个元素时，它首先将包含这个元素的缓存行提取到缓存中。这需要一个内存操作。然后，当不需要这个缓存行时，它将从缓存中闪现出来，这需要另一个内存操作。\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/asst1-performance-analysis-on-a-quad-core-cpu/","summary":"参考 任务 Program 1: Parallel Fractal Generation Using Threads (20 points) 提示: 需要先看CMU15-418/CS149的L2再完成Pro1 任务描述: 用多线程画mandelbrot fractal. 代码中给","title":"asst1"},{"content":"C++ Sync thread的使用 #include \u0026lt;thread\u0026gt; #include \u0026lt;stdio.h\u0026gt; void my_func(int thread_id, int num_threads) { printf(\u0026#34;Hello from spawned thread %d of %d\\n\u0026#34;, thread_id, num_threads); } int main(int argc, char** argv) { std::thread t0 = std::thread(my_func, 0, 2); std::thread t1 = std::thread(my_func, 1, 2); printf(\u0026#34;The main thread is running concurrently with spawned threads.\\n\u0026#34;); t0.join(); t1.join(); printf(\u0026#34;Spawned threads have terminated at this point.\\n\u0026#34;); return 0; } mutex #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;mutex\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;thread\u0026gt; std::map\u0026lt;std::string, std::string\u0026gt; g_pages; std::mutex g_pages_mutex; void save_page(const std::string\u0026amp; url) { // simulate a long page fetch std::this_thread::sleep_for(std::chrono::seconds(2)); std::string result = \u0026#34;fake content\u0026#34;; std::lock_guard\u0026lt;std::mutex\u0026gt; guard(g_pages_mutex); g_pages[url] = result; } int main() { std::thread t1(save_page, \u0026#34;http://foo\u0026#34;); std::thread t2(save_page, \u0026#34;http://bar\u0026#34;); t1.join(); t2.join(); // safe to access g_pages without lock now, as the threads are joined for (const auto\u0026amp; pair : g_pages) std::cout \u0026lt;\u0026lt; pair.first \u0026lt;\u0026lt; \u0026#34; =\u0026gt; \u0026#34; \u0026lt;\u0026lt; pair.second \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Output\nhttp://bar =\u0026gt; fake content http://foo =\u0026gt; fake content condition_variable 线程调用 wait (lock)来指示它希望等待来自另一个线程的通知。\n注意，互斥对象(包装在 std: : only _ lock 中)被传递给 wait ()调用。当通知线程时，条件变量将获得锁。\n这意味着当调用 wait ()返回时，调用线程是锁的当前持有者。锁通常用于保护线程现在需要检查的共享变量，以确保它正在等待的条件为真。\n创建 N 个线程。N-1个线程等待来自线程0的通知，然后在接到通知后，自动递增一个受共享互斥锁保护的计数器。\n/* * Wrapper class around a counter, a condition variable, and a mutex. */ class ThreadState { public: std::condition_variable *condition_variable_; std::mutex *mutex_; int counter_; int num_waiting_threads_; ThreadState(int num_waiting_threads) { condition_variable_ = new std::condition_variable(); mutex_ = new std::mutex(); counter_ = 0; num_waiting_threads_ = num_waiting_threads; } ~ThreadState() { delete condition_variable_; delete mutex_; } }; void signal_fn(ThreadState *thread_state) { // Acquire mutex to make sure the shared counter is read in a // consistent state. thread_state-\u0026gt;mutex_-\u0026gt;lock(); while (thread_state-\u0026gt;counter_ \u0026lt; thread_state-\u0026gt;num_waiting_threads_) { thread_state-\u0026gt;mutex_-\u0026gt;unlock(); // Release the mutex before calling `notify_all()` to make sure // waiting threads have a chance to make progress. thread_state-\u0026gt;condition_variable_-\u0026gt;notify_all(); // Re-acquire the mutex to read the shared counter again. thread_state-\u0026gt;mutex_-\u0026gt;lock(); } thread_state-\u0026gt;mutex_-\u0026gt;unlock(); } void wait_fn(ThreadState *thread_state) { // A lock must be held in order to wait on a condition variable. // This lock is atomically released before the thread goes to sleep // when `wait()` is called. The lock is atomically re-acquired when // the thread is woken up using `notify_all()`. std::unique_lock\u0026lt;std::mutex\u0026gt; lk(*thread_state-\u0026gt;mutex_); thread_state-\u0026gt;condition_variable_-\u0026gt;wait(lk); // Increment the shared counter with the lock re-acquired to inform the // signaling thread that this waiting thread has successfully been // woken up. thread_state-\u0026gt;counter_++; printf(\u0026#34;Lock re-acquired after wait()...\\n\u0026#34;); lk.unlock(); } /* * Signaling thread spins until each waiting thread increments a shared * counter after being woken up from the `wait()` method. */ void condition_variable_example() { int num_threads = 3; printf(\u0026#34;==============================================================\\n\u0026#34;); printf(\u0026#34;Starting %d threads for signal-and-waiting...\\n\u0026#34;, num_threads); std::thread *threads = new std::thread[num_threads]; ThreadState *thread_state = new ThreadState(num_threads - 1); threads[0] = std::thread(signal_fn, thread_state); for (int i = 1; i \u0026lt; num_threads; i++) { threads[i] = std::thread(wait_fn, thread_state); } for (int i = 0; i \u0026lt; num_threads; i++) { threads[i].join(); } printf(\u0026#34;==============================================================\\n\u0026#34;); delete thread_state; delete[] threads; } part_a step 1 实现TaskSystemParallelSpawn void TaskSystemParallelSpawn::run(IRunnable *runnable, int num_total_tasks) { // // TODO: CS149 students will modify the implementation of this // method in Part A. The implementation provided below runs all // tasks sequentially on the calling thread. // std::atomic\u0026lt;int\u0026gt; taskId(0); int num_threads = this-\u0026gt;num_threads; std::thread threads[num_threads]; // 交叉分配任务 for (int i = 0; i \u0026lt; num_threads; i++) { threads[i] = std::thread([\u0026amp;, i]() { int task_id = taskId.fetch_add(1); while (task_id \u0026lt; num_total_tasks) { runnable-\u0026gt;runTask(task_id, num_total_tasks); task_id = taskId.fetch_add(1); } }); } for (int i = 0; i \u0026lt; num_threads; i++) { threads[i].join(); } // printf(\u0026#34;done\\n\u0026#34;); } Q:How will you assign tasks to your worker threads? Should you consider static or dynamic assignment of tasks to threads? A:交叉分配任务，动态分配任务\nQ:How will you ensure that all tasks are executed exactly once? A:使用原子变量taskId\nstep 2 实现 TaskSystemParallelThreadPoolSpinning step1 的overhead主要是创建线程的开销(尤其是计算量低的任务上)，因此使用线程池可以减少开销\n要求: 在TestSystem 创建时,或者在run时创建线程池\nQ1: 作为一个开始的实现，我们建议您将worker threads设计为连续循环，始终检查它们是否有更多的工作要执行。(进入 while 循环直到条件为真的线程通常称为“spinning”) 那么worker thread 如何确定有work要执行呢？\nTaskSystemParallelThreadPoolSpinning::TaskSystemParallelThreadPoolSpinning(int num_threads) : ITaskSystem(num_threads) { // // TODO: CS149 student implementations may decide to perform setup // operations (such as thread pool construction) here. // Implementations are free to add new class member variables // (requiring changes to tasksys.h). // exit_flag_ = false; for (int i = 0; i \u0026lt; num_threads; i++) { threads.emplace_back(\u0026amp;TaskSystemParallelThreadPoolSpinning::func, this); } } TaskSystemParallelThreadPoolSpinning::~TaskSystemParallelThreadPoolSpinning() { exit_flag_ = true; for (auto \u0026amp;thread : threads) { thread.join(); } } void TaskSystemParallelThreadPoolSpinning::run(IRunnable *runnable, int num_total_tasks) { // // TODO: CS149 students will modify the implementation of this // method in Part A. The implementation provided below runs all // tasks sequentially on the calling thread. // // printf(\u0026#34;run\\n\u0026#34;); runnable_ = runnable; num_tasks_ = num_total_tasks; num_tasks_done_ = num_total_tasks; for (int i = 0; i \u0026lt; num_total_tasks; i++) { tasks_mutex_.lock(); tasks_.push(i); tasks_mutex_.unlock(); } while (num_tasks_done_ \u0026lt; num_total_tasks) { std::this_thread::yield(); }; // Q:为什么要使用yield // A:因为如果不使用yield，那么线程会一直占用CPU，导致其他线程无法运行 // Q:那我直接死循环呢 // A:死循环会导致CPU占用率100%，导致其他线程无法运行 } Q2:确保 run ()实现所需的同步行为是非常重要的。如何更改 run ()的实现以确定批量任务启动中的所有任务都已完成？ A:使用原子变量num_tasks_done_，每个任务完成时，num_tasks_done_加一，当num_tasks_done_等于num_total_tasks时，所有任务完成\nstep 3 实现 TaskSystemParallelThreadPoolSleeping Step2的缺点： 当线程“spin”等待某些操作时，它们会利用 CPU 核心的执行资源。\n例如，工作线程可能会循环等待新任务到达。 另一个例子是，主线程可能会循环等待辅助线程完成所有任务，这样它就可以从 run ()调用返回。 这可能会影响性能，因为即使这些线程没有做有用的工作，也会使用 CPU 资源来运行这些线程。\n在任务的这一部分中，我们希望您通过让线程处于休眠状态来提高任务系统的效率，直到它们所等待的条件得到满足。\n您的实现可以选择使用条件变量来实现此行为。条件变量是一个同步原语，它允许线程在等待条件存在时休眠(不占用 CPU 处理资源)。其他线程向等待唤醒的线程发出“信号”，以查看它们所等待的条件是否已经满足。例如，如果没有工作要做，您的工作线程可能会处于休眠状态(这样它们就不会从尝试执行有用工作的线程那里占用 CPU 资源)。另一个例子是，调用 run ()的主应用程序线程可能希望在等待批量任务启动中的所有任务由工作线程完成时休眠。(否则，一个旋转的主线程将从工作线程那里夺走 CPU 资源!)有关 C + + 中条件变量的更多信息，请参见我们的 C + + 同步教程。\n您在这部分作业中的实现可能需要考虑棘手的race conditions 。您需要考虑许多可能的线程行为交错\n您可能需要考虑编写额外的测试用例来测试您的系统。赋值入门代码包括评分脚本用于评分代码性能的工作负载，但是我们也将使用一组更广泛的工作负载来测试您的实现的正确性，而我们在入门代码中并没有提供这些工作负载！\nThe assignment starter code includes the workloads that the grading script will use to grade the performance of your code, but we will also test the correctness of your implementation using a wider set of workloads that we are not providing in the starter code!\ntasksys.h\n/* * TaskSystemParallelThreadPoolSleeping: This class is the student\u0026#39;s * optimized implementation of a parallel task execution engine that uses * a thread pool. See definition of ITaskSystem in * itasksys.h for documentation of the ITaskSystem interface. */ class TaskSystemParallelThreadPoolSleeping : public ITaskSystem { public: TaskSystemParallelThreadPoolSleeping(int num_threads); ~TaskSystemParallelThreadPoolSleeping(); const char *name(); void run(IRunnable *runnable, int num_total_tasks); TaskID runAsyncWithDeps(IRunnable *runnable, int num_total_tasks, const std::vector\u0026lt;TaskID\u0026gt; \u0026amp;deps); void sync(); private: std::vector\u0026lt;std::thread\u0026gt; threads; int num_tasks_; bool exit_flag_; std::atomic\u0026lt;int\u0026gt; num_tasks_done_; std::queue\u0026lt;int\u0026gt; tasks_; std::mutex tasks_mutex_; IRunnable *runnable_{}; void func(); std::condition_variable *queue_condition_ = new std::condition_variable(); std::condition_variable *all_done_condition_ = new std::condition_variable(); int num_waiting_threads_; std::atomic\u0026lt;int\u0026gt; num_tasks_remaining_; std::mutex all_done_mutex_; }; tasksys.cpp\n/* * ================================================================ * Parallel Thread Pool Sleeping Task System Implementation * ================================================================ */ const char *TaskSystemParallelThreadPoolSleeping::name() { return \u0026#34;Parallel + Thread Pool + Sleep\u0026#34;; } void TaskSystemParallelThreadPoolSleeping::func() { int task_id; while (!exit_flag_) { task_id = -1; while (task_id == -1) { std::unique_lock\u0026lt;std::mutex\u0026gt; lk(tasks_mutex_); // 等待任务 queue_condition_-\u0026gt;wait(lk, [] { return 1; }); if (exit_flag_) { return; } if (!tasks_.empty()) { task_id = tasks_.front(); tasks_.pop(); } } runnable_-\u0026gt;runTask(task_id, num_tasks_); num_tasks_remaining_--; if (!num_tasks_remaining_) { // 通知主线程 // printf(\u0026#34;notify_all_done\\n\u0026#34;); all_done_condition_-\u0026gt;notify_one(); } else { // 通知其他线程 // printf(\u0026#34;notify_all\\n\u0026#34;); queue_condition_-\u0026gt;notify_one(); } } } TaskSystemParallelThreadPoolSleeping::TaskSystemParallelThreadPoolSleeping(int num_threads) : ITaskSystem(num_threads) { // // TODO: CS149 student implementations may decide to perform setup // operations (such as thread pool construction) here. // Implementations are free to add new class member variables // (requiring changes to tasksys.h). // exit_flag_ = false; for (int i = 0; i \u0026lt; num_threads; i++) { threads.emplace_back(\u0026amp;TaskSystemParallelThreadPoolSleeping::func, this); } } TaskSystemParallelThreadPoolSleeping::~TaskSystemParallelThreadPoolSleeping() { // // TODO: CS149 student implementations may decide to perform cleanup // operations (such as thread pool shutdown construction) here. // Implementations are free to add new class member variables // (requiring changes to tasksys.h). // exit_flag_ = true; queue_condition_-\u0026gt;notify_all(); for (auto \u0026amp;thread : threads) { thread.join(); } } void TaskSystemParallelThreadPoolSleeping::run(IRunnable *runnable, int num_total_tasks) { // // TODO: CS149 students will modify the implementation of this // method in Parts A and B. The implementation provided below runs all // tasks sequentially on the calling thread. // runnable_ = runnable; num_tasks_ = num_total_tasks; num_tasks_remaining_ = num_total_tasks; tasks_mutex_.lock(); for (int i = 0; i \u0026lt; num_total_tasks; i++) { tasks_.push(i); } tasks_mutex_.unlock(); // 通知其他线程 queue_condition_-\u0026gt;notify_all(); // printf(\u0026#34;run\\n\u0026#34;); while (num_tasks_remaining_) { std::unique_lock\u0026lt;std::mutex\u0026gt; lk2(all_done_mutex_); all_done_condition_-\u0026gt;wait(lk2, [] { return 1; }); } // printf(\u0026#34;all done\\n\u0026#34;); // printf(\u0026#34;all done\\n\u0026#34;); } 结果分析:\nsleep对spin的提升效果不明显，可能是因为任务太少，线程切换的开销比较大.\n运行结果:\n================================================================================ Running task system grading harness... (11 total tests) - Detected CPU with 16 execution contexts - Task system configured to use at most 8 threads ================================================================================ ================================================================================ Executing test: super_super_light... Reference binary: ./runtasks_ref_linux Results for: super_super_light STUDENT REFERENCE PERF? [Serial] 5.281 5.788 0.91 (OK) [Parallel + Always Spawn] 95.221 92.995 1.02 (OK) [Parallel + Thread Pool + Spin] 10.877 10.446 1.04 (OK) [Parallel + Thread Pool + Sleep] 6.943 42.705 0.16 (OK) ================================================================================ Executing test: super_light... Reference binary: ./runtasks_ref_linux Results for: super_light STUDENT REFERENCE PERF? [Serial] 37.497 37.844 0.99 (OK) [Parallel + Always Spawn] 108.136 108.805 0.99 (OK) [Parallel + Thread Pool + Spin] 10.777 13.615 0.79 (OK) [Parallel + Thread Pool + Sleep] 10.274 44.686 0.23 (OK) ================================================================================ Executing test: ping_pong_equal... Reference binary: ./runtasks_ref_linux Results for: ping_pong_equal STUDENT REFERENCE PERF? [Serial] 603.419 606.739 0.99 (OK) [Parallel + Always Spawn] 167.412 178.638 0.94 (OK) [Parallel + Thread Pool + Spin] 105.983 123.525 0.86 (OK) [Parallel + Thread Pool + Sleep] 108.243 148.316 0.73 (OK) ================================================================================ Executing test: ping_pong_unequal... Reference binary: ./runtasks_ref_linux Results for: ping_pong_unequal STUDENT REFERENCE PERF? [Serial] 1126.19 1109.329 1.02 (OK) [Parallel + Always Spawn] 259.271 260.822 0.99 (OK) [Parallel + Thread Pool + Spin] 199.088 198.013 1.01 (OK) [Parallel + Thread Pool + Sleep] 198.777 214.293 0.93 (OK) ================================================================================ Executing test: recursive_fibonacci... Reference binary: ./runtasks_ref_linux Results for: recursive_fibonacci STUDENT REFERENCE PERF? [Serial] 1052.273 1128.069 0.93 (OK) [Parallel + Always Spawn] 156.014 172.113 0.91 (OK) [Parallel + Thread Pool + Spin] 156.31 171.337 0.91 (OK) [Parallel + Thread Pool + Sleep] 156.462 166.476 0.94 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop STUDENT REFERENCE PERF? [Serial] 411.426 423.96 0.97 (OK) [Parallel + Always Spawn] 537.747 532.353 1.01 (OK) [Parallel + Thread Pool + Spin] 99.286 104.844 0.95 (OK) [Parallel + Thread Pool + Sleep] 95.817 239.76 0.40 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop_fewer_tasks... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop_fewer_tasks STUDENT REFERENCE PERF? [Serial] 413.681 415.961 0.99 (OK) [Parallel + Always Spawn] 514.021 505.234 1.02 (OK) [Parallel + Thread Pool + Spin] 108.644 117.702 0.92 (OK) [Parallel + Thread Pool + Sleep] 106.84 260.724 0.41 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop_fan_in... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop_fan_in STUDENT REFERENCE PERF? [Serial] 212.534 211.52 1.00 (OK) [Parallel + Always Spawn] 76.402 76.09 1.00 (OK) [Parallel + Thread Pool + Spin] 37.203 39.662 0.94 (OK) [Parallel + Thread Pool + Sleep] 36.523 57.039 0.64 (OK) ================================================================================ Executing test: math_operations_in_tight_for_loop_reduction_tree... Reference binary: ./runtasks_ref_linux Results for: math_operations_in_tight_for_loop_reduction_tree STUDENT REFERENCE PERF? [Serial] 208.076 207.488 1.00 (OK) [Parallel + Always Spawn] 45.054 45.227 1.00 (OK) [Parallel + Thread Pool + Spin] 33.079 33.9 0.98 (OK) [Parallel + Thread Pool + Sleep] 34.502 38.389 0.90 (OK) ================================================================================ Executing test: spin_between_run_calls... Reference binary: ./runtasks_ref_linux Results for: spin_between_run_calls STUDENT REFERENCE PERF? [Serial] 353.553 382.373 0.92 (OK) [Parallel + Always Spawn] 180.401 197.119 0.92 (OK) [Parallel + Thread Pool + Spin] 205.374 222.315 0.92 (OK) [Parallel + Thread Pool + Sleep] 214.819 197.579 1.09 (OK) ================================================================================ Executing test: mandelbrot_chunked... Reference binary: ./runtasks_ref_linux Results for: mandelbrot_chunked STUDENT REFERENCE PERF? [Serial] 257.289 256.815 1.00 (OK) [Parallel + Always Spawn] 34.395 34.058 1.01 (OK) [Parallel + Thread Pool + Spin] 34.241 34.922 0.98 (OK) [Parallel + Thread Pool + Sleep] 35.191 35.273 1.00 (OK) ================================================================================ Overall performance results [Serial] : All passed Perf [Parallel + Always Spawn] : All passed Perf [Parallel + Thread Pool + Spin] : All passed Perf [Parallel + Thread Pool + Sleep] : All passed Perf part_b 在任务的 B 部分中，您将扩展您的 A 部分任务系统实现，以支持可能依赖于以前任务的任务的异步启动。这些任务间依赖关系创建了任务执行库必须遵守的调度约束。\nITaskSystem 接口还有一个方法:\nvirtual TaskID runAsyncWithDeps(IRunnable* runnable, int num_total_tasks, const std::vector\u0026lt;TaskID\u0026gt;\u0026amp; deps) = 0; RunAsyncWithDeps ()类似于 run () ，因为它也用于执行 num total asks 任务的批量启动。但是，它与 run ()在许多方面有所不同\u0026hellip;\nAsynchronous Task Launch 首先，使用 runAsyncWithDeps ()创建的任务由任务系统与调用线程异步执行。\n这意味着 runAsyncWithDeps ()应该立即返回给调用方，即使任务尚未完成执行。\n该方法返回与此批量任务启动关联的唯一标识符。\n调用线程可以通过调用 sync ()来确定大容量任务启动的实际完成时间。\nvirtual void sync() = 0;\n只有当与之前所有批量任务启动关联的任务完成时，sync ()才返回给调用方。例如，考虑以下代码:\n// assume taskA and taskB are valid instances of IRunnable... std::vector\u0026lt;TaskID\u0026gt; noDeps; // empty vector ITaskSystem *t = new TaskSystem(num_threads); // bulk launch of 4 tasks TaskID launchA = t-\u0026gt;runAsyncWithDeps(taskA, 4, noDeps); // bulk launch of 8 tasks TaskID launchB = t-\u0026gt;runAsyncWithDeps(taskB, 8, noDeps); // at this point tasks associated with launchA and launchB // may still be running t-\u0026gt;sync(); // at this point all 12 tasks associated with launchA and launchB // are guaranteed to have terminated 如上面的注释中所述，在线程调用sync() runAsyncWithDeps() ) 的任务已完成。 准确地说， runAsyncWithDeps()告诉您的任务系统执行新的批量任务启动，但您的实现可以灵活地在下次调用sync()之前随时执行这些任务。 请注意，此规范意味着无法保证您的实现在从 launchB 启动任务之前先执行 launchA 中的任务！\nSupport for Explicit Dependencies runAsyncWithDeps()的第二个关键细节是它的第三个参数：TaskID 标识符向量，必须引用之前使用runAsyncWithDeps()启动的批量任务。 该向量指定当前批量任务启动中的任务所依赖的先前任务。 因此，在依赖向量中给出的启动中的所有任务完成之前，您的任务运行时无法开始执行当前批量任务启动中的任何任务！ 例如，考虑以下示例：\nstd::vector\u0026lt;TaskID\u0026gt; noDeps; // empty vector std::vector\u0026lt;TaskID\u0026gt; depOnA; std::vector\u0026lt;TaskID\u0026gt; depOnBC; ITaskSystem *t = new TaskSystem(num_threads); TaskID launchA = t-\u0026gt;runAsyncWithDeps(taskA, 128, noDeps); depOnA.push_back(launchA); TaskID launchB = t-\u0026gt;runAsyncWithDeps(taskB, 2, depOnA); TaskID launchC = t-\u0026gt;runAsyncWithDeps(taskC, 6, depOnA); depOnBC.push_back(launchB); depOnBC.push_back(launchC); TaskID launchD = t-\u0026gt;runAsyncWithDeps(taskD, 32, depOnBC); t-\u0026gt;sync(); 上面的代码有四个批量任务启动（taskA：128 个任务，taskB：2 个任务，taskC：6 个任务，taskD：32 个任务）。 请注意，任务 B 和任务 C 的启动取决于任务 A。 taskD 的批量启动 ( launchD ) 取决于launchB和launchC的结果。 因此，虽然您的任务运行时可以按任意顺序（包括并行）处理与launchB和launchC关联的任务，但这些启动中的所有任务必须在launchA的任务完成后开始执行，并且它们必须在运行时开始之前完成从launchD执行任何任务。\n我们可以通过任务图直观地说明这些依赖关系。 任务图是有向无环图 (DAG)，其中图中的节点对应于批量任务启动，从节点 X 到节点 Y 的边表示 Y 对 X 输出的依赖关系。上述代码的任务图是： 请注意，如果您在具有八个执行上下文的 Myth 计算机上运行上面的示例，则并行调度launchB和launchC中的任务的能力可能非常有用，因为单独的批量任务启动都不足以使用所有执行机器的资源。\nTask 您必须从 A 部分扩展任务系统实现，才能正确实现TaskSystem::runAsyncWithDeps()和TaskSystem::sync() 。 与 A 部分一样，我们为您提供以下入门提示：\nIt may be helpful to think about the behavior of runAsyncWithDeps() as pushing a record corresponding to the bulk task launch, or perhaps records corresponding to each of the tasks in the bulk task launch onto a \u0026ldquo;work queue\u0026rdquo;. Once the record to work to do is in the queue, runAsyncWithDeps() can return to the caller. The trick in this part of the assignment is performing the appropriate bookkeeping to track dependencies. What must be done when all the tasks in a bulk task launch complete? (This is the point when new tasks may become available to run.) It can be helpful to have two data structures in your implementation: (1) a structure representing tasks that have been added to the system via a call to runAsyncWithDeps(), but are not yet ready to execute because they depend on tasks that are still running (these tasks are \u0026ldquo;waiting\u0026rdquo; for others to finish) and (2) a \u0026ldquo;ready queue\u0026rdquo; of tasks that are not waiting on any prior tasks to finish and can safely be run as soon as a worker thread is available to process them. You need not worry about integer wrap around when generating unique task launch ids. We will not hit your task system with over 2^31 bulk task launches. You can assume all programs will either call only run() or only runAsyncWithDeps(); that is, you do not need to handle the case where a run() call needs to wait for all proceeding calls to runAsyncWithDeps() to finish. 在part_b/子目录中实现B部分实现，以与正确的参考实现（ part_b/runtasks_ref_* ）进行比较。\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/asst2/","summary":"C++ Sync thread的使用 #include \u0026lt;thread\u0026gt; #include \u0026lt;stdio.h\u0026gt; void my_func(int thread_id, int num_threads) { printf(\u0026#34;Hello from spawned thread %d of %d\\n\u0026#34;, thread_id, num_threads); } int main(int argc, char** argv) { std::thread t0 = std::thread(my_func, 0, 2); std::thread t1 = std::thread(my_func, 1, 2); printf(\u0026#34;The main thread is running concurrently with spawned threads.\\n\u0026#34;); t0.join(); t1.join(); printf(\u0026#34;Spawned threads have terminated at this point.\\n\u0026#34;); return 0; }","title":"asst2"},{"content":"安装docker sudo apt -y update sudo apt -y upgrade sudo apt -y full-upgrade # 安装依赖 sudo apt install -y apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release # 添加官方GPG密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg #添加仓库 echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # 更新apt sudo apt -y update # 安装docker sudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin # 安装docker-compose sudo apt install -y docker-compose centos 6.9 下安装docker\nhttps://medium.com/@zihansyu/centos-6-x-%E5%AE%89%E8%A3%9Ddocker-9e61354fd2ae\nhttps://blog.csdn.net/kinginblue/article/details/73527832\n1.镜像相关 # 构建镜像 docker build [选项] \u0026lt;上下文路径/URL/-\u0026gt; # 选项: -f, --file=\u0026#34;\u0026#34; # 指定要使用的Dockerfile路径（默认为./Dockerfile） # --force-rm=false # 在构建过程中删除中间容器 # --no-cache=false # 始终使用缓存 # --pull=false # 在构建过程中尝试去更新镜像的新版本 # --quiet=false # 安静模式，成功后只输出镜像ID # --rm=true # 在构建成功后删除临时容器 # -t, --tag=[] # 镜像名称（默认值：\u0026lt;上下文路径\u0026gt;的基本名称） # --ulimit=[] # Ulimit配置 # 拉取镜像 docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] # 查看镜像 docker images [选项] [仓库名] # 删除镜像 docker rmi [选项] \u0026lt;镜像1\u0026gt; [\u0026lt;镜像2\u0026gt; ...] # 查看镜像历史 docker history [选项] \u0026lt;镜像名\u0026gt; # 查看镜像详细信息 docker inspect [选项] \u0026lt;镜像名\u0026gt; 2.容器相关 # 创建容器 docker run [选项] \u0026lt;镜像名\u0026gt; [命令] #eg: docker run -d -p 8080:8080 --name tomcat tomcat:8.5.51 #选项 # -d 后台运行容器，并返回容器ID # -i 以交互模式运行容器，通常与 -t 同时使用 # -t 为容器重新分配一个伪输入终端，通常与 -i 同时使用 # -P 随机端口映射 # -p 指定端口映射，格式为：主机(宿主)端口:容器端口 # --name 指定容器名字 # --link 连接到其它容器 # --rm 容器退出后自动删除容器文件 # --volumes-from 从其它容器或数据卷挂载一些配置或其它文件 # --volume 挂载宿主机目录或文件，格式为：主机目录:容器目录 # --privileged=true 给容器内的root用户赋予最高权限，容器内的root用户就拥有了真正的root权限 # --restart # no 容器退出时不重启 # on-failure[:max-retries] 容器故障退出（返回值非零）时重启，最多重启max-retries次 # always 容器退出时总是重启 # unless-stopped 容器退出时总是重启，但是不考虑在Docker守护进程启动时就已经停止了的容器 # --env-file 从指定文件读入环境变量 # eg: # docker run -d -p 8080:8080 --name tomcat tomcat:8.5.51 --env-file ./env.list # 查看容器 docker ps [选项] # 删除容器 docker rm [选项] \u0026lt;容器名\u0026gt; # 启动容器 # 启动和创建容器的区别在于，启动容器是针对已经创建好的容器进行启动，而创建容器则是针对镜像进行的操作 docker start [选项] \u0026lt;容器名\u0026gt; # 停止容器 docker stop [选项] \u0026lt;容器名\u0026gt; # 查看容器日志 docker logs [选项] \u0026lt;容器名\u0026gt; # 查看容器内进程 docker top [选项] \u0026lt;容器名\u0026gt; # 查看容器详细信息 docker inspect [选项] \u0026lt;容器名\u0026gt; # 进入容器 docker exec [选项] \u0026lt;容器名\u0026gt; [命令] # 导出容器 docker export [选项] \u0026lt;容器名\u0026gt; # 导入容器 docker import [选项] \u0026lt;容器名\u0026gt; # 重命名容器 docker rename [选项] \u0026lt;容器名\u0026gt; \u0026lt;新容器名\u0026gt; # 查看容器使用的资源 docker stats [选项] \u0026lt;容器名\u0026gt; # 查看容器端口映射 docker port [选项] \u0026lt;容器名\u0026gt; # 导出容器中的文件 docker cp [选项] \u0026lt;容器名\u0026gt;:\u0026lt;容器内路径\u0026gt; \u0026lt;宿主机路径\u0026gt; # 选项: -a, --archive=false # 归档模式(默认) # -L, --follow-link=false # 总是解析符号链接 # -d, --device=false # 复制字符和块设备 # -r, --recursive=false # 递归复制整个目录 # -p, --pause=true # 暂停容器中的所有进程 docker 检查与排错 docker logs [选项] \u0026lt;容器名\u0026gt; # 选项: -f, --follow=false # 跟踪日志输出 # --since=\u0026#34;\u0026#34; # 显示自某个timestamp之后的日志，或相对时间，如42m（即42分钟） # --tail=\u0026#34;all\u0026#34; # 从日志末尾显示多少行日志， 默认是all # -t, --timestamps=false # 显示时间戳 # --until=\u0026#34;\u0026#34; # 显示自某个timestamp之前的日志，或相对时间，如42m（即42分钟） # 查看容器占用 docker stats [选项] \u0026lt;容器名\u0026gt; # 选项: --all=false # 显示所有容器（默认显示运行中的容器） # --format=\u0026#34;\u0026#34; # 使用Go模板显示 # --no-stream=false # 不显示实时流容器的统计信息 # --no-trunc=false # 不截断输出 # 停止所有容器 docker stop $(docker ps -a -q) # 移除所有容器 docker rm $(docker ps -a -q) # 移除所有镜像 docker image rmi $(docker images -q) # 清空docker中所有的东西 docker system prune -a # 清空缓存 docker system prune -f # 清空未使用的镜像 docker image prune -a # 清空未使用的容器 docker container prune # 清空未使用的卷 docker volume prune # 清空未使用的网络 docker network prune # 清空未使用的构建缓存 docker builder prune # 清空未使用的数据 docker system prune -a --volumes # 清空所有未使用的数据 docker system prune -a --volumes --force 3.容器日志 # 查看容器日志 docker logs [选项] \u0026lt;容器名\u0026gt; # 选项: -f, --follow=false # 跟踪日志输出 # --since=\u0026#34;\u0026#34; # 显示自某个timestamp之后的日志，或相对时间，如42m（即42分钟） # --tail=\u0026#34;all\u0026#34; # 从日志末尾显示多少行日志， 默认是all # -t, --timestamps=false # 显示时间戳 # --until=\u0026#34;\u0026#34; # 显示自某个timestamp之前的日志，或相对时间，如42m（即42分钟） docker submodule # 获取子模块 git submodule update --init --recursive docker-compose # 启动命令 docker-compose up [选项] [服务名] # 选项 # -d 后台运行 # --build 构建镜像 # 删除容器 docker-compose rm [选项] [服务名] # 删除镜像 docker-compose down [选项] [服务名] docker私服的相关命令 # 登录 docker login # 上传 docker push \u0026lt;镜像名\u0026gt; ","permalink":"https://ysyyhhh.github.io/posts/tool/docker/docker%E5%91%BD%E4%BB%A4/","summary":"安装docker sudo apt -y update sudo apt -y upgrade sudo apt -y full-upgrade # 安装依赖 sudo apt install -y apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release # 添加官方GPG密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg #添加仓库 echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo","title":"Docker命令"},{"content":"git 常用命令 git 基本配置 git config --global user.name \u0026#34;your name\u0026#34; git config --global user.email \u0026#34;your email\u0026#34; git 基本操作 git init # 初始化仓库 git add . # 添加所有文件到暂存区 git commit -m \u0026#34;commit message\u0026#34; # 提交到本地仓库 git remote add origin git push -u origin master # 推送到远程仓库 git pull origin master # 拉取远程仓库 git clone # 克隆远程仓库 git status # 查看当前状态 git log # 查看提交日志 git diff # 查看修改内容 git branch # 查看分支 git checkout -b branch_name # 创建并切换到新分支 git checkout branch_name # 切换分支 git merge branch_name # 合并分支 git branch -d branch_name # 删除分支 git reset --hard HEAD^ # 回退到上一个版本 git reset --hard commit_id # 回退到指定版本 git reflog # 查看命令历史 git rm file_name # 删除文件 git stash # 暂存当前修改 git stash list # 查看暂存列表 git stash apply # 恢复暂存 git stash drop # 删除暂存 git stash pop # 恢复并删除暂存 git remote -v # 查看远程仓库地址 git remote set-url origin new_url # 修改远程仓库地址 git push origin --delete branch_name # 删除远程分支 git push origin :branch_name # 删除远程分支 git tag # 查看标签 git tag tag_name # 创建标签 git tag tag_name commit_id # 指定提交创建标签 git tag -a tag_name -m \u0026#34;tag message\u0026#34; # 创建带有说明的标签 git tag -d tag_name # 删除标签 git push origin tag_name # 推送标签到远程 git push origin --tags # 推送所有标签到远程 git push origin :refs/tags/tag_name # 删除远程标签 git push origin --delete tag tag_name # 删除远程标签 git checkout -- file_name # 撤销工作区修改 git reset HEAD file_name # 撤销暂存区修改 git reset --hard HEAD^ # 撤销本地提交 git reset --hard commit_id # 撤销本地提交 git config --global alias.st status # 设置别名 git config --global alias.co checkout # 设置别名 git config --global alias.ci commit # 设置别名 git config --global alias.br branch # 设置别名 git config --global alias.unstage \u0026#39;reset HEAD\u0026#39; # 设置别名 git config --global alias.last \u0026#39;log -1\u0026#39; # 设置别名 git config --global alias.lg \u0026#34;log --color --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; # 设置别名 git 子模块 # 查看子模块 git submodule git submodule add # 添加子模块 # 添加子模块并自定义子模块目录名称和分支 git submodule add \u0026lt;repository\u0026gt; [\u0026lt;path\u0026gt;] git submodule init # 初始化子模块 git submodule update # 更新子模块 git submodule foreach git pull # 更新所有子模块 # 删除子模块 # 1. 删除.gitmodules中对应子模块的条目 # 2. 删除.git/config中对应子模块的条目 # 3. 执行git rm --cached path/to/submodule # 4. 执行rm -rf .git/modules/path/to/submodule # 5. 执行rm -rf path/to/submodule ","permalink":"https://ysyyhhh.github.io/posts/tool/git/git/","summary":"git 常用命令 git 基本配置 git config --global user.name \u0026#34;your name\u0026#34; git config --global user.email \u0026#34;your email\u0026#34; git 基本操作 git init # 初始化仓库 git add . # 添加所有文件到暂存区 git commit -m \u0026#34;commit message\u0026#34; # 提交到本地仓库 git remote add origin git push -u origin","title":"git"},{"content":"kubectl 命令行工具\nkubectl [command] [TYPE] [NAME] [flags]\n- command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。\n- TYPE：指定资源类型。资源类型不区分大小写， 可以指定单数、复数或缩写形式。\n- NAME：指定资源的名称。名称区分大小写。 如果省略名称，则显示所有资源的详细信息。例如：\n命令大全\n- kubectl get:列出资源,比如 pod、deployment、service 等 - kubectl describe:显示资源的详细信息 - kubectl create:创建资源,比如 pod、deployment、service 等 - kubectl delete:删除资源 - kubectl apply:对资源进行配置更改 - kubectl rollout:管理资源的发布,比如 deployment 的发布 - kubectl scale:扩缩 pod 副本数 - kubectl expose:暴露资源为 service - kubectl logs:打印 pod 的日志 - kubectl exec:在 pod 内执行命令 - kubectl cp:在 pod 之间 copy 文件 - kubectl port-forward:将 pod 的端口转发到本地 - kubectl label:给资源加标签 - kubectl annotate:给资源加注释 - kubectl config:管理 kubeconfig 文件 - kubectl cluster-info:显示集群信息 - kubectl version:显示 CLI 版本和服务端版本 - kubectl api-versions:显示所支持的 API 版本 - kubectl api-resources:显示每个API group下的资源列表 常用命令\nkubectl get 资源类型 kubectl get pod kubectl get pod -o wide kubectl get deployment kubectl get deployment -o wide kubectl get namespace # 指定查看某个命名空间下的pod kubectl get pod -n kube-system # 查看所有命名空间下的pod kubectl get pod -A -o wide kubectl describe 资源类型 kubectl describe pod kubectl describe pod web-nginx-dep2-5f4fbd5bfb-jqw9z kubectl describe pod -o wide kubectl describe deployment kubectl describe deployment -o wide kubectl describe namespace # 指定查看某个命名空间下的pod kubectl describe pod -n kube-system # 查看所有命名空间下的pod kubectl describe pod -A -o wide kubectl logs 显示pod中的容器中运行过程中产生的日志信息 kubectl logs ngx-dep3-64cfcc9ddc-92x9s kubectl logs injoi-5c9b8f98bd-trm95 | grep \u0026#34;capturing the emotions\u0026#34; -A 100 -B 100 搜索并查看上下文 kubectl run bx --image=busybox kubectl exec -it nginx-dep1-6dd5d75f8b-mgndd /bin/bash kubectl exec -it pod对象 /bin/bash https://kubernetes.io/zh-cn/docs/\ncontainer pod Pod 类似于共享名字空间并共享文件系统卷的一组容器。\ndeployment depployment.yaml ① apiVersion 是当前配置格式的版本。\n② kind 是要创建的资源类型，这里是 Deployment。\n③ metadata 是该资源的元数据，name 是必需的元数据项。\n④ spec 部分是该 Deployment 的规格说明。\n⑤ replicas 指明副本数量，默认为 1。\n⑥ template 定义 Pod 的模板，这是配置文件的重要部分。\n⑦ metadata 定义 Pod 的元数据，至少要定义一个 label。label 的 key 和 value 可以任意指定。\n⑧ spec 描述 Pod 的规格，此部分定义 Pod 中每一个容器的属性，name 和 image 是必需的。\nsecret Secret 是 Kubernetes 中的一种资源,用于存储敏感信息,比如密码、OAuth 令牌、SSH 密钥等。Secret 的数据是 base64 编码并存储在 etcd 中。Secret 有三种类型:1. Opaque:任意数据,用于存储密码、密钥等;base64 编码后存储。\n2. kubernetes.io/service-account-token:服务账号令牌,由 Kubernetes 自动创建和更新。\n3. kubernetes.io/dockercfg:Docker 配置文件,用来存储私有 Docker Registry 的认证信息。主要用途是:- 存储敏感数据,比如密码、密钥、认证信息等\n- 在 Pod 中设置环境变量\n- 用于拉取私有镜像仓库的镜像创建 Secret 有三种方式:1. 从文件中创建:\nbash kubectl create secret generic \u0026lt;secret-name\u0026gt; --from-file=path/to/file 2. 从字串中创建:\nbash kubectl create secret generic \u0026lt;secret-name\u0026gt; --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; 3. 编写 YAML 文件创建:\nyaml apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= # base64 编码后的密码或密钥 password: MWYyZDFlMmU2N2Rm # base64 编码后的密码或密钥 在 Pod 中可以以三种方式使用 Secret:1. 设置环境变量的值 env\n2. 以 Volume 文件的形式挂载,然后在 Volume 中访问\n3. 使用 kubectl 在本地执行工具中设置 Secret总的来说,Secret 用于在 Kubernetes 集群中存储敏感信息,有以下主要用途:- 存储密码、密钥、认证信息等敏感数据\n- 用于在 Pod 中设置环境变量的值\n- 用于在 Volume 中创建 config 文件\n- 拉取私有 Docker Registry 的镜像\nnamespace ","permalink":"https://ysyyhhh.github.io/posts/tool/k8s/k8s/","summary":"kubectl 命令行工具 kubectl [command] [TYPE] [NAME] [flags] - command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 - TYP","title":"k8s"},{"content":"并行程序 void sinx(int N, int terms, float* x, float* result) { for (int i = 0; i \u0026lt; N; i++) { float value = x[i]; float number = x[i] * x[i] * x[i]; int sign = -1; int denom = 6; for (int j = 0; j \u0026lt; terms; j++) { value += sign * number / denom; sign *= -1; denom *= (2*j+3)*(2*j+2); number *= x[i] * x[i]; } result[i] = value; } } 转换成汇编后大致如下:\nld r0, addr[r1] mul ri, r0, r0 mul r1, r1, r0 可以看到每次循环都是独立的。 对于最简单的是顺序执行。 通过超线程(超标量处理器具有从单个指令流中提取多个指令的能力)可以提高性能。有时称指令级并行性。(ILP, Instruction Level Parallelism) 但在这些汇编指令中必须顺序执行。 因此实现指令级并行性是一个挑战。 但即使是纯顺序执行的代码,也有很多方式使其运行更快(基于写代码的方式和编译器的智能程度). Pentium 4 比如先取多条指令等. (有个黑匣子会预测分支,预测错误的话就会清空流水线,浪费时间) 解决方法: 1. 通过pthread编写并行性的程序 2. 假设有一种语言可以表示并行性,编译器可以自动并行化程序 如: forall(int i from 0 to n-1){} 自动并行化可能的解决方法: 1. 直接分为k个线程,每个线程处理n/k个循环. 然后将结果合并 2. 在硬件上执行. 有一堆性能较低但具有并行性的处理器时, 也需要更多电力/时间来驱动很多信号从一端到另一端. ## CPU \u0026amp;\u0026amp; GPU GPU将核心的概念带到了极致, 抛弃了所有的分支预测, 只是控制逻辑而不完成计算. 对于上面的程序有垂直和水平两种分割方式: - 垂直: 每个线程处理一个循环 - 水平: 同时处理多个循环, 如先同时进行所有的第一个乘法... ## SIMD Single Instruction Multiple Data 假设我正在执行的多次操作之间没有依赖关系,都能够并行运行. a single instruction is applied to multiple data elements simultaneously. 即: 同时对8个数值和另一个地方的8个数值取出并进行加法. 有时这些数值可以被称作向量. 使用AVX intrinsics的向量化程序: ```c++ void sinx(int N, int terms, float* x, float* result) { for (int i = 0; i \u0026lt; N; i+=8) { __m256 origx = _mm256_load_ps(\u0026amp;x[i]); __m256 value = origx; __m256 number = _mm256_mul_ps(origx, _mm256_mul_ps(origx, origx)); float sign = -1; __m256 denom = _mm256_set1_ps(6); for (int j = 0; j \u0026lt; terms; j++) { //value += sign * number / denom; __m256 tmp = _mm256_div_ps(number, denom); tmp = _mm256_mul_ps(tmp, _mm256_set1_ps(sign)); value = _mm256_add_ps(value, tmp); sign *= -1; //denom *= (2*j+3)*(2*j+2); denom = _mm256_mul_ps(denom, _mm256_set1_ps((2*j+3)*(2*j+2))); //number *= x[i] * x[i]; number = _mm256_mul_ps(number, _mm256_mul_ps(origx, origx)); } _mm256_store_ps(\u0026amp;result[i], value); //result[i] = value; } } 编译成汇编后大致如下:\nvloadps xmm0, addr[r1] vmulps xmm1, xmm0, xmm0 vmulps xmm2, xmm1, xmm0 ... ... ... vstoreps addr[xmm2], xmm0 AVX代表高级矢量扩展, 256代表每次可以处理256位的数据, 也就是8个float. 有多个版本:\nAVX: 128位 = 4 * 4 * 8 = 32字节 AVX2: 256位 = 8 * 4 * 8 = 32字节 AVX512: 512位 = 16 * 4 * 8 = 64字节 XMM寄存器是特殊的32字节 256位寄存器, 有16个, 从xmm0到xmm15. 用于支持vectorized SIMD指令.\n那么有没有办法让编译器自动将代码向量化呢?\n有,GCC的-O3选项可以自动向量化代码. 但只有非常结构化,精心编写的代码才能被自动向量化.\n条件 如果加入条件判断,如何向量化?\nif(x \u0026lt; 0){ x = -x; }else{ x = x; } SIMD可能的做法: 设置一个掩码, 用于标记哪些元素需要执行哪些不需要执行.\nx \u0026lt; 0: 1 1 0 0 1 0 0 0 x = -x: 1 1 0 0 1 0 0 0 翻转: 0 0 1 1 0 1 1 1 x = x: 0 0 1 1 0 1 1 1 但大多时候只保留了一半的效率,因为每次有可能只有一半的数据需要执行. 不过这很好的保证了一致性,因为分支结束后又回到了同一个执行路径. 即保持一致性,远离分歧.\ncoherent execution: 所有的线程都执行相同的指令.\ndivergent: a lack of instruction stream coherence.\n对于生成这些矢量操作,要么有聪明的编译器,要么就是有耐心的程序员.\nSIMD execution on many modern GPUs SPMD: Single Program Multiple Data\nGPU给的不是SIMD,而是SPMD. 单个程序,多个数据. 意味着程序的不同部分可以执行不同的指令.\n在这之下,还是用SIMD来实现大部分逻辑,采用异构的方式来实现并行.\n但有n个加法, 即两个包含n个值的向量相加. 实际上不是所有单位都在等待计算.而是会先计算出如何分配到块中,底层块的实际大小是32, 32values而不是32byte. 这个被称作SIMD宽度,一般是8-32.\nGPU和CPU的差别 CPU i7:\n4核 8 SIMD ALUs per core 每秒大概几千次浮点运算 GPU: RTX 1080\n20 cores 32 SIMD ALUs per core 每秒大概8m次浮点运算 GPU的核心摒弃了分支预测等只用做control,因此可以有更多的ALU.填充进来.\n大概是80:1的原始计算能力差异.\n总结 三种方法实现并行计算\n多核CPU:\n线程级实现并行 SIMD:\n指令级并行 通过向量化指令实现 但依赖于事先知道执行的指令优先级顺序 Superscaler: exploit ILP within an instruction stream\npaart2 accessing memory Memory latency: 从CPU到内存的时间\nexample: DRAM访问时间 100 cycles, 100ns Memory bandwidth: 从内存到CPU的时间\nexample: 20GB/s 其实不是很快 Stall: CPU等待内存的时间 当cpu试图进行读取而内存不可用时，就会停等知道内存可用.\n缓存就是为了解决Stall的问题.\n在多级缓存中,靠近核心的缓存是私有的. 这样可以通过写入读出L2缓存的数据来实现通讯,而不需要经过DRAM.\n缓存对延迟和带宽都有帮助.\nPrefecthing reduces stalls 硬件通常通过预取来减少延迟. 即预测下一次可能会访问的数据,并将其提前读取到缓存中. 不过可能会造成信息泄露\n使用预取的效果: Multi-threading reduces stalls 让多个线程交替进行, 如asst1/prog2的实现\n这也是超线程的实现,在一个核心中多路复用多个指令流. 对于CPU\u0026amp;GPU, 谁来组织线程是不同的做法.(操作系统 or 硬件)\n通常情况下内存要比其他因素更加限制速度\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l2/","summary":"并行程序 void sinx(int N, int terms, float* x, float* result) { for (int i = 0; i \u0026lt; N; i++) { float value = x[i]; float number = x[i] * x[i] * x[i]; int sign = -1; int denom = 6; for (int j = 0; j \u0026lt; terms; j++) { value += sign * number / denom; sign *= -1; denom *= (2*j+3)*(2*j+2); number *= x[i]","title":"L2"},{"content":"主要用三种方式实现并行程序(没有进行真正的优化)\n例子 n-body simulation\n创建并行程序的过程\n1. Decomposition 主要思想: 创造至少足够的任务让所有的处理单元都有事情做\nAmdahl\u0026rsquo;s Law: 串行部分的比例越大, 并行程序的加速比就越小,因为增加处理单元的数量并不能减少串行部分的时间\n分解的任务更多是程序员的工作, 编译器还无法很好的帮助我们\n2.Assignment 需要考虑让每个处理单元尽可能减少沟通.\n有一种方法是随机分配,但会最大化沟通 还有一个极端是全部由一个处理单元完成,但是这样就没有并行了\n这是另一个挑战\n分配可以静态也可以动态发生\n静态: 在程序开始时就确定好. 动态: 在程序运行时分配 静态分配的问题:\n无法适应不同的输入(如:工作量不均匀) 无法适应不同的处理单元数量 动态分配: 通过消息传递来实现, 每个处理单元都有一个队列, 用来存放需要处理的任务(tasks). 当一个处理单元完成了一个任务, 就从队列中取出一个任务来处理 缺点: 队列需要同步, 会有额外的开销\n3. Orchestration 编排阶段 编排的目标是: 减少沟通和同步的成本, preserve locality of data reference, reduce overhead.\n4.mapping 这是程序员最不需要关心的, 交给编译器就好了 example 顺序程序: 那么如何并行执行呢?\nStep1: identify dependencies(problem decomposition) 因为会迭代很多次,所以会引起不同迭代次数的数据竞争.\n有一种划分方法是沿着对角线: 不足之处是:\n有些对角线很短, 负载不均衡 需要额外的计算(对角线下标) 另一种方法是滚动数组: 用两个数组, 一个用来存放当前迭代的结果, 一个用来存放上一次迭代的结果\n这样计算时不会有数据竞争.\n但很多人不希望有额外的内存开销.\n事实上使用的是红黑排序.\n每次迭代只更新红色的部分, 然后再翻转. 这样就不需要复制数组了.\nStep2: assign tasks 我们不把每一个元素作为一个任务,而是把每一行作为一个任务.\n同时: 红黑排序有一个同步的步骤: 必须等待所有的红色部分都计算完毕, 才能开始计算黑色部分.\n为了最小化沟通, 相邻行作为捆包是更好的选择, 这样只在更新边界时需要沟通.\n三种实现方法 Data-parallel expression of solver 这个的特点是系统做了很多工作, 程序只需要指定哪里需要并行.\nshared-address-space code version1 : 但是有个锁会使得程序变慢 version2: 有三个barrier来保证红黑顺序 为什么是三个呢?\n每一部分都要被分割\n最后一个是为了diff的分割 第一个是为了myDiff的分割 第二个是为了diff的分割\n所以可以使用diff数组\nversion3: barrier的问题: barrier还是有点笨重, 这会强制所有线程到一个起跑线 但如果有更精确的信息, 只需要等待依赖的线程就好了\nmessage-passing code 需要有额外的划分,来存储相邻处理器的数据\n同时,在最后计算diff时,需要等待所有的处理器都计算完毕. 这里选中了一个processor zero来计算diff, 其他的处理器都发送自己的diff给它.\n但沟通时有可能发生死锁. 因为每个处理器都在等待其他处理器的消息, 但是自己的消息又没有发送出去.\n所以需要分奇偶来发送\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l4/","summary":"主要用三种方式实现并行程序(没有进行真正的优化) 例子 n-body simulation 创建并行程序的过程 1. Decomposition 主要思想: 创造至少足够的任务让所有的处理单元都有事情做 Amdahl\u0026rsquo;s Law: 串行","title":"L4 Parallel Programing basics"},{"content":"三种分配策略的总结 静态分配 优点:\n几乎没有运行时的开销(关于分配) 缺点:\n不总是均匀的分配任务 什么时候使用:\n(最简单的例子) 当知道每个任务的工作量相当的时候 当每个任务的工作量是可预测的,但不一定相等的时候 半静态分配\n场景: 当工作量会随时间发生改变,当变化比较慢时.(任务量不可预测) 做法: 定期的重新分配任务 动态分配 场景: 当每个任务的工作量或者任务的数量是不可预测的时候\n每个计算单元都要去获取任务\n但这样的实现, 每次的任务可能会很少, 会使得更多的开销在争夺锁(获取任务的锁)上面.\n有一个办法是一次性计算更多的任务.\n但分配更多的任务可能会导致负载不平衡.\n因此需要在分配任务数量上要找一个平衡, 不花费过多的时间在争夺锁上, 也不会导致负载不平衡.\nSchedule long tasks first 但如果有一个大任务在最后，将出现如下情况： 因此，如果知道有一个大任务，可以提前处理，而不是放到最后一个.\nWork stealing 当一个计算单元没有任务的时候, 从其他计算单元那里偷取任务.\n实现的一些问题:\n1.从哪个线程开始偷取任务呢? 有随机的, 也有从最后一个开始偷取的.\n2.应该偷取多少任务呢? 应该偷取尽可能多一些,这样可以减少偷取任务的次数.\n3.怎样检测一个计算单元是否有任务呢? 可能会循环遍历,\n4.使用本地队列(分布式队列)会更快(在有互斥锁的情况下)\n还有一种方式是使用特殊的数据结构来存储任务间的依赖关系, 从而可以在任务完成的时候, 自动的调度下一个任务. 缺点是额外开销 常见的并行编程模式 循环 创建显示线程 递归时的并行\n递归可以编写出简单的代码, 但是递归的并行化是比较困难的.\n因为递归的并行化需要在递归的每一层都要进行并行化, 并且需要在每一层都要进行同步.\n但只要有独立的子问题, 就可以创造很多潜在的并行性.\nFork-Join pattern cilk_spawn: 会创建一个新的线程, 并且在新的线程中执行函数, 并且不会阻塞当前的线程.\ncilk_sync: 会等待所有的子线程执行完毕, 并且会阻塞当前的线程.\n每个函数的结尾隐式的调用了cilk_sync.\nexample: 有一个主线程+fork的线程. 快排的例子: 在规模较小的时候, 使用串行的快排. 这样可以减少线程的创建和销毁的开销. 不要忽略了抽象和实现的区别. spawn不是生成一个具体的线程, 而是声明这里有一个可以并行的任务.\n任务的数量至少需要比硬件线程多,但也不能大于100倍. 8倍是一个比较好的选择. Cilk的实现 假设我们要去实现clik_spawn 和 cilk_sync 线程池的实现(CILB):\nthread1 需要找到一种方法来发现有新的任务可以执行. 所以thread 0不能简单的调用foo, 它的作用是执行foo.\n但需要在执行foo前,把特殊的东西放入工作队列中.\n此时如果另一个线程突然变得空闲, 它就可以从工作队列中获取任务.\n为什么不把foo放入队列, 直接执行bar呢?(上面是执行foo bar放入队列)\n这涉及到 continuation first(child stealing) 和 child first(continuation stealing) 的问题.\ncontinuation first会导致线程0的大量工作排队.(广度优先队列) child first会导致其他线程把下一个任务偷走时, 会导致线程0的工作队列为空.(深度优先队列)\n实际上child first是合理的.(在递归中是最合适的)\n在递归程序中,会先将所有深度的任务放入队列中.\n按照之前优先执行大任务的策略, 其他线程会优先从队列顶部(先入的)中偷取任务. 因为在分而治之的算法中, 大任务会被分解成小任务, 因此大任务会先被放入队列中.\n实际中使用了双端队列:\n从队列头部获取任务 从队列尾部放入任务 但之前有一个问题: 很多队列,该从哪个队列中获取任务呢? 也许是随机的. 偷取任务的时候, 不随机的更可能会引起负载不均衡.\n本地线程访问的是本地队列的尾部, 偷取时也是放入尾部.(偷其他队列的头部) 这样也有利于空间局部性.\n那么如何实现同步呢?\nExample1: stalling join policy 拖延政策: 所有我创建的任务都必须完成后, 我才能继续执行. Example2: greedy join policy(cilk的实现方法)\n有一个跟踪数据结构,但那个东西可以四处移动.\n最后一个完成的线程会偷走这个数据结构\n所以一旦最后一个任务完成, 就可以继续执行了.\n这样不会浪费时间等待同步.\n第一个方法实现起来更简单,但速度更慢. 因为它总是首线程只等待其他线程完成.\n总结 ","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l5/","summary":"三种分配策略的总结 静态分配 优点: 几乎没有运行时的开销(关于分配) 缺点: 不总是均匀的分配任务 什么时候使用: (最简单的例子) 当知道每个任务的工作","title":"L5 Work distribution and scheduling"},{"content":"虚拟文件系统\n/proc/cpuinfo\nmodel name cpu MHz - 频率 cache size - 缓存大小 siblings - 逻辑cpu数量 processor - 逻辑cpu编号 cpu cores - 物理cpu数量 core id - 物理cpu编号 这样查看的cpu数量很多\nsiblings是逻辑cpu的数量\ncpu cores是物理cpu的数量\n为什么报告的processor数量是40而siblings是20呢? 因为报告的processor包括超线程的逻辑cpu. 这样操作系统就可以直接根据逻辑cpu的数量来分配任务.\nMemory bandwidth - 内存带宽 Power consumption - 功耗 能源消耗实际上是一个很大的问题. Intel code name - 代号 Functional units\nlatency - 延迟 issue time - 发射时间 capacity - 容量 微处理架构\nfunction units latency - 延迟，执行一个指令所需要的时钟周期数(不包括等待) issue time - 发射时间，指令发射到执行所需要的时钟周期数(包括等待) capacity - 容量 优化的地方:\n搞清楚到底哪些代码是执行次数最多的(内部循环)(对实际使用情况来说) 基本运算消耗时间: 除法 \u0026gt; 乘法 \u0026gt; 加法 \u0026gt; 位移 基本的程序: 合并重复计算的简单的提升: 将除法次数减少,(不依赖于内层循环的变量的计算拿出来)\n循环展开 loop unrolling\n如果每一次循环都要进行一次是否终止的测试,开销会很大.(尤其是一次循环的计算 相比于 循环次数来说很小 时)\n所以处理器从简单的策略开始,如预测循环的次数. 大部分都是基于统计预测的.\n如果可以预测循环的次数,就可以将循环展开. 每次循环多执行4 或 8 或\u0026hellip;次原来循环做的事情.\n但展开时不一定均匀,\nuniform可以使得循环展开的更好.\n为什么8维向量获得了超过8倍的加速呢? 因为uniform, 原本要做8次的判断,现在只需要做一次.\n常规优化提升了15倍 向量优化提升了5.4倍 总计提升了82倍\n向量化很好且是free的,但不能忽略了传统的优化\n传统的优化(213 program)使得速度提升了三倍\n要做到极致的优化,就比如要花3个星期的时间在编码风格上, 最后30分钟花在向量化上.\n但要看情况来决定编码风格的优化. 因为如果我们编写的代码不是执行次数最多(如内核,场景仿真,高频), 那么可能更需要的是可读性.\n可读性变差 可能会导致bug很容易被引入, 并且非常不容易被发现和维护.\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l6/","summary":"虚拟文件系统 /proc/cpuinfo model name cpu MHz - 频率 cache size - 缓存大小 siblings - 逻辑cpu数量 processor - 逻辑cpu编号 cpu cores - 物理cpu数量 core id - 物理cpu编号 这样查看的cpu数量很","title":"L6"},{"content":"GPU\n图形渲染 图像中的每个对象都有很自然的并行性。\n","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l7/","summary":"GPU 图形渲染 图像中的每个对象都有很自然的并行性。","title":"L7"},{"content":"lambda表达式 描述:\n一个匿名函数对象 一个可调用的代码单元 一个函数对象的语法糖 语法规则: {};\n[]: lambda表达式的引导符 (): 参数列表 {}: 函数体\n具体示例\nfor (auto \u0026amp;thread: threads) { thread = std::thread([\u0026amp;taskId, num_total_tasks, runnable] { for (int id = taskId++; id \u0026lt; num_total_tasks; id = taskId++) runnable-\u0026gt;runTask(id, num_total_tasks); }); } 在示例中, thread的初始化是一个lambda表达式, 该lambda表达式的参数列表为空, 函数体为\n{ for (int id = taskId++; id \u0026lt; num_total_tasks; id = taskId++) runnable-\u0026gt;runTask(id, num_total_tasks); } 这样一个thread可以负载多个任务.\n如果不用lambda表达式, 那么就需要定义一个函数, 然后将函数的地址传递给thread, 这样就会增加代码量.\n也就是说实际上 thread的参数可以是一个函数对象, 也可以是一个函数指针, 也可以是一个lambda表达式.\nexplicit 不能在传参时隐式调用构造函数。 使用reset来用子类覆盖父类 auto new_node = new TrieNodeWithValue(std::move(**end_node), value); end_node-\u0026gt;reset(new_node); ","permalink":"https://ysyyhhh.github.io/posts/language/c++/modern-c++/","summary":"lambda表达式 描述: 一个匿名函数对象 一个可调用的代码单元 一个函数对象的语法糖 语法规则: {}; []: lambda表达式的引导符 (): 参数列表 {}: 函数体 具","title":"Modern C++"},{"content":"常用命令 # 连接 mongosh ip[:port]/database -u username -p password # 查看数据库 show dbs # 切换数据库 use database # 查看集合 show collections # file # 查看集合数据 db.{collection}.find() # 按条件查看集合数据 ## pid=1 db.{collection}.find({pid:1}) ## 限制4条 db.{collection}.find().limit(4) ## 只显示其中一个字段 db.{collection}.find({}, {name:1}) ## 统计数量 db.{collection}.find().count() ## 全部删除 db.{collection}.remove({}) ## 插入或更新数据 ","permalink":"https://ysyyhhh.github.io/posts/tool/sql/mongodb/","summary":"常用命令 # 连接 mongosh ip[:port]/database -u username -p password # 查看数据库 show dbs # 切换数据库 use database # 查看集合 show collections # file # 查看集合数据 db.{collection}.find() # 按条件查看集合数据 ## pid=1 db.{collection}.find({pid:1}) ## 限制4条 db.{collection}.find().limit(4) ## 只显示其","title":"mongoDB"},{"content":"MySql常用命令 修改用户 修改密码\nalter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;123456\u0026#39;; 修改用户host\nupdate mysql.user set host = \u0026#39;%\u0026#39; where user = \u0026#39;root 刷新权限 flush privileges; 添加一个远程用户 create user \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;password\u0026#39; GRANT all ON *.* TO \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39;; grant all privileges on *.* to \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; with grant option; *.*所有数据库下的所有表 删除用户 drop user \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39;; 创建数据库并设定中文编码 CREATE DATABASE `db_name` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 登录格式 mysql -h #{数据库IP} -P 3306 -u #{用户名} -p -D #{数据库名} 自增id 不连续时 SET @auto_id = 0; UPDATE 表名 SET 自增字段名 = (@auto_id := @auto_id + 1); ALTER TABLE 表名 AUTO_INCREMENT = 1; 文件 source 数据库 设置数据库的字符集\nalter database 数据库名 character set utf8; 表 # 添加一列 alter table 表名 add column 列名 类型; 数据 # 插入数据 insert into 表名 (字段1,字段2) values (值1,值2); # 更新数据 update 表名 set 字段1=值1,字段2=值2 where 条件; # 删除数据 delete from 表名 where 条件; 时间处理 Date\n条件语句 CASE\n强制转换\nCAST\n","permalink":"https://ysyyhhh.github.io/posts/tool/sql/mysql/","summary":"MySql常用命令 修改用户 修改密码 alter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified with mysql_native_password by \u0026#39;123456\u0026#39;; 修改用户host update mysql.user set host = \u0026#39;%\u0026#39; where user = \u0026#39;root 刷新权限 flush privileges; 添加一个远程用户 create user \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;password\u0026#39; GRANT all ON *.* TO \u0026#39;remote\u0026#39;@\u0026#39;%\u0026#39;; grant","title":"MySQL"},{"content":"nginx安装 1. 安装依赖 yum install -y gcc gcc-c++ autoconf automake make yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 2. 转发后端图片 # 1. 创建目录 mkdir -p /data/nginx/cache # 2. 修改目录权限 chown -R nginx:nginx /data/nginx/cache nginx 命令 # 启动 nginx # 重启, 重新加载配置文件 nginx -s reload # 停止 nginx -s stop # 测试配置文件是否正确 nginx -t ","permalink":"https://ysyyhhh.github.io/posts/tool/nginx/nginx/","summary":"nginx安装 1. 安装依赖 yum install -y gcc gcc-c++ autoconf automake make yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 2. 转发后端图片 # 1. 创建目录 mkdir -p /data/nginx/cache # 2. 修改目录权限 chown -R nginx:nginx /data/nginx/cache nginx 命令 # 启动 nginx","title":"nginx"},{"content":"NoRBERT: Transfer Learning for Requirements Classification Tobias Hey， Jan Keim， Anne Koziolek， Walter F. Tichy Karlsruhe Institute of Technology (KIT)\n引用 T. Hey, J. Keim, A. Koziolek and W. F. Tichy, \u0026ldquo;NoRBERT: Transfer Learning for Requirements Classification,\u0026rdquo; 2020 IEEE 28th International Requirements Engineering Conference (RE), Zurich, Switzerland, 2020, pp. 169-179, doi: 10.1109/RE48521.2020.00028.\n论文：https://sdq.kastel.kit.edu/publications/pdfs/hey2020.pdf 仓库：https://github.com/tobhey/NoRBERT\n摘要 本文提出了NoRBERT，它使用了BERT模型进行微调，在需求工程领域进行迁移学习。他在PROMISE NFR数据集上，对功能性和非功能性需求的分类任务F1分数高达94％。对于分类非功能性需求的子类，超越了最近的方法，最常见的类别平均F1分数达到87%。在一个未见过的项目设置下，它比最近的方法高出15个百分点。此外，我们根据所包含的关注点，即功能、数据和行为，来分类功能性要求，标记了PROMISE NFR数据集中的功能性需求，并应用于测试NoRBERT，最终取得了高达92%的F1分数。NoRBERT提高了需求分类的准确性，并且可以很好地应用于未见过的项目。\n1 引言 需求的主要来源仍然是自然语言文档。对需求进行分类对于在项目早期识别出特定需求（如与安全相关的需求）非常重要。尽管已有的自动分类方法在多样化数据集上表现不错，但它们在未见过的项目上的性能会下降，因为需求的表述和结构依赖于项目和作者，而现有方法缺乏泛化能力。为了解决这个问题，我们探索了迁移学习方法，这种方法在NLP领域广泛应用，通过在大型数据集上训练来捕捉文本的深层含义，并能针对特定任务进行微调，从而在少量数据下实现更好的性能和泛化。 我们提出了NoRBERT方法，它基于BERT模型。BERT是一种预训练在大型文本语料库上的语言模型。BERT可以通过提供少量数据在特定任务上进行微调。我们使用NoRBERT在PROMISE NFR数据集上进行需求分类，并测试了其在重新标记的NFR数据集上的泛化能力。此外，我们还使用NoRBERT根据Glinz提出的关注点对功能性需求进行分类。这种方法在需求工程中特别有用，因为它可以在有限的标记数据情况下提供有效的分类。 本文的贡献包括以下三方面： 我们研究了通过迁移学习对已知和未知项目的需求进行分类的改进程度。 我们提供了一个新的数据集，根据功能、数据和行为的关注点进一步对功能性需求进行分类。 我们评估了基于迁移学习的方法在新数据集和任务上的表现。\n2 技术介绍 2.1相关工作 在需求工程领域，功能性需求与非功能性需求的区分是一个广泛研究的主题。功能性需求指的是系统必须执行的具体任务，而非功能性需求则涉及系统的质量标准，如性能、可靠性等，它们没有明确的执行标准。一些研究者将非功能性需求视为质量目标，而另一些则将其归类为系统属性或约束。 自动从文档中提取和分类需求是研究者关注的重点，已经发展出多种方法。例如，Cleland-Huang等人利用信息检索技术，通过识别指示词来分类需求，但这种方法在精确度上有所欠缺。Hussain等人通过使用特定词汇和决策树分类器改进了需求分类。Kurtanović和Maleej则采用自动化特征选择来预测需求类别。还有研究者使用深度学习技术，如卷积神经网络（CNN），来分类需求。 尽管这些方法展示了不同技术的潜力，但它们在实际应用中可能存在局限性，如过度依赖特定数据集、对措辞和句子结构敏感，或需要手动预处理。此外，这些方法在泛化到新项目上的能力上也存在不足。为了解决这些问题，我们尝试迁移学习方法，以期望在较少的训练数据下实现更好的性能和泛化能力。\n2.2 BERT介绍 BERT（Bidirectional Encoder Representations from Transformers）是一种语言模型（LM）。用于预测词序列中词的概率，具有迁移学习的能力，即能在微调后适应不同任务。它起源于词嵌入技术，如word2vec，但通过使用ELMo的双向LSTM解决了word2vec忽略词义歧义的问题。BERT采用预训练和微调的方法进一步发展了语言模型，。 BERT的核心是Transformer架构，它用自注意力编码器-解码器结构替代了LSTM。自注意力层让模型能够识别并赋予相关词更高的权重，而忽略不相关的词。Transformer的多头注意力机制使模型能同时关注不同位置的词，提高了对词义的理解。 BERT的训练结合了双向预训练和掩码语言建模。在训练中，一部分输入词被掩码，模型需要预测这些词，同时学习判断句子是否可能连续。这种训练方法使BERT在多个NLP任务上取得了优异的表现。 BERT有两个版本：基础模型和大模型，分别具有不同数量的编码器层、隐藏单元和注意力头，参数数量也有所不同。基础模型有1.1亿参数，而大模型有3.4亿参数。BERT最初是在英文维基百科和BooksCorpus上训练的。BERT和类似的方法目前正在取代传统的自然语言处理系统。Tenney等人对BERT的不同层次和底层学习结构的分析表明，BERT重塑了类似于NLP流水线的相似结构。 图1：用于微调 BERT 进行分类的架构。 图1展示了如何使用BERT进行分类。输入被分词。BERT的第一个输入标记始终是特殊标记[CLS]。标记[SEP]是一个特殊的分隔符标记，例如用于分隔句子，而标记[PAD]用于填充。对于分类和类似的下游任务，BERT产生的唯一输出是BERT为第一个标记([CLS])产生的输出，这是所有标记的聚合输出。这个聚合输出可以输入到一个单层前馈神经网络中，该网络使用softmax为不同类别分配概率。\n3 实验评估 3.1 实验设置 研究问题。在本文中，我们研究以下研究问题： RQ1：迁移学习在需求分类中的表现如何？ RQ2：迁移学习是否提高了在未见项目上对需求进行分类的性能？ RQ3：迁移学习方法能够多大程度上检测到功能性需求的子类？ 评估数据集。为了回答RQ1和RQ2，我们利用了两个现有的数据集。一个是广泛使用的PROMISE NFR数据集，该数据集在RE'17数据挑战中进行了处理，另一个是由Dalpiaz等人提供了一个重新标记的版本。前者来自15个项目的625个需求。这625个需求包括255个功能性需求和370个非功能性需求。表I显示了数据集中类别的分布以及每个类别需求的平均长度。每个需求只被标记为一个类别（F或11个NFR子类之一）。这些类别的分布不均匀。该数据集中F类要比NFR类少115个，且NFR子类的数量差异很大，从可用性的67个到可移植性的1个不等。可用性、安全性、操作性和性能是超过50个示例的类别，而容错性、法律性、可维护性和可移植性的类别则低于20个。 表I：原始 NFR 数据集的分布 由于数据集中F和NFR之间的区分有争议，并且数据集包含重复和错误标记的需求，Dalpiaz等人提供了一个重新标记的数据集。表II展示了数据集的概述。它仅由原始625个需求中的612个组成，并且仅使用了两个类别。一个需求可以具有功能性（F）或质量方面（Q）或两者都有。80个需求两者都有。230个需求仅具有功能性方面（OnlyF），而302个需求仅包含质量方面（OnlyQ）。 表II：重新标记的 NFR 数据集的分布 对比方法。基于数据集，我们将我们的方法应用于以下任务： 任务1：在原始NFR数据集上对F/NFR进行二元分类。 任务2：在原始NFR数据集上对四个最常见的NFR子类（US，SE，O，PE）进行二元和多类分类。 任务3：在原始NFR数据集中对所有NFR子类进行多类分类。 任务4：使用Dalpiaz等人提供的重新标记的NFR数据集，根据功能和质量方面对需求进行二元分类。 评估指标。对于所有任务，使用精确率（P）、召回率（R）和F1分数（F1）。对于多类分类，还报告了预测类别的加权平均F1分数（A）。 采用不同的设置来评估这些任务，包括使用.75-split描述了一个单次分成75%训练和25%测试集分割的数据集，以及使用了分层的10折交叉验证，即将数据集分割为10次，其中90%为训练集，10%为测试集，并对结果进行平均。分层分割确保数据集中类别的分布在训练集和测试集中保持一致。 为了进一步研究方法的可迁移性，使用了两种特定于项目的折叠策略。使用p-fold描述了Dalpiaz等人使用的项目级交叉验证，将数据集分割为10次，其中3个项目作为测试集，12个项目作为训练集，确保功能和质量方面的均匀分布。此外，还使用了一种留一项目交叉验证（loPo），即在n-1个项目上进行n次训练，并在留出的项目上进行测试。对于像NFR子类这样高度不平衡的二元任务，我们尝试了欠采样和过采样策略。我们还尝试了早期停止（ES）和不同的训练周期数。早期停止是一种常用的正则化技术，用于避免迭代学习器过拟合。\n3.2NORBERT: 使用BERT进行非功能性和功能性需求分类 我们使用BERT的微调版本来研究迁移学习对需求分类任务的影响。 我们使用了两种不同的预训练BERT模型，基础和大型模型，都是带词性的版本。我们还尝试了不带词性的模型，但带词性的模型表现更好。这可能是因为需求中使用的命名实体被误认为是普通名词。我们使用BERT分词器，不对需求进行预处理。在预训练模型的基础上，我们定义了输出层，即分类头。我们使用BERT的序列中的第一个标记[CLS]的输出。这个输出被输入到由一个单层的线性神经元组成的前馈神经网络的分类头。输出直接计算自加权输入的总和（再加上一些偏差）。我们使用softmax函数为不同的标签获得概率分布。在训练过程中，我们使用交叉熵损失函数，并使用以下公式量化预测分布与真实分布的接近程度。：\nH(p，q)=-∑_x▒〖p(x) log⁡〖q(x)〗 〗\np(x)表示目标概率，q(x)表示实际概率，x代表不同的标签。对于正确标签，p(x)设为1，其他错误标签设为0。损失函数旨在惩罚不准确或不确定的预测，同时奖励那些准确的自信预测。 我们使用AdamW优化器而非传统的随机梯度下降来更新网络权重。AdamW引入了权重衰减校正，但不补偿偏差。我们设置权重衰减为0.01，最大学习率为2e-05，与BERT原始论文中的设置相同。实验表明，批量大小16在所有测试中表现最佳。我们根据模型大小设定最大序列长度，以优化性能并避免内存问题。 在微调NoRBERT的超参数时，我们认为增加训练周期可以提高模型对训练数据的拟合度，但也可能增加过拟合的风险。实验显示，对于二元分类，10到32个训练周期，对于多类分类，10到64个训练周期效果最佳。\n3.3任务1分类功能性和非功能性需求 对于第一个任务，我们想要衡量NoRBERT在原始PROMISE NFR数据库上将需求分类为功能性（F）或非功能性（NFR）的性能。 我们使用分层10折交叉验证设置来回答关于迁移学习在需求分类性能方面的RQ1。我们训练了二元分类模型，即预测一个需求是F还是NFR，并与其他最新方法进行比较。 表III：在 PROMISE NFR 数据集上的 F/NFR 分类。粗体数值显示每个类别的每个指标的最高得分。 表III显示了我们的结果与其它方法报告的结果的比较。NoRBERT 在功能性需求和非功能性需求的 F1 分数上分别为 90% 和 93%。在NFR上，除了依赖手动提供的字典和规则进行数据预处理的Abad等人的方法外，NoRBERT超过了所有其他方法。NoRBERT与之相比不需要手动预处理，因此可以轻松地迁移到任何其他数据集。 我们的10折交叉验证结果显示，模型性能不受BERT模型选择或训练周期数的显著影响。Kurtanovi ´c和Maleej在功能性需求上取得了更高的F1分数，但他们的方法可能存在过拟合问题。 表III中的结果也有助于回答关于我们方法泛化能力的RQ2。10折没有考虑到数据集包含不同项目、不同领域和措辞。为了评估在未见过的项目上的性能，我们使用了特定于项目的设置（loPo和p-fold）。结果与10折评估相似或更好。这表明NoRBERT能够从训练期间看到的措辞中泛化出来。在不同项目和领域的数据集上，NoRBERT也能保持稳定性能。这与需要针对每个项目调整的手动字典和规则定义方法形成鲜明对比。\n3.4 非功能性需求子类的分类 为了解决PROMISE NFR数据库中定义的非功能性需求（NFR）子类的分类任务。首先研究了数据库中四个最频繁的NFR子类的分类（任务2）。接着还研究了所有NFR子类的分类（任务3）。\nA. 任务2: 最常见NFR子类的分类 我们使用NoRBERT对四种最常见的NFR进行了二元和多类分类。结果显示，NoRBERT在二元分类中取得了高达83%的加权平均F1分数，在多类分类中达到了87%。NoRBERT在大多数类别上的表现都超过了Kurtanovi´c和Maleej的模型。此外，NoRBERT在不同设置下的表现也有所不同，最佳二元分类结果是在16个周期、过采样和早停的情况下获得的。 表V: NFR 数据集上所有 NFR 子类的多类别分类。16、32 和 50 表示使用的时代数，bin 表示二元分类，mult 表示多类分类，B 和 L 分别表示使用的 BERT 模型（基础/大型）。bin16 还额外使用了 OS，multiL32 使用了 ES。LDA 和 NB（朴素贝叶斯）是指 Abad 等人的方法，其中有（P）或没有（UP）预处理过的数据。 B. 任务3: 所有NFR子类的多类分类 我们使用NoRBERT对所有NFR子类别进行了多类分类。多类分类器尤其是在代表数量较少的类别上表现良好。所有多类模型在平均性能上都优于相应的二元分类器。基于BERT-large的模型在这项任务上表现最佳，这可能是因为它们具有更大的参数空间，能够更好地处理语言的微妙差异。NoRBERT在这项任务上的表现超过了Navarro-Almanza等人的方法，这表明迁移学习在这项任务上优于基于词嵌入的深度学习。 我们还探讨了NoRBERT在未见过的项目上的性能（RQ2）。在p折和loPo设置中，NoRBERT的性能略有下降，但多类分类器仍然优于二元分类器。尽管在loPo设置中NoRBERT的表现略逊于p折，但多类分类器和大型模型在所有设置中都优于二元分类器。 NoRBERT在分类NFR子类别方面表现出色，即使在训练数据有限的情况下也能识别出代表性不足的子类别。NoRBERT提供了一种可行的方法来替代手动数据预处理。\n3.5 任务4: 功能性和质量方面的分类 功能性（F）和非功能性需求（NFR）之间的区别并不总是清晰的，一些需求包含了两者的方面。因此，我们在Dalpiaz等人提供的重新标记的PROMISE NFR数据库上衡量NoRBERT的性能[9]，并将NoRBERT与Kurtanovi´c和Maleej[8]的方法进行比较。 表 VI：重新标记的 PROMISE NFR 数据集中类的二元分类。粗体数值代表每个类别每个指标的最高分。星号标记的 F1 分数与其他出版物报道的精度和召回率不匹配。 表VI显示了在重新标记集上训练的二元分类器的结果。我们使用与Dalpiaz等人相同的设置，即.75-split、10-fold、p-fold和随机种子（42），此外还评估了loPo设置。NoRBERT在所有这些设置中都优于其他方法。因此，迁移学习方法明显提高了分类需求的性能（RQ1）。 在.75分割中，NoRBERT的基础模型在纯功能或质量类别上表现更佳，而大型模型在其他类别上表现更好。在10折交叉验证中，NoRBERT的最佳模型平均优于Dalpiaz等人的最佳模型10个百分点，尤其在仅含功能方面的需求上（F1得分91% vs 73%）。p折和loPo的结果显示NoRBERT在未见项目上的迁移能力强。与Kurtanovi´c和Maleej的方法相比，NoRBERT在处理未见项目上表现更好，平均F1得分高出15个百分点。 这表明NoRBERT具有良好的泛化能力，无需重新训练即可在实际项目中使用。我们认为，在评估需求分类方法时，应更重视其在未见项目上的表现。\n3.6功能性需求的分类 NoRBERT在分类非功能性需求上表现出了良好的效果。现在研究它在分类功能性需求方面的表现。功能性需求通常根据它们所属的产品部分（例如用户界面或业务逻辑）来分类。有些模型则采用基于关注点的方法，包括功能性和行为性需求以及数据。如果我们想要开发能够自动理解功能性需求的系统，比如自动化追踪或建模系统，那么了解功能性需求的子类别就很重要，因为它们定义了需求可能的实现方式。 我们采用了Glinz提出的基于关注点的模型，它帮助我们理解功能性需求是描述系统的功能、系统展示的行为，还是仅仅是数据和数据结构。我们使用以下子类别： 功能：系统应执行的功能。 例子：系统应允许房地产经纪人查询MLS信息。 数据：应成为系统状态一部分的数据项或数据结构。 例子：审计报告应包括估计中使用的回收部件总数。 行为：系统展示的行为或由一个或多个刺激触发的反应。 例子：如果射门被标记为命中，产品应允许进攻球员定义射门。 需求可能包含多个关注点，这些类别可能会重叠，例如，“只有注册客户可以购买流媒体电影”。它既包含功能也包含行为。 为了回答RQ3，即迁移学习方法是否能够识别功能性需求中的关注点，由两位作者独立手动标记了PROMISE NFR数据集中的310个功能性需求。我们使用了Krippendorff的α（Kα）来衡量标注者间的一致性，发现功能和数据类别的一致性超过了0.8，而行为类别的一致性为0.752，虽然略低，但仍然是可接受的。 表VII: 功能性需求数据集概述 表VII显示了项目中类别的分布。需求的数量少于每个项目中类别代表的数量，因为每个需求可能包含多个关注点。项目11到14包含的需求数量较少，因为它们主要由非功能性需求组成。 A. 在功能性需求数据集上评估NoRBERT 我们使用标记好的数据集来评估NoRBERT在分类功能性需求上的表现。我们训练了二元分类模型，并在10折交叉验证和loPo（低代表性项目）设置下进行了评估。 表 VIII：使用 NoRBERT 在新数据集上对功能性需求子类进行二元分类，采用 10 折交叉验证和 loPo 设置。粗体数值代表每个类别每个指标的最高分。b 和 l 分别代表基础模型和大型模型 表VIII结果显示，NoRBERT在功能和行为类别上表现良好，但在数据类别上的召回率较低。这可以归因于这个类别缺乏训练数据和数据集的不平衡。我们发现欠采样和过采样对不同类别的影响不同。 在loPo设置中，NoRBERT的表现与10折交叉验证相似，功能类别的F1分数甚至略有提高，但数据类别的性能下降到了56%。尽管如此，与10折交叉验证相比，这个结果仍然是好的。我们还发现，在loPo设置中，大型模型在所有类别上的表现优于基础模型，这可能是因为loPo设置要求更好的泛化能力。 为了回答RQ3，我们可以得出结论，对于这些数据量，NoRBERT在处理功能性需求方面表现不错，尤其是在功能和行为类别上。虽然数据类别的表现有待提高，但通过更多的训练数据，NoRBERT的性能有望进一步提升。这可能有助于能够改进诸如追踪链接恢复或自动化建模等方法。\n","permalink":"https://ysyyhhh.github.io/posts/paper/llm/nobert/","summary":"NoRBERT: Transfer Learning for Requirements Classification Tobias Hey， Jan Keim， Anne Koziolek， Walter F. Tichy Karlsruhe Institute of Technology (KIT) 引用 T. Hey, J. Keim, A. Koziolek and W. F. Tichy, \u0026ldquo;NoRBERT: Transfer Learning for Requirements Classification,\u0026rdquo; 2020 IEEE 28th International Requirements Engineering Conference (RE), Zurich, Switzerland, 2020, pp. 169-179, doi: 10.1109/RE48521.2020.00028. 论文","title":"NoRBERT：使用迁移学习改进需求分类任务"},{"content":"L1 Relational Model \u0026amp; Relational Algebra 1.1 Databases 数据库：an organized collection of inter-related data that models some aspect of the real-world\n数据库管理系统 DBMS：the software that manages a database\n1.2 Flat File Strawman 数据库常常以CSV(comma-separated value)文件的形式存储，由DBMS进行管理。每次应用程序要读取或者更新记录时，都必须解析文件(parse files)。\n1.3 Database Management System A general-purpose(通用) DBMS is designed to allow the definition, creation, querying, update, and administration of databases in accordance with some data model. A data model is a collection of concepts for describing the data in database Examples: relational (most common), NoSQL (key/value, graph), array/matrix/vectors A schema(模式) is a description of a particular collection of data based on a data model 早期，数据库应用很难建立和维护，因为逻辑层和物理层之间存在着紧密的耦合。\n逻辑层： 描述了数据库有哪些实体和属性。 物理层： 是这些实体和属性的存储方式。 所以早期的数据库，一旦改变了物理层，逻辑层也得跟着变。\n1.4 Relational Model 人们每次改变物理层都要重新写数据管理系统，故Ted Codd注意到后提出了关系模型。\nThe relational model defines a database abstraction based on relations to avoid maintenance overhead(维护开销).\n三要点:\nStore database in simple data structures (relations). Access data through high-level language, DBMS figures out(找出) best execution strategy. Physical storage left up to(取决于) the DBMS implementation. 三概念\nThe relational data model defines three concepts:\nStructure: The definition of relations and their contents. This is the attributes the relations have and the values that those attributes can hold. Integrity(完整性): Ensure the database’s contents satisfy constraints. 实体完整性 entity integrity:主属性不能为空 参照完整性 referential integrity: 外键的值必须存在 用户定义的完整性 An example constraint would be that any value for the year attribute has to be a number. Manipulation: How to access and modify a database’s contents. 关系:\nA relation is an unordered set that contains the relationship of attributes that represent entities. So the DBMS can store them in any way it wants, allowing for optimization(允许优化).\n元组:\nA tuple is a set of attribute values (also known as its domain 域 一组相同数据类型的值的集合) in the relation. Originally, values had to be atomic or scalar(标量), but now values can also be lists or nested(嵌套) data structures. Every attribute can be a special value, NULL, which means for a given tuple the attribute is undefined.\n关系:\nA relation with n attributes is called an n-ary relation.\n键\nA relation’s primary key uniquely identifies a single tuple.\nSome DBMSs automatically create an internal primary key if you do not define one. A lot of DBMSs have support for autogenerated keys so an application does not have to manually increment(手动增加) the keys, but a primary key is still required for some DBMSs. A foreign key specifies that an attribute from one relation has to map to a tuple in another relation.\n1.5 Data Manipulation Languages (DMLs) Methods to store and retrieve information from a database.\nThere are two classes of languages for this:\nProcedural(程序上的): The query specifies the (high-level) strategy the DBMS should use to find the desired result based on sets / bags. (relational algebra 代数) Non-Procedural (Declarative 声明): The query specifies only what data is wanted and not how to find it. (relational calculus 微积分/关系演算) 一般现在都是用第二种的，我不管DBMS用什么策略，我只需要你给我我想要的数据。\n1.6 Relational Algebra 关系代数 Relational Algebra is a set of fundamental operations to retrieve(检索) and manipulate tuples in a relation.\nEach operator takes in(需要) one or more relations as inputs, and outputs a new relation. To write queries we can “chain(链)” these operators together to create more complex operations.\n5种基本关系操作\n选择,投影,并,差,笛卡儿积 传统集合运算\n二目运算: 并,差,交,笛卡尔积 专门的关系运算\n选择selection ,投影projection,连接 join,除 dividen 选择:在关系R中,选择符合条件的元组,是从行的角度进行运算.\n投影:在关系R中,选择出若干属性列组成新的关系,从列的角度\n连接:两个关系的笛卡尔积中,选取属性中满足一定条件的元组\n自然连接 内连接: 不保留悬浮元组. 外连接 outer join : 保留悬浮元组(填NULL)的连接 左连接 left outer join: 只保留左边关系R中的悬浮空组. 右连接 right outer join: 只保留右边关系R中的悬浮空组. 除: R除以S得到T, 则T包含所有在R而不在S的属性及其值. 且T和S的所有组合都在R中.\nL2 Modern SQL 2.1 Relation Languages 用户只需要使用声明性语言（即SQL）来指定他们想要的结果。DBMS 负责确定产生该答案的最有效计划。\n关系代数基于 sets (unordered, no duplicates)。 SQL基于 bags (unordered, allows duplicates)\n2.2 SQL History SEQUEL Structured English Query Language\nSQL Structured Query Language\n该语言由不同类别的命令组成：\nDML Data Manipulation Language SELECT, INSERT, UPDATE, DELETE DDL Data Definition Language Schema definitions for tables, indexes, views, and other objects. DCL Data Control Language Security, access controls. SQL仍在不断发展的\n2.3 Join 结合一个或多个表的列，产生一个新的表。用来表达涉及跨越多个表的数据的查询\nCREATE TABLE student ( sid INT PRIMARY KEY, name VARCHAR(16), login VARCHAR(32) UNIQUE, age SMALLINT, gpa FLOAT ); CREATE TABLE course ( cid VARCHAR(32) PRIMARY KEY, name VARCHAR(32) NOT NULL ); CREATE TABLE enrolled ( sid INT REFERENCES student (sid), cid VARCHAR(32) REFERENCES course (cid), grade CHAR(1) ); 找出在15-721拿到A的学术 SELECT s.name FROM enrolled AS e, student AS s WHERE e.grade = \u0026#39;A\u0026#39; AND e.cid = \u0026#39;15-721\u0026#39; AND e.sid = s.sid; 2.4 Aggregates 聚合函数 聚合函数接受一组列表，然后产生一个单一的标量值作为其输出。基本上只能在SELECT输出列表中使用！\n函数：\nAVG MIN MAX COUNT 当使用goup时,聚合函数输出为每个组的输出.\nhaving 在聚合计算的基础过滤输出结果.而不是where\nSELECT AVG(s.gpa) AS avg_gpa, e.cid FROM enrolled AS e, student AS s WHERE e.sid = s.sid GROUP BY e.cid HAVING avg_gpa \u0026gt; 3.9; 2.5 Sting Operation SQL标准是区分大小写的，而且只能是单引号！有一些函数可以处理字符串，可以在查询的任何部分使用。\nPattern Matching:\nLike 关键字\n% : matches any substrings (including empty). _ : matches any one character String Function: SUBSTRING(S, B, E) UPPER(S)\nConcatenation(连接) : || concatenate two or more strings together into a single string\n2.6 Date and Time 时间函数\n当前日期时间 NOW(), CURRENT_TIMESTAMP() 当前UNIX时间戳 UNIX_TIMESTAMP() 当前日期 CURRENT_DATE() 当前时间 CURRENT_TIME() 日期时间转换函数\n当前时间戳转换为北京时间 FROM_UNIXTIME() 北京时间转换为时间戳 UNIX_TIMESTAMP() 时间中解析年月日时间 DATE_FORMAT(date, format) select DATE_FORMAT(\u0026#39;2021-01-01 08:30:50\u0026#39;,\u0026#39;%Y-%m-%d\u0026#39;) 日期时间运算函数\n在某个时间的基础上加上或者减去某个时间 DATE_ADD(date,INTERVAL expr unit) DATE_SUB(date,INTERVAL expr unit)\n返回两个日期值之间的天数 DATEDIFF(expr1,expr2))\nselect DATEDIFF(\u0026#39;2021-01-02\u0026#39;,\u0026#39;2021-01-01\u0026#39;) 时间差函数 TIMESTAMPDIFF(unit,datetime_expr1,datetime_expr2)\nunit：天(DAY)、小时(HOUR），分钟(MINUTE)和秒(SECOND)，TIMESTAMPDIFF函数比DATEDIFF函数用起来更加灵活\n2.7 Output Redirection 你可以告诉DBMS将查询结果存储到另一个表中，而不是将查询结果返回给客户端（例如，终端）。结果存储到另一个表中。然后你可以在随后的查询中访问这些数据\nNew Table: 将查询的输出存储到一个新的（永久）表中。\nSELECT DISTINCT cid INTO CourseIds FROM enrolled; Exustubg Table:\n将查询的输出存储到数据库中已经存在的表中。该表 目标表必须有与目标表相同数量和相同类型的列，但输出查询中的列名不需要匹配。\nINSERT INTO CourseIds (SELECT DISTINCT cid FROM enrolled) 2.8 Output Control 可以用ORDER BY来对输出进行排序,后面可以加DESC, ASC来指定排序策略\n输出的数量可以用LIMIT n 进行指定,当然也可以用OFFSET 来提供一个bias。\nELECT sid, grade FROM enrolled WHERE cid = \u0026#39;15-721\u0026#39; ORDER BY grade; SELECT sid, name FROM student WHERE login LIKE \u0026#39;%@cs\u0026#39; LIMIT 20 OFFSET 10; 2.9 Nested Queries 在其他查询中调用查询，在单个查询中执行更复杂的逻辑。嵌套查询往往难以优化。\n外部查询的范围包括在内部查询中（即内部查询可以访问来自外部查询），反之不行。\n内部查询几乎可以出现在一个查询的任何部分。\nSELECT Output Targets\nSELECT (SELECT 1) AS one FROM student; FROM Clause(条件):\nSELECT name FROM student AS s, (SELECT sid FROM enrolled) AS e WHERE s.sid = e.sid; WHERE Clause SELECT name FROM student WHERE sid IN ( SELECT sid FROM enrolled ); 例子： 获取在15-445中注册的学生名字 SELECT name FROM student WHERE sid IN ( SELECT sid FROM enrolled WHERE cid = \u0026#39;15-445\u0026#39; ); 请注意，根据它在查询中出现的位置，sid有不同的范围。\n例子： 找到注册了至少一门课的最大的学生id\nSELECT student.sid, name FROM student JOIN (SELECT MAX(sid) AS sid FROM enrolled) AS max_e ON student.sid = max_e.sid; Nested Query Results Expressions: 关键字：\nALL Must satisfy expression for all rows in sub-query ANY Must satisfy expression for at least one row in sub-query. IN Equivalent to =ANY(). EXISTS At least one row is returned. 例子： 找到所有没有学生注册的课\nSELECT * FROM course WHERE NOT EXISTS( SELECT * FROM enrolled WHERE course.cid = enrolled.cid ); 2.10 Window Functions 函数： 窗口函数可以是我们上面讨论的任何一个聚合函数。也有一些特殊的窗口函数。\nROW_NUMBER: 当前列的数字 RANK: 当前列的顺序 Grouping: OVER子句指定了在计算窗口函数时如何对图元进行分组。使用PARTITION BY来指定分组\nSELECT cid, sid, ROW_NUMBER() OVER (PARTITION BY cid) FROM enrolled ORDER BY cid; 我们也可以在OVER中放入ORDER BY，以确保结果的确定性排序，即使数据库内部发生变化。\nSELECT *, ROW_NUMBER() OVER (ORDER BY cid) FROM enrolled ORDER BY cid; 重要提示：\nDBMS在窗函数排序后计算RANK，而在排序前计算ROW_NUMBER。\n找到每门课程中成绩第二高的学生\nSELECT * FROM ( SELECT *, RANK() OVER (PARTITION BY cid ORDER BY grade ASC) AS rank FROM enrolled) AS ranking WHERE ranking.rank = 2; 2.11. Commom Table Expressions 在编写更复杂的查询时，通用表表达式（CTE）是窗口或嵌套查询的一种替代方法。复杂的查询时，可以替代窗口或嵌套查询。它们提供了一种方法来为用户在一个更大的查询中编写辅助语句.\n可以理解为一个辅助表。\nWITH子句将内部查询的输出与一个具有该名称的临时结果绑定。\n例子： 生成一个名为cteName的CTE，其中包含一个单一属性设置为 \u0026ldquo;1 \u0026ldquo;的元组。从这个CTE中选择所有属性。\nWITH cteName AS ( SELECT 1 ) SELECT * FROM cteName; 我们可以在AS之前将输出列绑定到名称上\nWITH cteName (col1, col2) AS ( SELECT 1, 2 ) SELECT col1 + col2 FROM cteName; 一个查询可能包含多个CTE声明\nWITH cte1 (col1) AS (SELECT 1), cte2 (col2) AS (SELECT 2) SELECT * FROM cte1, cte2; 递归能力 在WITH后面添加RECURSIVE关键字允许CTE引用自己。这使得在SQL查询中可以实现递归。有了递归的CTE，SQL被证明是图灵完备的，这意味着它在计算上的表现力不亚于更多的通用编程语言\n例子：打印从1到10的数字\nWITH RECURSIVE cteSource (counter) AS ( ( SELECT 1 ) UNION ( SELECT counter + 1 FROM cteSource WHERE counter \u0026lt; 10 ) ) SELECT * FROM cteSource; Homework #1 - SQL CASE 语句\nSELECT name, CASE when died is not null then died-born else \u0026#39;2022\u0026#39; - born END as age FROM people where born \u0026gt;= \u0026#39;1900\u0026#39; ORDER by age DESC,name LIMIT 20; CAST AS TEXT 转换成字符串，字符串连接用 ||\nselect CAST(titles.premiered/10*10 AS TEXT) || \u0026#39;s\u0026#39; as decade, round(avg(rating),2) as avg, max(rating) as top, min(rating) as min, count(*) as NUM_RELEASES from ratings join titles on titles.title_id = ratings.title_id where decade is not null GROUP by decade order by avg DESC,decade limit 20; 第一个with后，用,隔开\nwith person_title as ( select people.name, people.person_id, crew.title_id from people join crew on crew.person_id = people.person_id join titles on titles.title_id = crew.title_id where born = 1955 and titles.type = \u0026#34;movie\u0026#34; ), person_avg as ( select person_title.name,round(avg(rating),2) as avg from RATINGS join person_title on person_title.title_id = ratings.title_id group by person_title.person_id ), quantiles as ( select * ,NTILE(10) OVER (ORDER BY avg ASC) as QuantileRating from person_avg ) select name,avg from quantiles where QuantileRating = 9 order by avg DESC,name; 把表格数据压缩成一行，用，隔开\nwith p as( select akas.title as dubbed from titles join akas on akas.title_id = titles.title_id where primary_title = \u0026#34;House of the Dragon\u0026#34; and type = \u0026#34;tvSeries\u0026#34; group by primary_title,dubbed order by dubbed ) select GROUP_CONCAT(dubbed,\u0026#39;, \u0026#39;) from p; L3 -4Database Storage 3.1 Storage The DBMS assumes that the primary storage location of the database is on non-volatile disk.\nThe DBMS\u0026rsquo;s components manage the movement of data between non-volatile and volatile storage.\nVolatile Devices:\nVolatile means that if you pull the power from the machine, then the data is lost. Volatile storage supports fast random access with byte-addressable locations. This means that the program can jump to any byte address and get the data that is there. For our purposes, we will always refer to this storage class as “memory.” Non-Volatile Devices:\nNon-volatile means that the storage device does not require continuous power in order for the device to retain the bits that it is storing. It is also block/page addressable. This means that in order to read a value at a particular offset, the program first has to load the 4 KB page into memory that holds the value the program wants to read. Non-volatile storage is traditionally better at sequential access (reading multiple contiguous chunks of data at the same time). We will refer to this as “disk.” We will not make a (major) distinction between solid-state storage (SSD) and spinning hard drives (HDD). 3.2 Disk-Oriented DBMS Overview The database is all on disk, and the data in database files is organized into pages, with the first page being the directory page.\nTo operate on the data, the DBMS needs to bring the data into memory. It does this by having a buffer pool that manages the data movement back and forth between disk and memory\nThe DBMS also has an execution engine that will execute queries. The execution engine will ask the buffer pool for a specific page, and the buffer pool will take care of bringing that page into memory and giving the execution engine a pointer to that page in memory.\nThe buffer pool manager will ensure that the page is there while the execution engine operates on that part of memory.\n3.3 DBMS vs OS DBMS的一个高级设计目标是支持超过可用内存量的数据库。因为访问disk的代价很大，所以使用disk应该要小心。我们不希望从磁盘上访问数据时停顿太久，从而拖慢其他一切。我们希望DBMS能够处理在等待从磁盘获取数据时，能够处理其他查询。 这个高层次的设计目标就像虚拟内存一样，有一个大的地址空间和一个供操作系统从磁盘引入页面的地方。 实现这种虚拟内存的方法之一是使用mmap来映射进程地址空间中的文件内容，这使得操作系统负责在磁盘和内存之间来回移动页面。 但不幸的是，如果mmap遇到页面故障，进程将会被阻塞。 如果你需要写入，你永远不想在你的DBMS中使用mmap。 DBMS（几乎）总是想自己控制事情，而且可以做得更好，因为它知道更多关于被访问的数据和被处理的查询。 可以通过使用操作系统： madvise: 告诉操作系统你打算何时读某些页面。 mlock: 告诉操作系统不要把内存范围换到磁盘上。 msync: 告诉操作系统将内存范围刷新到磁盘。 出于正确性和性能的考虑，我们不建议在DBMS中使用mmap。\n3.4 File Storage 在其最基本的形式中，DBMS将数据库存储为磁盘上的文件。有些可能使用文件层次结构，有些则可能使用单个文件 操作系统对这些文件的内容一无所知。只有DBMS知道如何解读它们的内容，因为它是以DBMS特有的方式编码的。 DBMS的存储管理器负责管理数据库的文件。它将文件表示为一个 页的集合。它还跟踪哪些数据被读和写到了页面上，以及这些页面有多少可用空间。这些页面中还有多少可用空间。 3.5 Database Pages DBMS将数据库组织在一个或多个文件中的固定大小的数据块，称为页。页面可以包含不同种类的数据（tuple、indexes等）。 大多数系统不会将这些类型混合在一页中。 有些系统会要求页面是自成一体（self-contained）的，也就是说，阅读每个页面所需的所有信息都在页面本身。读取每一页的所有信息都在页面本身 Each page is given a unique identifier. The DBMS uses an indirection layer to map page IDs to physical locations. There are three different notions of \u0026ldquo;pages\u0026rdquo; in a DBMS:\nHardware Page (usually 4KB) OS Page (usually 4KB) Database Page (512B-16KB) 存储设备保证写的操作是atomic 原子的。 这意味着，如果我们的数据库页面比我们的硬件页面大，DBMS将不得不采取额外的措施 以确保数据被安全地写出来。 因为当系统崩溃时，程序可能已经完成了将数据库页面写入磁盘的一部分\nDifferent DBMSs manage pages in files on disk in different ways.\nHeap File Organization Tree File Organization Sequential / Sorted File Organization (ISAM) Hashing File Organization At this point in the hierarchy(层次结构) we don\u0026rsquo;t need to know anything about what is inside of the pages.\n3.6 Database Heap A heap file is an unordered collection of pages with tuples that are stored in random order.\nCreate / Get / Write / Delete Page Must also support iterating over all pages. DBMS可以通过使用页面的链接列表或页面目录在磁盘上找到一个给定的页面ID\nLinked List: Header page持有指向自由页列表和数据页列表的指针。然而，如果 DBMS正在寻找一个特定的页面，它必须在数据页列表上进行顺序扫描，直到它找到它要找的页面。\nPage Directory:\nDBMS维护特殊的页面，跟踪数据页的位置以及每页的可用空间。\nNeed meta-data to keep track of what pages exist in multiple files and which ones have free space.\n3.7 Page Layout 每个页面都包括一个header，记录关于页面内容的元数据。 Page size Checksum DBMS version Transaction visibility Self-containment(自成一体) (Some systems like Oracle require this.) For any page storage architecture, we now need to decide how to organize the data inside of the page.\nWe are still assuming that we are only storing tuples.\nTwo approaches:\nTuple-oriented (其实是 slotted-page) 页面将slots映射到offsets,slot array 记录对应tuple的便宜量 Header keeps track of the number of used slots, the offset of the starting location of the last used slot, and a slot array, which keeps track of the location of the start of each tuple. To add a tuple, the slot array will grow from the beginning to the end, and the data of the tuples will grow from end to the beginning. The page is considered full when the slot array and the tuple data meet Log-structured 3.8 Tuple Layout tuple 内部的结构\ntuples本质上是一个字节序列。DBMS的工作是将这些字节解释为属性类型和值。\nTuple Header：包含了tuple的元数据\nDBMS的并发控制协议的可见性信息。关于哪个事务创建/修改了该元组 NULL值的位图。 注意，DBMS不需要在这里存储关于数据库模式的元数据。 Tuple Data：数据的实际属性\n属性通常按照你创建表时指定的顺序存储 大多数DBMS不允许一个tuple超过一个页面的大小。 Unique Identifier\n数据库中的每个tuple都被分配一个唯一的标识符 一般是：page_id + (offset or slow) 一个应用程序不能依赖这些ID来表示任何东西 De-normalized Tuple Data:\n如果两个表是相关的，DBMS可以 \u0026ldquo;pre-join\u0026quot;它们，所以这些表最终会出现在 在同一个页面上。这使得读取速度加快，因为DBMS只需要加载一个页面而不是两个 独立的页面。然而，这使得更新更加昂贵，因为DBMS需要更多的空间给每个 tuples\n3.9 Log-Structured Storage （也叫Append-only Sequence of Data） 参考: Log-Structured 结构\n与Slotted-Page Design有关的问题是： Fragmentation: 删除tuple会在page中留下空隙。\nUseless Disk I/O:\n由于非易失性存储的block-oriented的性质，需要读取整个块来获取tuple。\nRandom Disk I/O: 磁盘阅读器可能不得不跳到20个不同的地方来更新20个不同的tuples，这可能会非常慢。\n如果我们在一个只允许创建新数据而不允许覆盖的系统上工作呢？日志结构的存储模型与这个假设相配合，解决了上面列出的一些问题。\nLog-Structured Storage: DBMS不存储tuples，只存储日志记录。\n将数据库如何被修改的记录存储到文件中（放入和删除）。每条日志包含tuples的唯一标识符 要读取一条记录，DBMS会从最新的到最旧的逆向扫描日志文件，并 \u0026ldquo;重新创建 \u0026ldquo;这个 tuple。 写的快，读的可能慢。磁盘写入是连续的，现有的页面是不可改变的，这导致了随机磁盘I/O的减少。 在append-only的存储上工作得很好，因为DBMS不能回溯并更新数据。 The log will grow forever. The DBMS needs to periodically compact(紧凑) pages to reduce wasted space.\n由于不再需要时间信息，数据库可以将日志压缩到一个按id排序的表中。这些被称为分类字符串表（SSTables），它们可以使tuple搜索非常快。 紧凑化的问题是，DBMS最终会出现写入放大的情况。(它一次又一次地重写相同的数据）。 Compaction coalesces larger log files into smaller files by removing unnecessary records.\nLog-structured storage managers are more common today. This is partly due to the proliferation(扩散) of RocksDB.\n3.11 Data Representation A tuple is essentially a sequence of bytes.\nIt\u0026rsquo;s the job of the DBMS to interpret those bytes into attribute types and values.\nThe DBMS\u0026rsquo;s catalogs(目录) contain the schema information about tables(数据表示方案) that the system uses to figure out the tuple\u0026rsquo;s layout.\nINTEGER, BIGINT, SMALLINT, TINYINT. (Integers)大多数DBMS使用IEEE-754标准规定的 \u0026ldquo;native \u0026ldquo;C/C++类型来存储整数。这些值是固定长度的。 FLOAT, REAL (Variable Precision Numbers) 这些是不精确的、可变精度的数字类型, \u0026ldquo;native \u0026ldquo;C/C++类型,这些值也是固定长度的。 变精度数的运算比任意精度数的运算更快，因为CPU可以直接对其执行指令。然而，在进行计算时可能会出现精度损失！ NUMERIC, DECIMAL.(Fixed-Point Precision Numbers) 通常以精确的、可变长度的二进制表示法（像一个字符串）来存储.带有额外的元数据，这些数据将告诉系统诸如数据的长度和小数点应该在哪里。 当误差不可接受的时候，DBMS就要付出性能的代价来提高精度。 VARCHAR, VARBINARY, TEXT, BLOB Variable-Length Data\n代表任意长度的数据类型。它们通常是用一个header来存储的，这个header可以追踪到字符串的长度，以便于跳转到下一个值。它还可能包含一个数据的校验和。 大多数DBMS不允许一个tuple超过一个页面的大小。 但是！那些允许的系统将数据存储在一个特殊的 \u0026ldquo;溢出 \u0026ldquo;页上，并让tuple包含一个对该页的引用。这些溢出页可以包含指向其他溢出页的指针，直到所有的数据都可以被存储。(类似分级页表) 有些系统会让你把这些大的数值存储在一个外部文件中，然后元组会包含一个指向该文件的指针。例如，如果数据库存储的是照片信息，DBMS可以将照片存储在外部文件中，而不是让它们占用DBMS中的大量空间。这样做的一个缺点是，DBMS不能操作这个文件的内容。因此，No durability protections. No transaction protections.没有耐久性或交易保护。 TIME, DATE, TIMESTAMP. Dates and Times 不同的系统对日期/时间的表示方法不同。通常情况下，它们被表示为一些单位时间 (自unix时代的（微/毫）秒）。\nSystem Catalogs\n为了使DBMS能够识别tuple的内容，它维护了 INFORMATION_SCHEMA内部目录来告诉它关于数据库的元数据。元数据将包含关于数据库有哪些表和列的信息，以及它们的类型和值的顺序。\n大多数DBMS将其目录以其表的格式存储在自己的内部。他们使用 特殊代码来 \u0026ldquo;bootstrap \u0026ldquo;这些目录表。\nL5 Storage Models \u0026amp; Compression 5.1 Database Workloads OLTP: Online Transaction Processing\nAn OLTP workload is characterized by fast, short running operations, simple queries that operate on single entity at a time, and repetitive operations.\nAn OLTP workload will typically handle more writes than reads.\nAn example of an OLTP workload is the Amazon storefront. Users can add things to their cart, they can make purchases, but the actions only affect their account.\nOLAP: Online Analytical Processing\nAn OLAP workload is characterized by long running(长期运行), complex queries, reads on large portions of the database.\nIn OLAP workloads, the database system is analyzing and deriving(推导/添加) new data from existing data collected on the OLTP side.\nAn example of an OLAP workload would be Amazon computing the most bought item in Pittsburgh on a day when its raining.\nHTAP: Hybrid Transaction + Analytical Processing\nA new type of workload which has become popular recently is HTAP, which is like a combination which tries to do OLTP and OLAP together on the same database.\n5.2 Storage Models N-Ary Storage Model (NSM) In the n-ary storage model, the DBMS stores all of the attributes for a single tuple contiguously in a single page\nThis approach is ideal for OLTP workloads where requests are insert-heavy and transactions tend to operate only an individual entity.\nIt is ideal because it takes only one fetch to be able to get all of the attributes for a single tuple.\nAdvantages:\nFast inserts, updates, and deletes. Good for queries that need the entire tuple. Disadvantages:\nNot good for scanning large portions of the table and/or a subset of the attributes. Decomposition 分解 Storage Model (DSM) This model is ideal for OLAP workloads with many read-only queries that perform large scans over a subset of the table’s attributes.\nAdvantages:\nReduces the amount of I/O wasted because the DBMS only reads the data that it needs for that query. Better query processing and data compression Disadvantages:\nSlow for point queries, inserts, updates, and deletes because of tuple splitting/stitching(缝合). To put the tuples back together when using a column store,there are two common approaches:\nfixed-length offsets(most common): Each value is the same length for an attribute\nEmbedded Tuple Ids:Each value is stored with its tuple id in a column\n5.3 Database Compression 压缩操作被在disk-based DBMSs广泛应用。因为disk的I/O总是瓶颈，所以压缩可以让系统提升性能，尤其是只读analyt Managical workloads上。\n如果事先对tuples进行了压缩，DBMS可以获取更多有用的tuple，但代价是要付出更大的压缩和解压的计算开销。\n内存中的DBMS更加复杂，因为它们不必从磁盘中获取数据来执行一个查询。 内存比磁盘快得多，但压缩数据库可以减少DRAM需求和处理。\n而且必须在速度和压缩率中取得一个平衡。压缩数据库可以减少DRAM的需求和查询执行过程中的CPU成本。\n如果数据集是完全随机的bits，那么我们没有办法进行压缩。然而，现实世界中的数据集有一些key properties 是可以进行压缩的。\n数据集往往具有高度倾斜的属性值分布（例如，Brown语料库的Zipfian分布）。 数据集往往在同一元组的属性之间有很高的相关性（例如，邮政编码到城市。订单日期与发货日期）。 Manag Given this, we want a database compression scheme to have the following properties:\nMust produce fixed-length values. The only exception is var-length data stored in separate pools. This because the DBMS should follow word-alignment(对齐) and be able to access data using offsets. Allow the DBMS to postpone(推迟) decompression as long as possible during query execution (late materialization). Must be a lossless scheme because people do not like losing data. Any kind of lossy compression has to be performed at the application level. Compression Granularity 在给DBMS增加压缩功能之前，我们需要决定我们要压缩什么样的数据。这个决定决定了压缩方案的可用性。有四个级别的压缩 Manag颗粒度（granularity）\nBlock Level: 压缩同一张表的tuple块。\nTuple Level: 压缩整个tuples的内容（仅NSM）。\nAttribute Level: 在一个tuple内压缩单个属性值。可以针对同一tuple的多个属性。\nColumnar Level:\n为多个tuple存储的一个或多个属性压缩多个值 (只限于DSM)。这允许更复杂的压缩方案。\n5.4 Naive Compression DBMS使用一个通用的算法对数据进行压缩 (e.g., gzip, LZO, LZ4, Snappy, Brotli, Oracle OZIP, Zstd)。 尽管DBMS可以使用几种压缩算法，但工程师们往往选择那些经常提供较低压缩率以换取更快的压缩/解压的算法。\nnaive compression例子： MySQL InnoDB\nDBMS对磁盘页面进行压缩，将其压缩到2KB的幂数，并将其存储到缓冲池中。然而，每次DBMS试图读取数据时，缓冲池中的压缩数据必须被解压\n缺点:\n由于访问数据需要对压缩的数据进行解压，这就限制了压缩方案的范围。 如果目标是将整个表压缩成一个巨大的块，使用naive compression 方案是不可能的，因为每次访问都需要对整个表进行压缩/解压缩。 因此，对于MySQL来说，由于压缩范围有限，它将表分解成更小的块状。 naive方案也没有考虑到数据的高级含义或语义。 该算法既不考虑数据的结构，也不考虑查询打算如何访问 数据。因此，这就失去了利用late materialization 的机会，因为DBMS不能知道它何时能够延迟数据的解压。 5.5 Columnar Compression 柱状压缩** Run-length Encoding Compress runs of the same value in a single column into triplets:\nThe value of the attribute. The start position in the column segment. The # of elements in the run. Requires the columns to be sorted intelligently(智能) to maximize compression opportunities.\nBit-Packing Encoding When values for an attribute are always less than the value\u0026rsquo;s declared largest size, store them as smaller data type.\nBit-packing variant that uses a special marker to indicate when a value exceeds largest size and then maintain a look-up table to store them.\nBitmap Encoding Store a separate bitmap for each unique value for an attribute where an offset in the vector corresponds to a tuple.\nThe i th position in the Bitmap corresponds to the i th tuple in the table. Typically segmented into chunks to avoid allocating large blocks of contiguous memory. Only practical if the value cardinality(基数) is low. Some DBMSs provide bitmap indexes. Delta Encoding Recording the difference between values that follow each other in the same column.\nStore base value in-line or in a separate look-up table. Combine with RLE to get even better compression ratios. Incremental Encoding Type of delta encoding that avoids duplicating common prefixes/suffixes between consecutive tuples. This works best with sorted data.\nDictionary Encoding Build a data structure that maps variable-length values to a smaller integer identifier.\nReplace those values with their corresponding identifier in the dictionary data structure.\nNeed to support fast encoding and decoding. Encode/Locate: For a given uncompressed value, convert it into its compressed form. Decode/Extract: For a given compressed value, convert it back into its original form. No magic hash function will do this for us. Need to also support range queries. Most widely used compression scheme in DBMSs.\nHomework #2 Storage \u0026amp; Indexes Question 1: Storage Models a database with a single table R(q_id,txns,total,failed), where q_id is the primary key, and all attributes are the same fixed width.\nSuppose R has 20,000 tuples that fit into 100 pages, Ignore any additional storage overhead for the table (e.g., page headers, tuple headers).\nAssumptions:\nThe DBMS does not have any additional meta-data (e.g., sort order, zone maps). R does not have any indexes (including for primary key q_id) None of R’s pages are already in the buffer pool. SELECT total - failed FROM R WHERE q id = 96 AND txns \u0026gt; 420; (a) Suppose the DBMS uses the decomposition storage model (DSM) with implicit offsets i What is the minimum number of pages that the DBMS will potentially have to read from disk to answer this query?\n题意是存在这样一条记录,找到该条记录最少的磁盘读次数.\n官方解答是: 4 pages. 1 to find the primary key, + 3 to access txns, total, failed at their corresponding offsets.\n因为使用分解存储模式DSM, 按列存储.\n因此找到主键后就可以通过偏移量找其他属性的值. 读盘次数的限制在找主键上.\n显然,找主键至少需要一次读盘.\nii What is the maximum number of pages that the DBMS will potentially have to read from disk to answer this query?\n官方解答: 28 pages. There are 25 pages per attribute. In the worst case, we scan through all 25 pages to find the primary key, and then + 3 to access txns, total, failed at their corresponding offsets.\n上题可知,找主键次数决定了读盘次数.\n也就是当q_id = 96的记录越靠后, 找主键次数越多.\n因为没有顺序,所以96是无用的信息.\n同时因为有四个属性,且所有属性大小固定. 所以当每个属性都占25页时,且q_id=26所在的记录是最后一个(25),找主键次数最大为25次.\n(b)Suppose the DBMS uses the N-ary storage model (NSM) i. What is the minimum number of pages that the DBMS will potentially have to read from disk to answer this query?\nSolution: We find the tuple with the matching primary key on the first page. No need to look in other pages since all attributes are stored together.\n在第一页\nii. [4 points] What is the maximum number of pages that the DBMS will potentially have to read from disk to answer this query?\n在最后一页 100\nQuestion 2: Cuckoo Hashing 几道哈希表计算的题目,较简单\n2进制：0b，8进制：0o，10进制：无前缀，16进制：0x\nQuestion 3: Extendible Hashing Consider an extendible hashing structure such that:\nEach bucket can hold up to two records. The hashing function uses the lowest g bits, where g is the global depth. 题目是一个二进制可扩展哈希. 哈希函数是二进制最低g位.\n每个哈希值对应一个桶, 每个桶最多两个, 所有刚好就是二进制.\n(a) Starting from an empty table, insert keys 15, 14, 23, 11, 9. 二进制表示\n15: 00001111 14: 00001110 23: 00010111 11: 00001011 9 : 00001001 插入步骤(括号内为局部深度,全局深度为最大局部深度)\n插入15 全局: 1; 桶 1(1):15 14 全局: 1; 桶 1(1):15 ; 0(1):14 23 全局: 1; 桶 1(1):15,23; 0(1):14 11 全局: 1; 桶 1(1):15,23,11; 0(1):14 桶1此时有三条记录,需要扩展深度,扩展为2 全局: 2; 桶 11(2):15,23,11; 01(2):; 0(1):14 桶11此时有三条记录,需要扩展深度,扩展为3 全局: 3; 桶 111(3):15,23; 011(3):11;01(2):; 0(1):14 9 全局: 3; 桶 111(3):15,23; 011(3):11; 0(1):14; 01(2):9; i. What is the global depth of the resulting table?\n显然,当g至少等于3的时候才能使得同一个哈希值最多对应两个记录.\nii. [4 points] What is the local depth the bucket containing 15?\n为3\niii. [4 points] What is the local depth of the bucket containing 14?\n为1\n(b) Starting from the result in (a), you insert keys 12, 5, 7, 13, 2. 15: 00001111 14: 00001110 23: 00010111 11: 00001011 9 : 00001001 此时 全局: 3; 桶 111(3):15,23; 011(3):11; 0(1):14; 01(2):9; 12: 00001010 5 : 00000101 7 : 00000111 13: 00001101 2 : 00000010 12 全局: 3; 桶 111(3):15,23; 011(3):11; 0(1):14,12; 01(2):9; 5 全局: 3; 桶 111(3):15,23; 011(3):11; 0(1):14,12; 01(2):9; 7 全局: 3; 桶 111(3):15,23; 011(3):11; 0(1):14,12; 1(1):9,5; 此时111桶有3个,增加深度为4 全局: 4; 桶 1111(4):15;0111(4):23,7; 011(3):11; 0(1):14,12; 1(1):9,5; 13 全局: 4; 桶 1111(4):15;0111(4):23,7; 011(3):11; 0(1):14,12; 1(1):9,5; 01(2):13; 2 全局: 4; 桶 1111(4):15;0111(4):23,7; 011(3):11; 0(1):14,12; 1(1):9,5; 01(2):13;01(2):2 i. [4 points] Which key will first cause a split (without doubling the size of the table)?\n这里的split操作指的是增加桶的数量,而不double.也就是不增加全局深度.(增加全局深度会翻一倍)\n所以是13第一个增加了桶的数量.\nii. [4 points] Which key will first make the table double in size?\n7\n(c) Now consider the table below, along with the following deletion rules: If two buckets satisfy the following: (a) They have the same local depth d\n(b) They share the first d − 1 bits of their indexes (e.g. b010 and b110 share the first 2 bits)\n(c) Their constituent elements fit in a single bucket. Then they can be merged into a single bucket with local depth d − 1.\nIf the global depth g becomes strictly greater than all local depths, then the table can be halved in size. The resulting global depth is g − 1. 00 28:11100 8 :01000 01 25:11001 10 30:11110 22:10110 18:10010 011 11:01011 27:11011 111 23:10111 7 :00111 Starting from the table above, delete keys 25, 18, 22, 27, 7.\ndelete 25 00 28:11100 8 :01000 10 30:11110 22:10110 18:10010 011 11:01011 27:11011 111 23:10111 7 :00111 delete 18 00 28:11100 8 :01000 10 30:11110 22:10110 011 11:01011 27:11011 111 23:10111 7 :00111 delete 22 00 28:11100 8 :01000 10 30:11110 011 11:01011 27:11011 111 23:10111 7 :00111 此时00 和 10可进行合并 0 28:11100 8 :01000 30:11110 011 11:01011 27:11011 111 23:10111 7 :00111 delete 27 0 28:11100 8 :01000 30:11110 011 11:01011 111 23:10111 7 :00111 此时 011 和 111可合并为11,一次合并最多减少1, 此时global depth = 2 0 28:11100 8 :01000 30:11110 11 11:01011 23:10111 7 :00111 delete 7 0 28:11100 8 :01000 30:11110 11 11:01011 23:10111 i. Which deletion first causes a reduction in a local depth. 22\nii. Which deletion first causes a reduction in global depth.\n27\nSolution: Deleting 27 from bucket b011 allows it to merge with b111. Since these two buckets are the only ones of depth d = 3, this merge reduces the global depth to d = 2\nQuestion 4: B+Tree 还行,就是有个奇怪的点. 索引节点的值一般会出现在叶子节点上.\n最后一题是找出指定的节点 非法的地方.\nP1 1: 0001 2: 0010 3: 0011 4: 0100 5: 0101 6: 0110 7: 0111 8: 1000 9: 1001 1 2 3 4 全局1 1(1):1,3;0(1):2,4; 5 全局1 1(1):1,3,5;0(1):2,4; 全局2 01(2):1,5;11(2)3;0(1):2,4; 6 全局2 01(2):1,5;11(2)3;00(2):4;10(2):2,6; 7 全局2 01(2):1,5;11(2)3,7;00(2):4;10(2):2,6; 8 全局2 01(2):1,5;11(2)3,7;00(2):4,8;10(2):2,6; 9 全局2 01(2):1,5,9;11(2)3,7;00(2):4,8;10(2):2,6; 全局3 001(3):1,9;101(3):5;11(2)3,7;00(2):4,8;10(2):2,6; LRU-K The LRU-K algorithm evicts(剔除) a frame whose backward k-distance is maximum of all frames in the replacer.\nBackward k-distance is computed as the difference in time between current timestamp and the timestamp of kth previous access.\nA frame with less than k historical accesses is given +inf as its backward k-distance. When multipe frames have +inf backward k-distance, the replacer evicts the frame with the earliest timestamp.\n先弄清楚逻辑在写，不要把时间花在无意义的bug上。\nP2 B+Tree 概述 内部节点：索引作用\n叶子节点：包含真正的数据实体\n由于树是动态增长和收缩，所以要处理 split 和 merge\nSince the tree structure grows and shrink dynamically, you are required to handle the logic of split and merge.\nCheckpoint #1 — Due Date: Oct 11 @ 11:59pm\nTask #1 - B+Tree Pages Task #2 - B+Tree Data Structure (Insertion, Deletion, Point Search) Checkpoint #2 — Due Date: Oct 26 @ 11:59pm\nTask #3 - Index Iterator Task #4 - Concurrent Index 该project 要依赖于上一个 buffer pool 的正确实现。\n由于第一个检查点与第二个检查点密切相关，在第二个检查点中，您将在现有的B+索引中实现索引抓取，因此我们传入了一个名为transaction的指针参数，其默认值为nullptr。在任务#4之前，您不需要更改或调用与参数相关的任何函数。\nTask #1 - B+Tree Pages B+Tree Parent Page B+Tree Internal Page B+Tree Leaf Page B+Tree Parent Page ParenPage 被 Internal Page 和 Leaf Page所继承。\n并且只包含了 子类 所共享的信息。\n可以规定 parent_page_id_ 为 INVALID_PAGE_ID 表示根节点。\n| Variable Name | Size | Description | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | | page_type_ | 4 | Page Type (internal or leaf) | | lsn_ | 4 | Log sequence number (Used in Project 4) | | size_ | 4 | Number of Key \u0026amp; Value pairs in page | | max_size_ | 4 | Max number of Key \u0026amp; Value pairs in page | | parent_page_id_ | 4 | Parent Page Id | | page_id_ | 4 | Self Page Id |\nB+Tree Internal Page 不存储真实数据。只存储 m个有序 key和m+1个child 指针。 因为key的数量和指针数量不相等。第一个key被设定为invalid。所有方法需要从第二个key开始（下标为1） 实际存储如下 键,指针,键,指针,\u0026hellip;,键,指针. 此时有m+1个键,为了保证只有m个键,所以第一个键设置为无效的. key的数量限制\n在任何时间，最少装了一半（half-full）。 删除时，两个 half-full可以被joined 成为一个合法的 Internal Page 或者被重新分配来避免merge 当插入到一个fullpage时，可以被split成两个。 This is an example of one of the many design choices that you will make in the implementation of the B+ Tree. 三个泛型 KeyType, ValueType, KeyComparator。\nKeyType 不一定直接可用大于小于号比较，所以引入了 KeyComparator，从 cpp 文件中的实例化可以看出用的是 GenericKey 和 GenericComparator，查看二者源码可以得到以下信息：\nGenericKey 可以调用 ToString() 函数得到其 int64 表示，然后用 **%ld 格式符打印。**这对我们后面调试时非常重要。 GenericComparator 的比较规则是：左边小于右边时，返回 -1；左边大于右边时，返回 1；相等返回 0。\nValueType 代表的是指向子页面的指针，从实例化可以看出实际只用了 page_id_t，也就是 int。\n数据存储上，其理论结构应为 \u0026lt;指针，键，指针，键…，键，指针\u0026gt;，为方便存储，实际上在头部多补了一个无效键，从而可以用一个 pair 的数组存储：\n#define MappingType std::pair\u0026lt;KeyType, ValueType\u0026gt; ... class BPlusTreeInternalPage : public BPlusTreePage { ... private: // Flexible array member for page data. MappingType array_[1]; } array_[1] 等价与 pair类型的指针.指向pair数组. 数组的每一个元素是\u0026lt;键,值\u0026gt;\n在内部节点中,值代表指针.(第一个元素的键是无效的,但值是存在的).\n因为节点对象使用的是预先分配好的固定空间，array_ 可以控制从该位置开始到 Page 的 data 结束为止的这一段空间。\n因此，节点对象的生命周期也不是由 new 和 delete，而是由 BufferPoolManager 管理.\n取一个页面，用 FetchPage； 使用结束归还一个页面，用 UnpinPage。 page_id_ 不仅是 B+ 树中节点的编号，同时也是这个节点使用的 Page 在 BufferPool 中的编号。 B+Tree Leaf Page 存储 m个有序key，m个 value 实体 在这次实现中，value 仅仅是64位的record_id, 被用于定位真实的tuples的存储位置. see RID class defined under in src/include/common/rid.h. 对于键值对的数量限制与Internal Page一致. 重要\n尽管叶子节点和内部节点有相同类型的key.但他们值的类型不一样. 所以max_size 也应该是不同的 每个节点的 data_,都是从buffer pool 中fetch得到的 内存页面. 所以当写或读时,要先使用唯一的page_id,从buffer pool 中fetch 出来. 然后使用 reinterpret cast, 转换成 叶子节点或内部节点. 并且要在读或写操作结束后,unpin这个页面. Task #2 - B+Tree Data Structure 只支持 unique keys. 也就是说,当有重复的key插入时, 不进行任何操作并返回false 同时当删除操作导致 页面key数量低于阈值时, 需要正确地执行 merge 和 redistribute(也叫做 coalescing) 操作. 对于checkpoint , B+Tree Index 只需要支持 insertions (Insert()), point search (GetValue()), and deletes (Delete())\n插入操作 引起 split操作的情况:\n插入操作后, 叶子节点中 pair的数量 等于 max_size 插入 操作前, 内部节点 孩子数量等于 max_size 因为写操作会导致, root_page_id的改变,所以也要在 header page 同时更新 root_page_id (src/include/storage/page/header_page.h).\n更新的方法是调用 UpdateRootPageId (已经提供了)\n需要隐藏类型和比较.\ntemplate \u0026lt;typename KeyType, typename ValueType, typename KeyComparator\u0026gt; class BPlusTree{ // --- }; KeyType: 索引中key的类型,只可能是 GenericKey. GeneriKey真实的size是特定的. 是通过模板参数指定和实例化的，并取决于索引属性的数据类型. ValueType: 索引中值的类型. 只可能是 64-bit RID. KeyComparator: 用来比较两个KeyType实例 大小关系. 左边小于右边时，返回 -1；左边大于右边时，返回 1；相等返回 0。 Checkpoint 1：查找，插入和删除 约定 这里约定 内部节点array中key 与子节点的关系.\n左子节点的keys \u0026lt;= 父节点的keys 右子节点的keys \u0026gt; 父节点的keys 查找 给定key，返回查找的页面。\n页面的数据存在叶子节点中。\n因此要从根节点开始，一层一层往下找\nfindValue(const Page page,KeyType key){ //二分查找 page的array中,找到最大的小于等于key的 pair return pair; } GetLeaf(Page root,KeyType key){ Page now = root; while(now.type != leaf){ now = findValue(now,key); } return now; } GetValue(KeyType key){ Page now = GetLeaf(root,key); //现在now为叶子节点,在叶子节点中找对应的值. findValue(now,key); //如果返回值的key不相等则没找到. //相等则返回找到 } 插入(叶子节点) 因为以下的操作都基于至少有一个根节点的情况.\n因此在最开始要判断是否为空树,如为空树则创建根, 直接返回\n首先看键存不存在,如果已经存在,则直接返回.\n因为B+树的真实数据存在叶子节点中. 因此插入的第一步就是在找到对应叶子节点,并**插入(叶子节点)**到对应位置.\n此后可能会引起分裂.\n先直接**插入(叶子节点)**到叶子节点中. 当发现**插入(叶子节点)**后, 叶子节点的pair数量等于max_size时进行分裂. split(page) 操作这里不分内部叶子节点\n当前节点是full page,因此可以将当前节点分裂成 (左子节点 half full, 新父节点 , half full) 其中新父节点的值是左子节点的最后一个. 新父节点 需要 插入(内部节点) 到原来的父节点中. 插入(内部节点)前, 要判断 原父节点的孩子数量是否等于max_size 如果等于, 则先将原父节点分裂 后. 再执行当前的**插入(内部节点)**操作. 特殊的. 如果page是根节点, 即没有原父亲节点. 则将 根节点 更新成新父节点后, 返回即可. 插入(内部节点) : 因为在插入前已经处理好了插入操作的合法性,因此直接插入即可.\n伪代码:\nSetRoot(page_id){ root_page_id = page_id; //一定要调用 UpdateRootPageId } split(page){ //分裂出下面三个节点 left_page, right_page; new_parent_page;//key为左节点的最后一个key, value为指向左节点的指针 //判断是否为根节点 if(page.type == rootType){ SetRoot(new_parent_page.page_id); return ; } //获取父亲节点 parent_page; if(parent_page.isFull()){ split(parent_page); } //将new_parent_page 插入到 parent_page中 parent_page.insert(); } Insert(key,value){ //判断是否为空树 if(root == valid){ Page new_page = new Page(key,value); SetRoot(newpage.page_id); return ; } //先查找 Page leaf_page = GetLeaf(root,key); //直接调用叶子节点的插入 leaf_page.Insert(leaf_page, Page new page(key,value)); } LeafPage::Insert(Page new_page){ //直接插入,同时只需要更新指向 new_page 和 new_page的 next //满了就分裂. if(isFull()){ split(); ) } InternalPage::Insert(Page new_page){ //直接插入,但array中key的有效位置是从1开始 } //上面代码还需要考虑 //页面的创建和释放 删除 B+树最难的地方。但比起splay来说还是简单太多了。并且因为B+树只有叶子节点存值。在内部节点的删除上比B树简单了很多。\n删除的核心操作：\n叶子节点/内部节点删除元素后; 若小于下限,则按下面的优先级考虑; 如相邻兄弟结点丰满（即 元素个数 大于 下限+1），则向兄弟结点借一个元素。（具体是 先从相邻方向上的父亲结点变成借来的元素左右(这里具体要看借左边还是右边)，而后对应相邻的兄弟补回这个元素）。否则： 与相邻的兄弟结点 合并 父亲节点被删除后, 需要继续执行上面的操作.\n叶子节点和内部节点的差异:\n借 内部节点和B树操作一样, 先从相邻方向上的父亲结点拿一个元素，而后对应相邻的兄弟补回这个元素 叶子节点的父亲只是索引节点,因此 把兄弟节点的元素拿来后, 父亲节点的值根据 从左边/右边拿 变成相应的合法的值即可. 借的操作都不会涉及父亲节点的删除 合并 内部节点和B树操作一样, 将兄弟和父亲节点一起合并. (也相当于删除了父亲节点)下限+(下限-1) + 1 等于上限,因此合法 叶子节点是 两个叶子节点合并后直接删除父亲节点. 特殊地,如果删除的是根节点的元素，且根结点只有一个元素，则下降一层退出。\n因为只有一个待删除的元素，说明目前只有一个儿子结点（刚刚合并了）。 直接将儿子结点作为根节点。 伪代码\n//tree的Remove操作 Remove(){ //1.判断是否为空 if(empty){ //返回 } //2.找到叶子结点，并在叶子结点中删除该元素 leaf_page;//找到叶子节点 //TODO 删除元素 //3.判断叶子结点元素个数是是否小于下限 if( 小于 下限){ UnderFlow() } } Underflow(){ //3.0 判断是否为根节点 if(IsRoot){ //根节点的下限是1.说明应该减少一层. //3.0.1a找到根节点唯一的子节点. //该子节点变为根节点. //3.0.1b 根节点没有子节点 //直接删除,树变成空树. return } //3.1a. 找兄弟节点借 Borrow{ //3.1a.1a找左兄弟节点借 if(borrow(left_){ return } //3.1a.1b左兄弟不能借，就找右兄弟节点借 if(borrow(right_){ return } } //3.1b.借失败了，就准备合并 union{ //3.1b.1a 左兄弟结点存在,就找左兄弟节点合并 if(have(left) \u0026amp;\u0026amp; union(left_){ return } //3.1b.1b 没有左兄弟节点, 找右兄弟节点合并.(除根节点外肯定至少有一个兄弟节点) if(union(right_){ return } } } borrow(bro_page){ //如果兄弟节点的 size \u0026lt;= 下限,不能借 if(bro_page size \u0026lt;= 下限){ return ; } //否则可以借 //是叶子结点 //直接拿 对应兄弟结点最靠近的元素 //是内部结点 //加入的元素 = 中间父亲结点对应的元素 //中间父亲结点对应的元素 = 兄弟结点最靠近的元素. //兄弟结点最靠近的元素删除 } union(bro_page){ //是叶子结点 //两个结点合并成一个结点 //是内部结点 //好像都是直接把右合并到左就可以. //删除父亲结点中间的元素. //如果父亲结点size 小于等于 下限 //父亲结点采取下溢操作 } ","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15445/cmu15-445/","summary":"L1 Relational Model \u0026amp; Relational Algebra 1.1 Databases 数据库：an organized collection of inter-related data that models some aspect of the real-world 数据库管理系统 DBMS：the software that manages a database 1.2 Flat File Strawman 数据库常常以CSV(comma-sepa","title":"P0 [C++ Primer](https://15445.courses.cs.cmu.edu/fall2022/project0/)"},{"content":"Parallelism 加速比 Speed up 是指： 程序在单处理器上运行的时间 / 程序在多处理器上运行的时间\n我们一般会期望用两倍的硬件得到两倍的速度提升,但是实际上并不是这样的。\n制约性能提升可能的因素有:\n资源分配不均匀 通信开销 短板效应 共享资源读写冲突 为什么要去了解硬件？\n什么是限制性能的因素？ 导致性能瓶颈的原因是什么？ Efficiency fast != efficient\n什么是效率？ 尽可能地利用资源，减少浪费 比如按时间租用服务器。\n总结 并行程序的挑战：\n负载均衡 Load balance 通信延迟 Communication latency 集体工作时，真正用于计算的时间很少 ","permalink":"https://ysyyhhh.github.io/posts/public-course/cmu-15418cs-618/l1-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%92%8C%E9%AB%98%E6%95%88/","summary":"Parallelism 加速比 Speed up 是指： 程序在单处理器上运行的时间 / 程序在多处理器上运行的时间 我们一般会期望用两倍的硬件得到两倍的速度提升,但是实际上并不是这样的","title":"Why parallelism? Why efficiency?"},{"content":"docker 镜像上传到私有仓库 docker login -u username -p password registry.cn-hangzhou.aliyuncs.com docker tag image registry.cn-hangzhou.aliyuncs.com/username/image:tag docker push registry.cn-hangzhou.aliyuncs.com/username/image:tag Server SpringBoot application.yml 必须使用环境变量来进行配置。\nspring: datasource: url: jdbc:mysql://${MYSQL_HOST:localhost}:${MYSQL_PORT:3306}/${MYSQL_DATABASE:test}?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;serverTimezone=Asia/Shanghai username: ${MYSQL_USER:test} password: ${MYSQL_PASSWORD:123456} ","permalink":"https://ysyyhhh.github.io/posts/guide/%E9%83%A8%E7%BD%B2/%E9%83%A8%E7%BD%B2%E6%89%8B%E5%86%8C/","summary":"docker 镜像上传到私有仓库 docker login -u username -p password registry.cn-hangzhou.aliyuncs.com docker tag image registry.cn-hangzhou.aliyuncs.com/username/image:tag docker push registry.cn-hangzhou.aliyuncs.com/username/image:tag Server SpringBoot application.yml 必须使用环境变量来进行配置。 spring: datasource: url: jdbc:mysql://${MYSQL_HOST:localhost}:${MYSQL_PORT:3306}/${MYSQL_DATABASE:test}?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;serverTimezone=Asia/Shanghai username: ${MYSQL_USER:test} password: ${MYSQL_PASSWORD:123456}","title":"部署手册"},{"content":"语言模型的基础技术 transformer 大语言模型 GPT BERT BERT vs GPT\n","permalink":"https://ysyyhhh.github.io/posts/knowledge/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/","summary":"语言模型的基础技术 transformer 大语言模型 GPT BERT BERT vs GPT","title":"大语言模型原理"},{"content":"scp传输文件 # 本地到远程 scp -r /home/username/data username@ip:/home/username/data ","permalink":"https://ysyyhhh.github.io/posts/tips/server/","summary":"scp传输文件 # 本地到远程 scp -r /home/username/data username@ip:/home/username/data","title":"服务器"},{"content":"cmd 后缀 .bat 或 .cmd 的批处理文件\npowershell 后缀 .ps1 的脚本文件\nbash zsh fish ","permalink":"https://ysyyhhh.github.io/posts/tool/terminal/%E5%91%BD%E4%BB%A4%E8%A1%8C/","summary":"cmd 后缀 .bat 或 .cmd 的批处理文件 powershell 后缀 .ps1 的脚本文件 bash zsh fish","title":"命令行"},{"content":"系统资源相关 # 查看内存 free -h # 查看cpu cat /proc/cpuinfo # 查看cpu使用情况 top # 查看GPU使用情况 nvidia-smi # 查看磁盘 df -h # 查看系统版本 cat /etc/os-release # 查看系统信息 uname -a # 列出所有文件夹和文件 显示占用空间 du -sh * # 查看文件夹大小 du -sh folder_name # 查看文件大小 du -sh file_name 用户相关 # 创建用户 useradd -m -s /bin/bash -d /home/username username ## 解释: -m 创建用户目录, -s 指定shell, -d 指定用户目录 # 设置密码 passwd username # 删除用户 userdel -r username # 添加用户的sudo权限 ## 编辑sudoers文件 vi /etc/sudoers ## 在root ALL=(ALL) ALL下面添加 username ALL=(ALL) ALL # 查看用户组 groups username # 修改用户组 usermod -g groupname username # 查看所有用户 cat /etc/passwd 目录挂载 # 查看挂载 df -h # 挂载目录 mount /dev/sdb1 /home/username/data # 卸载目录 umount /home/username/data # 挂载硬盘 ## 查看硬盘 fdisk -l ## 格式化硬盘 fdisk /dev/sdb ## 格式化为ext4 mkfs.ext4 /dev/sdb1 挂载目录并立即生效\n# 挂载目录 mount /dev/sdb1 /home/username/data # 立即生效 mount -a 文件 # 带权限复制 cp -rp source dest # 远程连接复制文件 scp -r username@ip:/home/username/data /home/username/data 工具 压缩解压缩 # 压缩 tar -czvf filename.tar.gz foldername # 解压 tar -xzvf filename.tar.gz ### 定时脚本 ```shell # 查看定时脚本 crontab -l curl # 下载文件 curl -o filename url 系统路径/变量 持久化添加/改变系统路径/变量\n# 添加到系统路径 echo \u0026#39;export PATH=$PATH:/home/username/bin\u0026#39; \u0026gt;\u0026gt; /etc/profile # 立即生效 source /etc/profile tool ssh # 生成密钥 ssh-keygen -t rsa -C \u0026#34;{email}\u0026#34; # 查看密钥 cat ~/.ssh/id_rsa.pub apt # 设置tsinghua源 sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo sed -i \u0026#39;s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apt/sources.list # 更新源 s ","permalink":"https://ysyyhhh.github.io/posts/tips/linux/","summary":"系统资源相关 # 查看内存 free -h # 查看cpu cat /proc/cpuinfo # 查看cpu使用情况 top # 查看GPU使用情况 nvidia-smi # 查看磁盘 df -h # 查看系统版本 cat /etc/os-release # 查看系统信息 uname -a # 列","title":"Linux"},{"content":"1.命名规范 1.1.镜像命名规范 镜像命名规范：\u0026lt;小组名\u0026gt;/\u0026lt;项目名\u0026gt;/\u0026lt;镜像名\u0026gt;:\u0026lt;版本号\u0026gt;\n版本号：\u0026lt;主版本号\u0026gt;.\u0026lt;次版本号\u0026gt;.\u0026lt;修订号\u0026gt; eg: 1.0.0\n1.2.容器命名规范 容器命名规范：\u0026lt;小组名\u0026gt;-\u0026lt;项目名\u0026gt;-\u0026lt;容器名\u0026gt;-\u0026lt;版本号\u0026gt;\n","permalink":"https://ysyyhhh.github.io/posts/guide/%E9%83%A8%E7%BD%B2/docker%E7%9B%B8%E5%85%B3%E7%9A%84%E9%83%A8%E7%BD%B2%E8%A7%84%E8%8C%83/","summary":"1.命名规范 1.1.镜像命名规范 镜像命名规范：\u0026lt;小组名\u0026gt;/\u0026lt;项目名\u0026gt;/\u0026lt;镜像名\u0026gt;:\u0026lt;版本号\u0026gt;","title":"docker 相关的部署规范"},{"content":"wsl pass\n适用于Android的Windows子系统 ","permalink":"https://ysyyhhh.github.io/posts/tips/windows%E4%B8%8B%E7%9A%84%E5%AD%90%E7%B3%BB%E7%BB%9F/","summary":"wsl pass 适用于Android的Windows子系统","title":"windows 下的子系统"},{"content":"多阶段构建docker镜像 多阶段构建的修改不会保留到下一阶段，只有COPY和ADD命令会保留到下一阶段\nusages：\n第一阶段：编译/打包程序依赖 多阶段用途：\n缩小镜像体积 新系统build时出现Cannot autolaunch D-Bus without X11 $DISPLAY docker 拉取包时需要登录.\n问题出在Linux缺少一个密码管理包gnupg，它用于加密，我们在登录时需要这个包将密码加密后才能完成，因此直接安装\nsudo apt install gnupg2 pass ","permalink":"https://ysyyhhh.github.io/posts/tool/docker/docker/","summary":"多阶段构建docker镜像 多阶段构建的修改不会保留到下一阶段，只有COPY和ADD命令会保留到下一阶段 usages： 第一阶段：编译/打包程序","title":"docker Usage"},{"content":"docker中的npm # 设置npm源 npm config set registry https://registry.npm.taobao.org ","permalink":"https://ysyyhhh.github.io/posts/tool/npm/npm/","summary":"docker中的npm # 设置npm源 npm config set registry https://registry.npm.taobao.org","title":"npm"},{"content":"无法连接 Q:\nopenai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host=\u0026#39;api.openai.com\u0026#39;, port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError(\u0026#39;Unable to connect to proxy\u0026#39;, SSLError(SSLZeroReturnError(6, \u0026#39;TLS/SSL connection has been closed (EOF) (_ssl.c:1131)\u0026#39;)))) A:\n问题出在模块 urllib3 的版本，报错的是 1.26.3，没报错的是 1.25.11 在原报错环境中使用下面命令重装低版本 urllib3： pip install urllib3==1.25.11 然后测试果然就没问题了。 ","permalink":"https://ysyyhhh.github.io/posts/qa/openai/","summary":"无法连接 Q: openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host=\u0026#39;api.openai.com\u0026#39;, port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError(\u0026#39;Unable to connect to proxy\u0026#39;, SSLError(SSLZeroReturnError(6, \u0026#39;TLS/SSL connection has been closed (EOF) (_ssl.c:1131)\u0026#39;)))) A: 问题出在模块 urllib3 的版本，报错的是 1.26.3，没报错的是 1.25.11 在原报错环境中使","title":"openai 相关QA"},{"content":"锁 unique_lock 和 lock_guard unique_lock 和 lock_guard 都是 RAII 的封装，都是用来管理 mutex 的，但是 unique_lock 比 lock_guard 更加灵活，可以随时 unlock 和 lock，而 lock_guard 只能在构造的时候 lock，在析构的时候 unlock。\nunique_lock:\n```cpp std::mutex mtx; std::unique_lock\u0026lt;std::mutex\u0026gt; lck(mtx); lck.unlock(); lck.lock(); ``` lock_guard:\n```cpp std::mutex mtx; std::lock_guard\u0026lt;std::mutex\u0026gt; lck(mtx); ``` 和 condition_variable 使用时的区别 unique_lock 和 lock_guard 都可以和 condition_variable 一起使用，但是 unique_lock 更加灵活，可以随时 unlock 和 lock，而 lock_guard 只能在构造的时候 lock，在析构的时候 unlock。\nunique_lock:\n```cpp std::mutex mtx; std::condition_variable cv; std::unique_lock\u0026lt;std::mutex\u0026gt; lck(mtx); cv.wait(lck); /* 这部分仍然被锁住 */ lck.unlock(); lck.lock(); ``` lock_guard:\n```cpp std::mutex mtx; std::condition_variable cv; std::lock_guard\u0026lt;std::mutex\u0026gt; lck(mtx); cv.wait(lck); /* 这部分已经被解锁 */ ``` ","permalink":"https://ysyyhhh.github.io/posts/language/c++/%E5%A4%9A%E7%BA%BF%E7%A8%8B/","summary":"锁 unique_lock 和 lock_guard unique_lock 和 lock_guard 都是 RAII 的封装，都是用来管理 mutex 的，但是 unique_lock 比 lock_guard 更加灵活，可以随时 unlock 和 lock，而 lock_guard 只能在构造的时候 lock，在析构的时候 unloc","title":"多线程"},{"content":"markdown 快捷键 删除线: alt + s\n待办事项勾选/取消勾选: alt + c\nterminal 命令行创建: Ctrl + Shift + ~\n命令行切换: Ctrl + fn + upArrow/downArrow\nview Run Run python file in terminal: Ctrl + F5\n","permalink":"https://ysyyhhh.github.io/posts/tool/ide/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%85%A8/","summary":"markdown 快捷键 删除线: alt + s 待办事项勾选/取消勾选: alt + c terminal 命令行创建: Ctrl + Shift + ~ 命令行切换: Ctrl + fn + upArrow/downArrow view Run Run python file in terminal: Ctrl + F5","title":"工作学习流(vscode快捷键)"},{"content":"打包 mvn clean package -Dmaven.test.skip=true 常见问题 找不到主类 Error: Could not find or load main class com.xxx.xxx.xxx Caused by: java.lang.ClassNotFoundException: com.xxx.xxx.xxx 解决方法：在pom.xml中添加如下配置\n","permalink":"https://ysyyhhh.github.io/posts/tool/maven/maven/","summary":"打包 mvn clean package -Dmaven.test.skip=true 常见问题 找不到主类 Error: Could not find or load main class com.xxx.xxx.xxx Caused by: java.lang.ClassNotFoundException: com.xxx.xxx.xxx 解决方法：在pom.xml中添加如下配置","title":"maven"},{"content":"一、什么是I/O模型 及 I/O模型的分类 二、I/O 多路复用 三、实际应用 Reactor模式 Proactor模式 事件驱动模式 ","permalink":"https://ysyyhhh.github.io/posts/knowledge/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BC%96%E7%A8%8B/io%E6%A8%A1%E5%9E%8B/","summary":"一、什么是I/O模型 及 I/O模型的分类 二、I/O 多路复用 三、实际应用 Reactor模式 Proactor模式 事件驱动模式","title":"I/O模型"},{"content":"# 查看环境 conda env list # 创建环境 conda create -n py3 python=3.6 # 通过yml文件创建环境 conda env create -f environment.yml # 激活环境 conda activate py3 # 退出环境 conda deactivate # 删除环境 conda remove -n py3 --all 迁移时可能会出现pip问题 可以在yml的pip:上面加上pip\nname: py3 channels: - defaults dependencies: - python=3.6 - pip - pip: - -r requirements.txt ","permalink":"https://ysyyhhh.github.io/posts/tool/python/conda/","summary":"# 查看环境 conda env list # 创建环境 conda create -n py3 python=3.6 # 通过yml文件创建环境 conda env create -f environment.yml # 激活环境 conda activate py3 # 退出环境 conda deactivate # 删除环境 conda remove -n py3 --all 迁移时可能会出现pi","title":"conda"},{"content":"记把深度学习项目装入docker 安装时出现选项\n# RUN apt-get install libglib2.0-dev -y # 由于安装libglib2.0-dev的时候，bash会有交互操作叫你选择对应的时区，在docker build的时候没有交互的，所以需要加上DEBIAN_FRONTEND=\u0026#34;noninteractive\u0026#34; RUN DEBIAN_FRONTEND=\u0026#34;noninteractive\u0026#34; apt -y install libglib2.0-dev docker清理 在win10下，docker是基于wsl2的，所以docker的镜像和容器都是在wsl2的文件系统中。 所以在清理完docker的镜像和容器后，需要对wsl的盘进行压缩。\n# 停止所有的容器 docker stop $(docker ps -aq) # 删除所有未使用的容器 docker volume prune # 删除所有未使用的镜像 docker image prune -a # 删除缓存 docker builder prune # 查看当前占用的空间 docker system df 对wsl2的盘进行压缩\nwsl --shutdown # 查看wsl2的盘 wsl --list -v # 使用diskpart压缩 diskpart # open window Diskpart select vdisk file=\u0026#34;D:\\ubuntu\\wsl\\docker-desktop-data\\ext4.vhdx\u0026#34; attach vdisk readonly compact vdisk detach vdisk exit docker中安装conda # 安装conda RUN apt-get install -y wget # yhyu13 : donwload anaconda package \u0026amp; install RUN wget \u0026#34;https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh\u0026#34; RUN sh Anaconda3-2023.03-1-Linux-x86_64.sh -b -p /opt/conda # RUN rm /anaconda.sh RUN ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh RUN echo \u0026#34;. /opt/conda/etc/profile.d/conda.sh\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # yhyu13 : add conda to path ENV PATH /opt/conda/bin:/opt/conda/condabin:$PATH docker-compose 使用gpu version: \u0026#39;3.7\u0026#39; services: pytorch: build: . runtime: nvidia environment: - NVIDIA_VISIBLE_DEVICES=all - NVIDIA_DRIVER_CAPABILITIES=all volumes: - .:/workspace ports: - \u0026#34;8888:8888\u0026#34; - \u0026#34;6006:6006\u0026#34; command: bash -c \u0026#34;jupyter notebook --ip wsl 盘迁移到非系统盘 一般情况下 wsl盘的位置在 C:\\Users\\\u0026lt;用户名\u0026gt;\\AppData\\Local\\Docker\\wsl\ndocker的盘在 C:\\Users\\\u0026lt;用户名\u0026gt;\\AppData\\Local\\Docker\\wsl\\data\n# 1. 停止wsl wsl --shutdown # 2. 查看wsl状态 wsl --list -v # 可以看到docker有两个wsl，一个是docker-desktop-data，一个是docker-desktop # 只需要迁移docker-desktop-data即可,另一个很小 # 3. 迁移wsl wsl --export Ubuntu-20.04 D:\\ubuntu\\wsl\\Ubuntu-20.04.tar # 4. 删除wsl wsl --unregister Ubuntu-20.04 # 5. 查看是否删除成功 wsl --list -v # 6. 导入wsl wsl --import Ubuntu-20.04 D:\\ubuntu\\wsl\\Ubuntu-20.04 D:\\ubuntu\\wsl\\Ubuntu-20.04.tar --version 2 # 7. 查看是否导入成功 wsl --list -v docker 中设置特定版本的python # 创建一个基础镜像 FROM ubuntu:20.04 # 重置apt-get RUN rm -rf /etc/apt/sources.list # 安装conda # yhyu13 : install additional packages # 设置apt的源为tsinghua镜像源 RUN sed -i \u0026#39;s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apt/sources.list RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl wget # 安装conda RUN curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\ \u0026amp;\u0026amp; bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\ \u0026amp;\u0026amp; rm Miniconda3-latest-Linux-x86_64.sh # 创建conda环境并安装python RUN /opt/conda/bin/conda create -n py38 python=3.8.5 ENV PATH /opt/conda/envs/py38/bin:$PATH docker中使用display 在启动时需要设置环境变量DISPLAY\nwin下的情况 参考在Docker for Windows中运行GUI程序\n前后端项目静态资源转发 后端 springboot时： 把静态资源放在static目录下，然后在application中配置\nspring: mvc: static-path-pattern: /static/** resources: static-locations: classpath:/static/ 如果设置了拦截器，需要在拦截器上加入\n@Configuration public class WebMvcConfig implements WebMvcConfigurer { @Bean public CorsInterceptor corsInterceptor() { return new CorsInterceptor(); } @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(corsInterceptor()) .addPathPatterns(\u0026#34;/**\u0026#34;); } @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\u0026#34;/static/**\u0026#34;).addResourceLocations(\u0026#34;classpath:/static/\u0026#34;, \u0026#34;file:static/\u0026#34;); } } jar 包和静态路径关系\n- .jar - static 前端，需要在nginx上加入转发后访问静态路径后缀。\nlocation /api { rewrite ^/api(.*) $1 break; proxy_pass $SERVER_URL; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Photo $scheme; location ~*.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt|swf|xml|woff|woff2|ttf|eot|svg)$ { rewrite ^/api(.*) $1 break; proxy_pass $SERVER_URL; proxy_redirect off; } } ","permalink":"https://ysyyhhh.github.io/posts/tips/docker/","summary":"记把深度学习项目装入docker 安装时出现选项 # RUN apt-get install libglib2.0-dev -y # 由于安装libglib2.0-dev的时候，bash会有交互操作叫你选择对应的时","title":"docker相关技巧"},{"content":"poetry 出现的错误及解决方法 poetry install 时Failed to create the collection: Prompt dismissed 解决方案: 关闭keyring\npython3 -m keyring --disable 原因: https://github.com/python-poetry/poetry/issues/1917\n","permalink":"https://ysyyhhh.github.io/posts/tool/python/poetry/","summary":"poetry 出现的错误及解决方法 poetry install 时Failed to create the collection: Prompt dismissed 解决方案: 关闭keyring python3 -m keyring --disable 原因: https://github.com/python-poetry/poetry/issues/1917","title":"poetry"},{"content":"使用\n进入pods的容器\nkubectl exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- /bin/bash # 查看对应容器的日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 错误和解决方案 minikube 挂载 本地目录进minikube时,作为mysql的数据目录,但是mysql无法启动 挂载方式: 在minikube正常启动后, 使用\nminikube mount \u0026lt;本地目录\u0026gt;:\u0026lt;minikube目录\u0026gt; 进行挂载\n检查问题\n# 进入pod 的 db容器内查看日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 输出为\nfind: File system loop detected; \u0026#39;/var/lib/mysql/test\u0026#39; is part of the same file system loop as \u0026#39;/var/lib/mysql/\u0026#39;. 原因是挂载时发现循环\n解决方案:\n关闭并删除minikube minikube stop minikube delete 在minikube启动时就挂载 minikube start --mount --mount-string=\u0026#34;\u0026lt;本地目录\u0026gt;:\u0026lt;minikube目录\u0026gt;\u0026#34; 问题解决\nminikube 中 设置ingress未转发的问题 参考Could not access Kubernetes Ingress in Browser on Windows Home with Minikube?\n问题1： 当使用minikube时，设置ingress后，minikube ssh 内部可以通过ingress转发的服务端口访问。 但127.0.0.1 或 minikube ip 在主机上无法访问。\n解决方法：\nSet custom domain IP to 127.0.01 in %WINDIR%\\System32\\drivers\\etc\\hosts file, i.e. by adding line 127.0.0.1 my-k8s.com Get ingress pod name: kubectl get pods -n ingress-nginx Start port forwarding: kubectl -n ingress-nginx port-forward pod/ingress-nginx-controller-5d88495688-dxxgw --address 0.0.0.0 80:80 443:443, where you should replace ingress-nginx-controller-5d88495688-dxxgw with your ingress pod name. Enjoy using ingress on custom domain in any browser (but only when port forwarding is active) 问题2: ingress中使用prefix的转发规则时,无法获取路径中的query\n解决方法:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; # 需要添加这个 nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: defaultBackend: service: name: default-http-backend port: number: 80 rules: - host: fuzzs-scene-sim-test.localhost http: paths: - path: /FuzzsSceneSimTest(/|$)(.*) # 后缀加上(/|$)(.*) 用于获取query pathType: ImplementationSpecific backend: service: name: fuzzs-scene-sim-test port: number: 8089 ","permalink":"https://ysyyhhh.github.io/posts/tool/k8s/minikube/","summary":"使用 进入pods的容器 kubectl exec -it \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; -- /bin/bash # 查看对应容器的日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -c \u0026lt;container-name\u0026gt; 错误和解决方案 minikube 挂载 本地目录进minikube时,作为mysql的数据","title":"minikube"},{"content":"本规范用于在开发过程中，使得项目能够更好的部署，更好的维护。\n使用时间: 当项目开始开发时，就应该遵守本规范。\n核心要点:\n管理依赖库 使用docker 端口、ip地址等使用环境变量 路径不能写死！尤其是绝对路径和根目录等，需要放在环境变量中！！ 后端 python项目 python常见的依赖库管理有:\npoetry requirements.txt pipenv poetry 初始化\npoetry init 安装依赖\npoetry install 使用poetry运行项目\npoetry run python main.py 添加依赖\npoetry add \u0026lt;package\u0026gt; dockerfile示例\nFROM python:3.8.5-slim-buster WORKDIR /app # 拷贝依赖文件 COPY pyproject.toml poetry.lock ./ # 设置国内源 RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 安装poetry RUN pip install poetry # 安装依赖 RUN poetry config virtualenvs.create false \\ \u0026amp;\u0026amp; poetry install --no-dev --no-interaction --no-ansi # tips: 先只拷贝依赖文件，再安装依赖，可以利用docker的缓存机制，加快构建速度. # (防止只是项目文件改变，而依赖文件没有改变，导致重新安装依赖) # 拷贝项目文件 COPY . . # 运行项目 CMD [\u0026#34;poetry\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;main.py\u0026#34;] requirements.txt 导出依赖\npip freeze \u0026gt; requirements.txt 安装依赖\npip install -r requirements.txt dockerfile示例\nFROM python:3.8.5-slim-buster WORKDIR /app # 拷贝依赖文件 COPY requirements.txt ./ # 设置国内源 RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 安装依赖 RUN pip install -r requirements.txt # 拷贝项目文件 COPY . . # 运行项目 CMD [\u0026#34;python\u0026#34;, \u0026#34;main.py\u0026#34;] pipenv 初始化\npipenv --python 3.8 安装依赖\npipenv install 使用pipenv运行项目\npipenv run python main.py 添加依赖\npipenv install \u0026lt;package\u0026gt; dockerfile示例\nFROM python:3.8.5-slim-buster WORKDIR /app # 设置国内源 RUN pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 拷贝依赖文件 COPY Pipfile Pipfile.lock ./ # 安装依赖 RUN pip install pipenv \\ \u0026amp;\u0026amp; pipenv install --system --deploy --ignore-pipfile # 拷贝项目文件 COPY . . # 运行项目 CMD [\u0026#34;pipenv\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;main.py\u0026#34;] SpringBoot项目 参考 这里都以maven作为依赖管理工具。\n主要保留pom.xml文件\ndockerfile示例\n# 第一阶段: 构建jar包 FROM maven:3.6.3-jdk-8-slim AS build WORKDIR /app COPY pom.xml ./ # 设置国内源 RUN mvn -B -e -C -T 1C org.apache.maven.plugins:maven-dependency-plugin:3.1.2:go-offline # 拷贝项目文件 COPY . . # 构建jar包 RUN mvn clean install -DskipTests # 第二阶段: 运行jar包 FROM openjdk:8-jdk-alpine WORKDIR /app # 拷贝第一阶段构建的jar包 COPY --from=build /app/target/demo-0.0.1-SNAPSHOT.jar ./ # 运行项目 CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;target/demo-0.0.1-SNAPSHOT.jar\u0026#34;] 数据库 通常后端要连接数据库，这里只是简单的示例，实际项目中应该使用docker-compose来管理多个容器。\ndockerfile示例\nFROM mysql:8.0.22 # 设置时区 ENV TZ=Asia/Shanghai # 设置root密码 ENV MYSQL_ROOT_PASSWORD=123456 # 设置数据库名 ENV MYSQL_DATABASE=test # 设置用户名 ENV MYSQL_USER=test # 设置密码 ENV MYSQL_PASSWORD=123456 # 设置端口 EXPOSE 3306 单独运行mysql\ndocker run -d -p 3306:3306 --name mysql -v /path/to/mysql/data:/var/lib/mysql mysql:8.0.22 前端 前端使用npm作为依赖管理工具, 使用nginx作为web服务器。\n必要的文件:\npackage.json # 依赖文件 package-lock.json # 锁定依赖版本 nginx.conf # nginx配置文件 dockerfile npm npm初始化\nnpm init 安装依赖\nnpm install 添加依赖(默认添加到dependencies, 添加到devDependencies需要加上\u0026ndash;save-dev参数(或者-D)\nnpm install \u0026lt;package\u0026gt; nginx nginx.conf示例\nserver { listen 80; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; location / { try_files $uri $uri/ /index.html; } } docker docker 使用多阶段构建\ndockerfile示例\n# 第一阶段: 构建项目 FROM node:lts-alpine as build WORKDIR /app # 拷贝依赖文件 COPY package.json package-lock.json ./ # 安装依赖 RUN npm install # 拷贝项目文件 COPY . . # 构建项目 RUN npm run build # 第一段构建完成, 获得/app/build文件夹 # 使用nginx作为web服务器 FROM nginx:1.19.4-alpine # 拷贝nginx配置文件 COPY nginx.conf /etc/nginx/conf.d/default.conf # 拷贝第一阶段构建的项目文件 COPY --from=build /app/build /usr/share/nginx/html # 运行nginx CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 项目部署 TODO 封装整个项目(单个项目时) 经过上面的步骤已经将前后端 数据库封装到docker中了,但每次启动项目都需要手动启动三个容器, 这里使用docker-compose来管理多个容器。\ndocker-compose.yml示例\nversion: \u0026#39;3.8\u0026#39; services: mysql: image: mysql:8.0.22 environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: test MYSQL_USER: test MYSQL_PASSWORD: 123456 ports: - 3306:3306 volumes: - ./mysql/data:/var/lib/mysql backend: build: ./backend ports: - 8080:8080 depends_on: - mysql frontend: build: ./frontend ports: - 80:80 depends_on: - backend 整个项目作为k8s的一个服务(多个项目时) 上面是使用docker-compose来管理 一个项目的多个容器.\n但如果有多个项目, 每个项目都有多个容器, 这时候就需要使用k8s来管理了.\n我们把一个项目(多个容器)作为一个k8s的一个服务.\nk8s的配置文件示例\napiVersion: v1 kind: Service metadata: name: test labels: app: test spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: test type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: test labels: app: test spec: replicas: 1 selector: matchLabels: app: test template: metadata: labels: app: test spec: containers: - name: mysql image: mysql:8.0.22 env: - name: TZ value: Asia/Shanghai - name: MYSQL_ROOT_PASSWORD value: 123456 - name: MYSQL_DATABASE value: test - name: MYSQL_USER value: test - name: MYSQL_PASSWORD value: 123456 ports: - containerPort: 3306 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql - name: backend image: backend:latest ports: - containerPort: 8080 env: - name: MYSQL_HOST value: mysql - name: MYSQL_PORT value: \u0026#34;3306\u0026#34; - name: MYSQL_DATABASE value: test - name: MYSQL_USER value: test - name: MYSQL_PASSWORD value: 123456 - name: frontend image: frontend:latest ports: - containerPort: 80 volumes: - name: mysql-data hostPath: path: /path/to/mysql/data ","permalink":"https://ysyyhhh.github.io/posts/guide/%E9%83%A8%E7%BD%B2/%E5%88%A9%E4%BA%8E%E9%83%A8%E7%BD%B2%E7%9A%84%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/","summary":"本规范用于在开发过程中，使得项目能够更好的部署，更好的维护。 使用时间: 当项目开始开发时，就应该遵守本规范。 核心要点: 管理依赖库 使用docke","title":"利于部署的开发规范手册"},{"content":"grep工具\n","permalink":"https://ysyyhhh.github.io/posts/tool/terminal/grep/","summary":"grep工具","title":"grep"},{"content":"光标移动 h: 左移一个字符 j: 下移一行 k: 上移一行 l: 右移一个字符\nw: 移动到下一个单词的开头 e: 移动到当前单词末尾 b: 移动到上一个单词的开头\n0: 移动到行首 $: 移动到行尾\nn + 上面的命令: 移动n次\ngg: 移动到文件开头 G: 移动到文件末尾\n上面所有命令构成了一个移动命令，可以和d命令组合使用，删除从当前光标到移动命令所指的位置的内容\nctrl + f: 下翻一页 ctrl + b: 上翻一页 ctrl + G: 显示当前光标所在行的行号\nctrl + i: 跳转到上次位置· ctrl + o: 跳转到下次位置\nG + n: 移动到第n行\n插入 i: 在当前光标处插入 I: 在当前行首插入\na: 在当前光标后插入 A: 在当前行尾插入\no: 在当前行下方插入一行 O: 在当前行上方插入一行\n删除 x: 删除当前光标所在的字符 X: 删除当前光标所在的前一个字符\ndd: 删除当前行 D: 删除当前光标所在位置到行尾的内容\nd + 移动命令: 删除从当前光标到移动命令所指的位置的内容\n如: dw: 删除当前光标所在的单词 db: 删除当前光标所在的单词 d$: 删除当前光标所在位置到行尾的内容 dnG: 删除当前光标所在行到第n行的内容 dG: 删除当前光标所在行到文件末尾的内容\n剪切 上面删除的内容都会被保存到剪切板中\n删除并进入插入模式 s: 删除当前光标所在的字符并进入插入模式 S: 删除当前行并进入插入模式\nc + 移动命令: 删除从当前光标到移动命令所指的位置的内容并进入插入模式\n如: cw: 删除当前光标所在的单词并进入插入模式 c$: 删除当前光标所在位置到行尾的内容并进入插入模式 cnG: 删除当前光标所在行到第n行的内容并进入插入模式\n复制 y + 移动命令: 复制从当前光标到移动命令所指的位置的内容\n如: yw: 复制当前光标所在的单词 yb: 复制当前光标所在的单词 y$: 复制当前光标所在位置到行尾的内容 ynG: 复制当前光标所在行到第n行的内容\n粘贴 所有删除的内容都会被保存到剪切板中，可以使用p命令将剪切板中的内容粘贴到当前光标所在位置 p: 将剪切板中的内容粘贴到当前光标所在位置的后面 P: 将剪切板中的内容粘贴到当前光标所在位置的前面\n替换 r + 字符: 将当前光标所在的字符替换为指定的字符\nR + 字符串: 将当前光标所在位置开始的字符串替换为指定的字符串\n撤销 u: 撤销上一次操作 U: 撤销当前行的所有操作\nctrl + r: 恢复上一次撤销的操作\n重复 . : 重复上一次操作\n查找 / + 关键字: 从当前光标开始向下查找关键字 ? + 关键字: 从当前光标开始向上查找关键字\n输完后按回车，会跳转到第一个匹配的位置.\nn: 跳转到下一个匹配的位置 N: 跳转到上一个匹配的位置\n: 进阶命令 :w 保存文件 :q 退出 :q! 强制退出，不保存 :wq 保存并退出 :wq! 强制保存并退出\n上面的命令 + 文件名: 保存文件到指定的文件名\n:help 命令名: 查看命令的帮助文档\n替换 :%s/old/new/g 将所有的old替换为new :%s/old/new/gc 将所有的old替换为new，替换前询问是否替换\n:#,#s/old/new/g 将第#行到第#行的old替换为new\n外部命令 :! + 命令: 执行外部命令\n如: :!ls 执行ls命令 :!dir 执行dir命令\n","permalink":"https://ysyyhhh.github.io/posts/tool/vim/%E6%8C%87%E4%BB%A4%E6%89%8B%E5%86%8C/","summary":"光标移动 h: 左移一个字符 j: 下移一行 k: 上移一行 l: 右移一个字符 w: 移动到下一个单词的开头 e: 移动到当前单词末尾 b: 移动到上一个单词的开头 0: 移动到行首 $: 移","title":"vim的使用"},{"content":"TL;DR 背景: 已使用nextcloud和typora写笔记 需求: 将笔记转换为博客.(且因为本人太懒,😂 所以需要全自动化) 在nextcloud中, 专门设置一个文件夹\u0026quot;笔记\u0026quot; 转换为博客文件夹 不能改变原来记笔记的方式 不能有任何新增的操作 方案: 使用hugo搭建博客 使用Github pages部署博客 使用Github Actions自动化部署 使用py脚本将笔记转换为博客 使用任务计划程序定时执行py脚本 使用hugo搭建博客 参考: hugo官网 Hugo+Github Pages+Github Action博客方案之二 Hugo+Github Pages+Github Action博客方案之三 PaperMod主题\n创建github仓库 要创建两个仓库\n一个仓库用于存放博客源码 一个仓库用于存放博客静态文件 创建博客静态文件仓库 设置仓库名为: 用户名.github.io 我的博客仓库\n创建博客源码仓库 设置仓库名为: hugo-blog // 仓库名可以自定义 我的博客源码仓库\n安装hugo scoop install hugo 创建hugo博客 hugo new site hugo-blog 安装主题 cd hugo-blog ## 进入博客目录, 这个是博客源码仓库 git init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive ## needed when you reclone your repo (submodules may not get cloned automatically) 配置主题 这里使用yaml格式的配置文件, 也可以使用toml格式的配置文件 所以需要删除config.toml文件, 并创建config.yaml文件\nconfig.yaml:\nbaseURL: / title: ysyy\u0026#39;s blog theme: PaperMod languageCode: zh-cn 剩余配置参考\n创建文章 hugo new posts/first/hello-world.md 本地预览 hugo server -D 生成静态文件 生成静态文件, 生成的静态文件在 public文件夹中。 之后我们将这个文件夹中复制到博客静态文件仓库中\nhugo 部署到github pages 创建静态文件夹\ngit clone git@用户名.github.io.git cd 用户名.github.io cp -r hugo-blog/public/* ./ 提交到github\ngit add . git commit -m \u0026#34;first commit\u0026#34; git push origin main 配置github pages 在github中的 用户名.github.io仓库中, 点击 Settings选项卡, 找到 GitHub Pages选项, 将 Source选项设置为 main分支, 点击 Save按钮, 这样就可以通过 https://用户名.github.io访问博客了\n使用Github Actions自动化部署 参考\n如果每一次更新/发布新博客都需要手动执行上面的步骤, 那么就太麻烦了, 所以我们需要自动化部署\n在博客源码仓库的根目录下创建 .github/workflows/deploy.yml文件\nname: ysyyblog on: push: branches: - main jobs: build-deploy: runs-on: ubuntu-20.04 # runs-on: macos-latest steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: personal_token: ${{ secrets.PERSONAL_TOKEN }} # 另外还支持 deploy_token 和 github_token external_repository: ysyyhhh/ysyyhhh.github.io # 修改为你的 静态文件GitHub Pages 仓库 publish_dir: ./public # keep_files: false publish_branch: main # 如果使用自定义域名，还需要添加下面一行配置 # cname: www 创建personal_token 在github主页的右上角点击头像, 点击 Settings选项卡, 找到 Developer settings选项,\n找到 Personal access tokens选项, 点击 Generate new token按钮, 创建一个新的token\n配置personal_token 在hugo-blog仓库中, 点击 Settings选项卡, 找到 Secrets选项, 点击 New repository secret按钮,\n新增一个名为 PERSONAL_TOKEN的secret, 值为上面创建的personal_token\n测试自动化部署 在本地的hugo-blog仓库中, 修改 content/posts/first/hello-world.md文件, 然后提交到github\n可以在 Actions选项卡中查看自动化部署的状态\n如果在 Actions选项卡中看到了 build-deploy任务, 且状态为 success, 那么就说明自动化部署成功了\n可以在 用户名.github.io仓库中查看是否已经更新.\n使用任务计划程序和py脚本实现全自动化 上面的步骤已经让我们发布笔记的过程变成:\n使用hugo new / 直接编辑 content的文件 来创建笔记 提交到hugo-blog仓库 然后hugo-blog仓库就会自动部署到用户名.github.io仓库中\n虽然已经只剩两步了,但遵循能自动化就自动化的原则, 我们还是要把这两步也自动化\n使用py脚本将笔记转换为博客 安装python这些步骤就省去了,这里直接给出py脚本\n\u0026#39;\u0026#39;\u0026#39; 每天定时更新博客内容 1.进入项目根目录: D:/program_about/hugo/hugo-blog 2. 将D:/nextcloud/笔记/下的文件同步到 ./content/posts/下 3. 执行./push.bat 或 git add . \u0026amp;\u0026amp; git commit -m \u0026#34;update\u0026#34; \u0026amp;\u0026amp; git push \u0026#39;\u0026#39;\u0026#39; import os import shutil def create_index(root, name): \u0026#39;\u0026#39;\u0026#39; name = A.md 在root下生成\u0026#39;A\u0026#39;文件夹 将A.md移动到A文件夹下，并重命名为index.md 如果 存在 root + \u0026#39;/img\u0026#39; 的文件夹 将 root + \u0026#39;/img\u0026#39; 复制到 root + \u0026#39;/A/img\u0026#39; 下 \u0026#39;\u0026#39;\u0026#39; # 生成文件夹 dir_name = name.split(\u0026#39;.\u0026#39;)[0] print(root, name, dir_name) os.mkdir(os.path.join(root, dir_name)) # 移动文件 shutil.move(os.path.join(root, name), os.path.join(root, dir_name, \u0026#39;index.md\u0026#39;)) # 处理img if os.path.exists(os.path.join(root, \u0026#39;img\u0026#39;)): shutil.copytree(os.path.join(root, \u0026#39;img\u0026#39;), os.path.join(root, dir_name, \u0026#39;img\u0026#39;)) def adjust(dir): os.chdir(dir) \u0026#39;\u0026#39;\u0026#39; 将所有下面的格式 - A.md - img - A-1.png 转换成 - A - index.md - img - A-1.png 如果遇到\u0026#34;.md\u0026#34;文件,直接删除 \u0026#39;\u0026#39;\u0026#39; for(root, dirs, files) in os.walk(\u0026#34;.\u0026#34;): root = os.path.join(dir, root) for name in files: if name == \u0026#39;.md\u0026#39;: os.remove(os.path.join(root, name)) continue if name.endswith(\u0026#39;.md\u0026#39;): create_index(root, name) for name in dirs: # 递归调用 adjust(os.path.join(root, name)) def sync(): root_path = \u0026#39;D:/program_about/hugo/hugo-blog\u0026#39; os.chdir(root_path) # 当文件已存在时，无法创建该文件。: \u0026#39;./content/posts/\u0026#39; shutil.rmtree(\u0026#39;./content/posts/\u0026#39;) # git中也要删除 os.system(\u0026#39;git rm -r ./content/posts/\u0026#39;) shutil.copytree(\u0026#39;D:/nextcloud/笔记/\u0026#39;, \u0026#39;./content/posts/\u0026#39;) # 把所有文件夹和文件的名称大写转换为小写 os.chdir(\u0026#39;./content/posts/\u0026#39;) for root, dirs, files in os.walk(\u0026#34;.\u0026#34;): for name in files: new_name = name.lower() os.rename(os.path.join(root, name), os.path.join(root, new_name)) for name in dirs: new_name = name.lower() os.rename(os.path.join(root, name), os.path.join(root, new_name)) # 调整文件夹结构 adjust(root_path+\u0026#39;./content/posts/\u0026#39;) # 上传到git # os.chdir(\u0026#39;./content/posts/\u0026#39;) os.chdir(\u0026#39;D:/program_about/hugo/hugo-blog\u0026#39;) os.system(\u0026#39;git add ./content/posts/\u0026#39;) os.system(\u0026#39;git commit -m \u0026#34;update\u0026#34;\u0026#39;) os.system(\u0026#39;git push\u0026#39;) os.chdir(\u0026#39;D:/program_about/hugo/hugo-blog\u0026#39;) print(\u0026#39;sync done\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: sync() 将上面的路径修改为自己的路径, 然后保存为 sync.py文件 可以执行py脚本,测试一下\n关于图片路径问题 参考方案\n因为我平时的图片路径是\n- A.md - img - A-1.png 但是hugo会将A.md文件转换为A文件夹, 所以此时是无法访问A-1.png的.\n这里是通过改变相对路径关系来解决的, 即代码中的adjust()\n当然如果你有图床就不需要这么麻烦了\n使用任务计划程序定时执行py脚本 参考 这里我使用的是win10自带的任务计划程序, 其他系统的任务计划程序也是类似的\n以下步骤由Claude生成\n下面是如何使用Windows任务计划程序来配置定时每天执行Python脚本的步骤: 打开任务计划程序(Windows + R 输入taskschd.msc回车) 点击\u0026#34;操作\u0026#34;栏中的\u0026#34;创建基本任务\u0026#34; 输入任务名称,选择触发器为每天定时,设置执行时间 在操作栏中,点击“新建” 选择“启动一个程序” 在“程序/脚本”框中输入Python解释器的路径,例如C:\\Python37\\python.exe 在“添加参数(可选)”中输入python脚本文件的完整路径,例如C:\\Users\\username\\script.py 点击“确定”保存此操作 在下一页中选择用户账号,例如“当前用户” 点击“确定”完成创建任务 根据需要配置触发器记录和其他选项 点击“确定”保存任务 任务将在设定的时间自动执行python脚本文件 每次修改脚本后需要停止原有任务,然后再新建一个相同的任务来加载修改后的脚本代码。 需要注意python interpreter路径和脚本路径的正确性。定时执行格式也需要正确,这样就可以实现Windows系统中的自动定时任务执行Python脚本了。 ","permalink":"https://ysyyhhh.github.io/posts/guide/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","summary":"TL;DR 背景: 已使用nextcloud和typora写笔记 需求: 将笔记转换为博客.(且因为本人太懒,😂 所以需要全自动化) 在nextcloud中, 专","title":"用Hugo + Github Pages/Action + py + 任务计划程序 搭建 全自动化markdown笔记转博客"},{"content":"测试环境\n自动化部署 一键转移到生成环境 部署 项目代码 测试代码\n编码风格测试\u0026amp;修正 功能测试 Dockerfile\ndocker-compose\n测试脚本\n部署脚本\n前端代码结构 优点 接口可以根据环境自动替换:\n开发环境的接口 生产环境 1.docker-run.sh 描述: 部署脚本在git clone之后运行的脚本\n内容: 包括docker的构建和运行\n示例\ndocker-compose down docker rmi digitalmapadmin-frontend docker build -t digitalmapadmin-frontend . docker-compose pull docker-compose up -d 2.docker-compose.yaml 描述: docker-run.sh 会用到docker-compose指令\n内容:\nDockerfile 路径 端口 环境变量 生产环境下的后端接口 # 这样可以在部署时 自动替换成生产环境的后端接口. version: \u0026#39;3.0\u0026#39; services: frontend: build: context: . dockerfile: ./Dockerfile ports: - 8004:80 environment: - NODE_ENV=production - VITE_APP_TITLE=数据资源管理平台 - VITE_APP_BASE_API=/api - VITE_SERVE=http://121.40.252.139:8089/ 3.Dockerfile 描述: 根据项目构建docker 镜像\n内容:\n获取dist: 安装 npm: 并进行npm install npm run build 用nginx运行项目 配置nginx 启动项目 FROM node:lts-alpine WORKDIR /app # 先将package.json和package-lock.json拷贝到工作目录中 COPY package*.json ./ RUN npm install # 将当前目录下的所有文件拷贝到工作目录中 COPY . . RUN npm run build FROM nginx:alpine # 将打包后的dist目录下全部文件拷贝到nginx的html/目录下 # COPY ./dist/ /usr/share/nginx/html/ COPY --from=0 /app/dist /usr/share/nginx/html # 删除nginx中之前的配置 RUN rm /etc/nginx/conf.d/default.conf # 拷贝当前的文件到nginx中 COPY nginx.conf /etc/nginx/nginx.conf COPY default.conf.template /etc/nginx/conf.d # 启动nginx CMD /bin/sh -c \u0026#34;envsubst \u0026#39;80\u0026#39; \u0026lt; /etc/nginx/conf.d/default.conf.template \u0026gt; /etc/nginx/conf.d/default.conf\u0026#34; \u0026amp;\u0026amp; nginx -g \u0026#39;daemon off;\u0026#39; ","permalink":"https://ysyyhhh.github.io/posts/guide/%E5%BC%80%E5%8F%91/%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B/","summary":"测试环境 自动化部署 一键转移到生成环境 部署 项目代码 测试代码 编码风格测试\u0026amp;修正 功能测试 Dockerfile docker-compose 测试脚本 部署脚本 前端代码结构 优点 接口可以根据环","title":"开发流程"},{"content":"1.分层领域模型规约：\n• DO（ Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。 • DTO（ Data Transfer Object）：数据传输对象，Service或Manager向外传输的对象。 • BO（ Business Object）：业务对象。 由Service层输出的封装业务逻辑的对象。 • AO（ Application Object）：应用对象。 在Web层与Service层之间抽象的复用对象模型，极为贴近展示层，复用度不高。 • VO（ View Object）：显示层对象，通常是Web向模板渲染引擎层传输的对象。 • POJO（ Plain Ordinary Java Object）：在本手册中， POJO专指只有setter/getter/toString的简单类，包括DO/DTO/BO/VO等。 • Query：数据查询对象，各层接收上层的查询请求。 注意超过2个参数的查询封装，禁止使用Map类来传输。 2.领域模型命名规约：\n• 数据对象：xxxDO，xxx即为数据表名。 • 数据传输对象：xxxDTO，xxx为业务领域相关的名称。 • 展示对象：xxxVO，xxx一般为网页名称。 • POJO是DO/DTO/BO/VO的统称，禁止命名成xxxPOJO。 Restful 接口风格 ","permalink":"https://ysyyhhh.github.io/posts/language/spring/%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC/","summary":"1.分层领域模型规约： • DO（ Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。 • DTO（ Data Transfer Object）：数据传","title":"代码风格"},{"content":"https://blog.csdn.net/pyufftj/article/details/83102530\nhttps://blog.csdn.net/fragrant_no1/article/details/85986511\nnohup\nnohup python3 -u tcp_client.py \u0026gt; tcp.log 2\u0026gt;\u0026amp;1 \u0026amp; nuhup : 不挂起的意思 python3 tcp_client.py : 使用python3环境运行 tcp_client.py文件 -u : 代表程序不启用缓存，也就是把输出直接放到log中，没这个参数的话，log文件的生成会有延迟 \u0026gt; tcp.log : 把程序输出日志保存到tcp.log文件中 2\u0026gt;\u0026amp;1 : 换成2\u0026gt;\u0026amp;1，\u0026amp;与1结合就代表标准输出了，就变成错误重定向到标准输出 \u0026amp; : 最后一个\u0026amp; ，代表该命令在后台执行 nohup python3 -u main.py \u0026gt; chatbot.log 2\u0026gt;\u0026amp;1 \u0026amp; nohup ./go-cqhttp \u0026gt; go-cq.log 2\u0026gt;\u0026amp;1 \u0026amp; curl curl ","permalink":"https://ysyyhhh.github.io/posts/tool/terminal/%E5%B7%A5%E5%85%B7/","summary":"https://blog.csdn.net/pyufftj/article/details/83102530 https://blog.csdn.net/fragrant_no1/article/details/85986511 nohup nohup python3 -u tcp_client.py \u0026gt; tcp.log 2\u0026gt;\u0026amp;1 \u0026amp; nuhup : 不挂起的意思 python3 tcp_client.py : 使用python3环境运行 tcp_client.py文件 -u : 代表程序不启用缓存，也就是把输出直接","title":"工具"},{"content":"jar包启动\nnohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M jar包名.jar nacos启动\nsh /home/tmp/nacos/bin/startup.sh -m standalone 服务器启动\ncd /home/mind_wings nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M service-user-1.0-SNAPSHOT.jar \u0026amp; nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M service-timetable-1.0-SNAPSHOT.jar \u0026amp; nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M -noverify api-gateway-1.0-SNAPSHOT.jar \u0026amp; 出现过的问题\napi-gateway 启动失败 https://blog.csdn.net/crxk_/article/details/103196146 ","permalink":"https://ysyyhhh.github.io/posts/language/spring/%E6%9C%8D%E5%8A%A1%E7%AB%AF/","summary":"jar包启动 nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M jar包名.jar nacos启动 sh /home/tmp/nacos/bin/startup.sh -m standalone 服务器启动 cd /home/mind_wings nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M service-user-1.0-SNAPSHOT.jar \u0026amp; nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M service-timetable-1.0-SNAPSHOT.jar \u0026amp; nohup java -jar -Xms128M -Xmx128M -XX:PermSize=128M -XX:MaxPermSize=128M -noverify api-gateway-1.0-SNAPSHOT.jar \u0026amp; 出现","title":"服务端"},{"content":"session权限问题\nhttps://blog.51cto.com/u_15162069/2778036\nRSA前后端解密出错 14：07\nJSEncrypt支持的是openssl生成的pkcs1格式私钥，java需要pkcs8格式私钥，公钥格式不变\n前端加入替换 encodeURI(encodeData).replace(/\\+/g, \u0026lsquo;%2B\u0026rsquo;)\n后端接口加入 URLDecoder.decode(password,\u0026ldquo;UTF-8\u0026rdquo;);\nhttps://blog.csdn.net/qq_42979402/article/details/109184787\n真正错误是密码加了hash函数后，返回值是数字而不是字符串！！！\n数据库返回乱码 https://www.cnblogs.com/fanbi/p/13940432.html\n实际上是apigateway 放入 header 后再取出 乱码 存储过程返回多结果集并接收 test时报错 AOP之类的 禁用字节码校验\n","permalink":"https://ysyyhhh.github.io/posts/qa/%E6%9D%82/","summary":"session权限问题 https://blog.51cto.com/u_15162069/2778036 RSA前后端解密出错 14：07 JSEncrypt支持的是openssl生成的pkcs1格式私钥，java需要pkcs","title":"杂"},{"content":"https://www.jianshu.com/p/38d247f02724 安全 https://blog.csdn.net/qq_37023928/article/details/116777630 解决前后端分离的跨域问题 https://www.yisu.com/zixun/606000.html 创建Test类注解 @RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest 依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 开启services Views -\u0026gt; Tool Windows -\u0026gt; Services Spring容器中找不到ServletWebServerFactory类出现的异常 https://cloud.tencent.com/developer/article/1893349 ","permalink":"https://ysyyhhh.github.io/posts/language/spring/spring%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/","summary":"https://www.jianshu.com/p/38d247f02724 安全 https://blog.csdn.net/qq_37023928/article/details/116777630 解决前后端分离的跨域问题 https://www.yisu.com/zixun/606000.html 创建Test类注解 @RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest 依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 开启services Views -\u0026gt; Tool Windows -\u0026gt; Services Spring容器中找不到Serv","title":"spring常用代码"},{"content":"github action\ngitlab pipline\n","permalink":"https://ysyyhhh.github.io/posts/tips/piplineaction/","summary":"github action\ngitlab pipline","title":"pipline\u0026action"},{"content":"多个笔记之间存在关联, 可以先在mind-wings内笔记搜索, 再搜索百度.\n笔记格式为\n指令集 名称解释 学习步骤 其中指令集和名称解释是将学习步骤的内容做一个简要整理, 即备忘录. 目的是方便快速搜索.\n如果是初学者,直接跳过指令集和名词解释.\ngit init # 初始化本地git仓库（创建新仓库） git config --global user.name \u0026#34;xxx\u0026#34; # 配置用户名 git config --global user.email \u0026#34;xxx@xxx.com\u0026#34; # 配置邮件 git config --global color.ui true # git status等命令自动着色 git config --global color.status auto git config --global color.diff auto git config --global color.branch auto git config --global color.interactive auto git config --global --unset http.proxy # remove proxy configuration on git git clone git+ssh://git@192.168.53.168/VT.git # clone远程仓库 git status # 查看当前版本状态（是否修改） git add xyz # 添加xyz文件至index git add . # 增加当前子目录下所有更改过的文件至index git commit -m \u0026#39;xxx\u0026#39; # 提交 git commit --amend -m \u0026#39;xxx\u0026#39; # 合并上一次提交（用于反复修改） git commit -am \u0026#39;xxx\u0026#39; # 将add和commit合为一步 git rm xxx # 删除index中的文件 git rm -r * # 递归删除 git log # 显示提交日志 git log -1 # 显示1行日志 -n为n行 git log -5 git log --stat # 显示提交日志及相关变动文件 git log -p -m git show dfb02e6e4f2f7b573337763e5c0013802e392818 # 显示某个提交的详细内容 git show dfb02 # 可只用commitid的前几位 git show HEAD # 显示HEAD提交日志 git show HEAD^ # 显示HEAD的父（上一个版本）的提交日志 ^^为上两个版本 ^5为上5个版本 git tag # 显示已存在的tag git tag -a v2.0 -m \u0026#39;xxx\u0026#39; # 增加v2.0的tag git show v2.0 # 显示v2.0的日志及详细内容 git log v2.0 # 显示v2.0的日志 git diff # 显示所有未添加至index的变更 git diff --cached # 显示所有已添加index但还未commit的变更 git diff HEAD^ # 比较与上一个版本的差异 git diff HEAD -- ./lib # 比较与HEAD版本lib目录的差异 git diff origin/master..master # 比较远程分支master上有本地分支master上没有的 git diff origin/master..master --stat # 只显示差异的文件，不显示具体内容 git remote add origin git+ssh://git@192.168.53.168/VT.git # 增加远程定义（用于push/pull/fetch） git branch # 显示本地分支 git branch --contains 50089 # 显示包含提交50089的分支 git branch -a # 显示所有分支 git branch -r # 显示所有原创分支 git branch --merged # 显示所有已合并到当前分支的分支 git branch --no-merged # 显示所有未合并到当前分支的分支 git branch -m master master_copy # 本地分支改名 git checkout -b master_copy # 从当前分支创建新分支master_copy并检出 git checkout -b master master_copy # 上面的完整版 git checkout features/performance # 检出已存在的features/performance分支 git checkout --track hotfixes/BJVEP933 # 检出远程分支hotfixes/BJVEP933并创建本地跟踪分支 git checkout v2.0 # 检出版本v2.0 git checkout -b devel origin/develop # 从远程分支develop创建新本地分支devel并检出 git checkout -- README # 检出head版本的README文件（可用于修改错误回退） git merge origin/master # 合并远程master分支至当前分支 git cherry-pick ff44785404a8e # 合并提交ff44785404a8e的修改 git push origin master # 将当前分支push到远程master分支 git push origin :hotfixes/BJVEP933 # 删除远程仓库的hotfixes/BJVEP933分支 git push --tags # 把所有tag推送到远程仓库 git fetch # 获取所有远程分支（不更新本地分支，另需merge） git fetch --prune # 获取所有原创分支并清除服务器上已删掉的分支 git pull origin master # 获取远程分支master并merge到当前分支 git mv README README2 # 重命名文件README为README2 git reset --hard HEAD # 将当前版本重置为HEAD（通常用于merge失败回退） git rebase git branch -d hotfixes/BJVEP933 # 删除分支hotfixes/BJVEP933（本分支修改已合并到其他分支） git branch -D hotfixes/BJVEP933 # 强制删除分支hotfixes/BJVEP933 git ls-files # 列出git index包含的文件 git show-branch # 图示当前分支历史 git show-branch --all # 图示所有分支历史 git whatchanged # 显示提交历史对应的文件修改 git revert dfb02e6e4f2f7b573337763e5c0013802e392818 # 撤销提交dfb02e6e4f2f7b573337763e5c0013802e392818 git ls-tree HEAD # 内部命令：显示某个git对象 git rev-parse v2.0 # 内部命令：显示某个ref对于的SHA1 HASH git reflog # 显示所有提交，包括孤立节点 git show HEAD@{5} git show master@{yesterday} # 显示master分支昨天的状态 git log --pretty=format:\u0026#39;%h %s\u0026#39; --graph # 图示提交日志 git show HEAD~3 git show -s --pretty=raw 2be7fcb476 git stash # 暂存当前修改，将所有至为HEAD状态 git stash list # 查看所有暂存 git stash show -p stash@{0} # 参考第一次暂存 git stash apply stash@{0} # 应用第一次暂存 git grep \u0026#34;delete from\u0026#34; # 文件中搜索文本“delete from” git grep -e \u0026#39;#define\u0026#39; --and -e SORT_DIRENT git gc git fsck 名词解释 repo\nGit概述 简要介绍 安装与检查 安装命令\nsudo apt-get install git #Ubuntu sudo yum install git #Centos 检查安装是否成功\ngit --version #检查版本号 配置 git config --global user.name \u0026#34;username\u0026#34; git config --global user.email \u0026#34;xxx@example.com\u0026#34; ","permalink":"https://ysyyhhh.github.io/posts/tool/git/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","summary":"多个笔记之间存在关联, 可以先在mind-wings内笔记搜索, 再搜索百度. 笔记格式为 指令集 名称解释 学习步骤 其中指令集和名称解释是将学习步骤的","title":"指令集"},{"content":" 程序优化侧重于提高程序的性能，通过对程序中关键函数的跟踪或者运行时信息的统计，找到系统性能的瓶颈，从而采取进一步行动对程序进行优化，同时减少资源使用。 程序正确性侧重于确保程序执行它应该做的事情，帮助开发者找出错误代码的位置。（本文以程序正确性的分析为主） 程序分析方法： 第一类是静态程序分析，即在不执行程序的情况下进行程序分析。\n第二类是动态程序分析，即通过运行程序或者在程序运行期间进行分析。\n动态分析方法包括：调试、覆盖测试、剖面测试、动态切片、动态污点分析等1。\n当然，也有很多研究工作是关于如何有效结合静态和动态程序分析的。同时，因为通常无法拿到真正的程序正确性的需求，绝大多数的程序分析技术着重于分析通用的程序正确性需求，比如如果有断言的话，我们尽量分析断言会不会被违背，再比如分析是否存在整数或者缓存溢出，再或者检测指针相关的安全漏洞等。\n符号执行（通过用求解每条程序路上上的条件来生成测试用例）\n模型检测（通过抽象并遍历所有的程序行为来判断程序是不是正确）\n模糊测试（通过优化大量的生成测试用例）\n模型检查、符号执行、抽象解释等1。\n对基于静态分析（比如抽象解释，或者 lint）的工具，一个重要的问题就是如何减少假警报的。\n而对于动态分析（比如测试）而言，对应的问题就是如何减少漏报。\n除了把静态分析做的更精确（比如设计更复杂的 lint 规则），和把动态分析做的更完备（比如提要求更高的覆盖率标准）\n还有一个趋势，就是结合不同的程序分析技术取长补短。比如 hybrid fuzzing 的做法是，通过有效的结合符号执行与模糊测试来提高测试的覆盖率。\n插桩、覆盖率、动态切片、动态污点分析等。1\n技术分享 | 浅谈程序分析\n","permalink":"https://ysyyhhh.github.io/posts/knowledge/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/","summary":"程序优化侧重于提高程序的性能，通过对程序中关键函数的跟踪或者运行时信息的统计，找到系统性能的瓶颈，从而采取进一步行动对程序进行优化，同时减少","title":"程序分析"},{"content":"flushall 清空\n","permalink":"https://ysyyhhh.github.io/posts/language/spring/redis/","summary":"flushall 清空","title":"redis"},{"content":"service 做校验，如果非法，直接抛异常 + 全局异常处理\ncontroller 正常就是组合 service ，返回前端需要的数据。\njava异常效率低下是因为抛出异常会遍历所有涉及堆栈，具体代码在基类Throwable的fillInStackTrace()方法里。但其实可以通过在自定义异常中重写fillInStackTrace()来大幅度提高异常效率。\nhttps://segmentfault.com/q/1010000020840854\nhttps://blog.csdn.net/qq_41107231/article/details/115874974\n","permalink":"https://ysyyhhh.github.io/posts/language/spring/code%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","summary":"service 做校验，如果非法，直接抛异常 + 全局异常处理 controller 正常就是组合 service ，返回前端需要的数据。 java异常效率低下是因为抛出异常会遍历所有涉及堆栈，具体","title":"code相关知识"},{"content":"各个springboot\nNacos注册中心 核心功能 服务注册：\n服务心跳：\n服务同步：\n服务发现：拿到微服务地址\n服务调用：\n服务健康检查：\nRibbon 负载均衡 feign 优雅地调用远程服务 解决的是微服务之间调用问题\nsentinel 服务容错 解决服务雪崩等问题\n服务网关 解决的问题 解决客户端访问微服务的问题：\n维护微服务的多个地址 认证 鉴权复杂 跨域问题 所谓的API网关，就是指系统的统一入口。对于客服端来说，它封装了应用程序的内部结构，为客户端提供统一服务，一些与业务本身功能无关的公共逻辑可以在这里实现，诸如认证、鉴权、监控、路由转发等等。\n目前主流的解决方案 Ngnix+lua 使用nginx的反向代理和负载均衡可实现对api服务器的负载均衡及高可用\nlua是一种脚本语言,可以来编写一些简单的逻辑, nginx支持lua脚本\nKong\n基于Nginx+Lua开发，性能高，稳定，有多个可用的插件(限流、鉴权等等)可以开箱即用。 问题：\n只支持Http协议；二次开发，自由扩展困难；提供管理API，缺乏更易用的管控、配置方式。\nZuul\nspringboot1系列用的，Netflix开源的网关，功能丰富，使用JAVA开发，易于二次开发。Zuul 1.0 有问题：缺乏管控，无法动态配置；依赖组件较多；处理Http请求依赖的是Web容器，性能不如Nginx。\nZuul有2.0\nSpring Cloud Gateway\nSpring公司为了替换Zuul而开发的网关服务，将在下面具体介绍。\nGateway\n缺点：\n其实现依赖Netty与WebFlux，不是传统的Servlet编程模型，学习成本高 不能将其部署在Tomcat、Jetty等Servlet容器里，只能打成jar包执行 需要Spring Boot 2.0及以上的版本，才支持 路由 route 路由(Route) 是 gateway 中最基本的组件之一，表示一个具体的路由信息载体。主要定义了下面的几个信息:\nid，路由标识符，区别于其他 Route，默认是一个随机的UID，最好自己起一个###\nuri，路由指向的目的地 uri，即客户端请求最终被转发到的微服务。\norder，用于多个 Route 之间的排序，数值越小排序越靠前，匹配优先级越高。\npredicate，断言的作用是进行条件判断，只有断言都返回真，才会真正的执行路由。\nfilter，过滤器用于修改请求和响应信息。\n断言 predicate 用于条件判断，只有全部的断言为真，才实现路由转发。\n4.5.1 内置路由断言工厂\n可以自定义断言\n过滤器 作用：在请求过程中，对请求和响应做手脚 生命周期：PRE 和 POST 分类：局部过滤器（作用在一个路由上），全局过滤器（全部路由是） PRE生命周期：在被路由之前调用，跨域实现验证身份，集群。\nPOST生命周期：可以添加标准的Header，收集统计信息。\n局部过滤器 GateAway 内置有很多，可以自定义\n全局过滤器 网关限流 用sentinel\nMQ消息队列 一般用于请求加快\n","permalink":"https://ysyyhhh.github.io/posts/language/spring/spring-cloud-alibaba/","summary":"各个springboot Nacos注册中心 核心功能 服务注册： 服务心跳： 服务同步： 服务发现：拿到微服务地址 服务调用： 服务健康检查： Ribbon 负载均衡 feign","title":"微服务"},{"content":"","permalink":"https://ysyyhhh.github.io/posts/language/unity3d-+-ar/unity3d-1-1-%E5%85%A5%E9%97%A8/","summary":"","title":"unity3d 1-1 入门"}]